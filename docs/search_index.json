[
["index.html", "Probation review report Introduction", " Probation review report Alessandro Gasparini 2017-08-29 Introduction This report outlines the work I have completed and the projects I have been developing during my first year as a PhD student at the Department of Health Sciences, University of Leicester, under the supervision of Dr. Michael Crowther and Prof. Keith Abrams. I will begin by briefly introducing some terminology and notation that I will be using throughout this report in Chapter 1. Second, I will introduce survival models with random effects (e.g. frailties, in the simplest form) and joint models for longitudinal and time-to-event data in Chapters 2 and 3, respectively. Some of the computational challenges that survival models with random effects and joint models pose are presented in Chapter 4. I will then present the results of two simulation studies in Chapters 5 and 6; the first simulation study investigates the accuracy of quadrature methods when approximating analytically intractable terms, while the second simulation study investigates the impact of model misspecification in survival models with shared frailty terms. I will introduce an interactive tool I have been developing to aid dissemination of results from simulation studies and motivated by the simulation studies of Chapter 5 and 6 in Chapter 7. Next, I will introduce the problem of informative visiting process in clinical research using healthcare consumption data in Chapter 8, and how I aim to evaluate and compare the different approaches that have been proposed and utilised in literature to tackle such problem in Chapter 9. Chapter 9 will also include further research goals for the second and third year of my PhD. Finally, I will summarise the training and personal development activities I attended to during my first year in Chapter 10. The report is written using R and the bookdown package (Xie 2016), and can be accessed online at https://ellessenne.github.io/prr/. References "],
["terminology-notation.html", "1 Terminology and notation", " 1 Terminology and notation I will introduce in this Section some notation that I will be using throughout this report. I denote the random variable for an individual’s survival time with \\(T^*\\); it denotes time and can, therefore, assume any non-negative value. Lower-case \\(t^*\\) represents a realisation of \\(T^*\\) for a given individual. In the case of right censoring, I denote with \\(C\\) the random variable representing censoring time, and \\(c\\) its realisation. The observed time is denoted with \\(T = \\min(T^*, C)\\), and its realisation is \\(t\\). Finally, I denote with \\(D = I(T^* \\le C)\\) the random variable indicating either occurrence of the event of interest or censoring; analogously as before, its realisation is lower-case \\(d\\). Next, I will define two of the main quantities of interest in survival analysis, the survival function and the hazard function. They are both functions of the observed time \\(t\\) and are denoted by \\(S(t)\\) and \\(h(t)\\), respectively. The survival function is the complement of the cumulative distribution function of the observed time \\(T\\) and represents the probability that a given individual survives (loosely speaking) longer than \\(t\\): \\[ S(t) = 1 - F_T(t) = 1 - P(T \\le t) = P(T &gt; t) \\] \\(t\\) ranges (theoretically) between 0 and infinity, hence the survival function can be plotted as a smooth, continuous function that tends to 0 as \\(t\\) goes to infinity. In practice, though, the survival function appears as a step function as (1) individuals can be observed at discrete times only and (2) not all individuals may experience the event before the end of the study. The hazard function \\(h(t)\\) is the limit of the probability of the survival time \\(T\\) laying within an interval \\([t, t + \\Delta_t)\\) given that an individual survived up to time \\(t\\) divided by the length of the interval \\(\\Delta_t\\), for \\(\\Delta_t\\) approaching zero: \\[ h(t) = \\lim_{\\Delta_t \\to 0} \\frac{P(t \\le T &lt; t + \\Delta_t | T \\ge t)}{\\Delta_t} \\] It represents the instantaneous potential (e.g. risk) for the event to occur within the interval \\([t, t + \\Delta_t)\\) (with \\(\\Delta_t \\to 0\\)), given that the individual survived up to time \\(t\\). The hazard function is always non-negative, it can assume different shapes over time, and it has no upper bound. The survival function and the hazard function are strictly related. In fact, there is a clearly defined mathematical relationship between them, and it is possible to derive the form of \\(S(t)\\) when knowing the form of \\(h(t)\\) (and vice versa). Formally: \\[ S(t) = \\exp \\left[ -\\int_0^t h(u) \\ du \\right] \\] \\[ h(t) = -\\left[ \\frac{d S(t) / dt}{S(t)} \\right] \\] A third quantity of interest strictly related to the survival and hazard functions is the cumulative hazard function \\(H(t)\\). The cumulative hazard function represents the accumulation of hazard \\(h(t)\\) over time, and is defined as \\[ H(t) = \\int_0^t h(u) \\ du; \\] it can be expressed in terms of survival function, via the relationships \\(H(t) = - \\log S(t)\\) or alternatively \\(S(t) = \\exp(-H(t))\\). The notation presented in this Chapter follows Collett (2014), where further details can be found. Other seminal books covering the topic of survival analysis are Cox and Oakes (1984), Kalbfleisch and Prentice (2011), and Kleinbaum and Klein (2012). The hazard function \\(h(t)\\) and the survival function \\(S(t)\\) form building blocks for the models that I will be presenting in the following Sections 2 and 3 and it is crucial to keep their formulation and interpretation clear in mind. References "],
["smre.html", "2 Survival models with random effects", " 2 Survival models with random effects Random effects models are a kind of hierarchical model in which the data is assumed to have some sort of hierarchical structure: for instance, individual patients data clustered into families, cities, regions, and so on (Figure 2.1). It is also generally assumed that individuals are homogeneous within a hierarchical unit, heterogeneous between different units; by comparison, fixed effects models do not take into account any hierarchy or heterogeneity in the data. Additionally, the term “fixed effects” traditionally refers to the population-average effects while the term “random effects” refers to subject-specific effects, with the latter generally assumed to be unknown, unobserved variables. Figure 2.1: Example of clustered data Random effects models are generally used to analyse hierarchical data with a continuous, normally distributed outcome; such models are referred to as linear mixed-effects models, as they can incorporate both fixed and random effects, and generalise the linear regression model. With data consisting of repeated observations over time, the terms longitudinal data is commonly used. It is possible to encounter hierarchical data originating from a variety of distributions from the exponential family such as the Poisson, Gamma, and Binomial distribution. Linear mixed-effects models can be generalised to include such data, and these models are generally referred to as generalised linear mixed-effects models. Practically speaking, it is the same process of generalising linear models to generalised linear models. Survival data can present a hierarchical structure too; for instance, data could be clustered in geographical areas, institutions, or patients themselves. Meta-analysis of individual-patient data is a common example of survival data (when the outcome is time to event) with some hierarchical structure; another example is given by repeated-events data, such as infections or acute recurrent events, in which the first level of the hierarchical structure consists in the patient. A final example of survival data with biological clusters is given by twin data, in which siblings share some genetic factors. This heterogeneity structure often leads to violation of the implicit assumption that populations are homogeneous: sometimes it is impossible to include all relevant risk factors, or maybe such risk factors are not known at all. The result is unobserved heterogeneity. The simplest survival model with random effects is the univariate frailty model, in which a random effect - named frailty - is included in the model to account for the unobserved heterogeneity. The univariate frailty model can be generalised by allowing the frailty term to be shared between observations belonging to the same cluster of data; the resulting models are named shared frailty model. The frailty term generally acts multiplicatively on the baseline hazard, and it is modelled on the hazard scale; it is possible to alternatively formulate the model in terms of random effects rather than frailties, by including the frailty as an additive term on the log-hazard scale. I will introduce the univariate frailty model in Section 2.1, and generalise it to allow shared frailty terms in Section 2.2. Finally, I will present the alternative formulation in terms of random effects in Section 2.3. A comprehensive treatment of frailty models in survival analysis is given in Hougaard (2000) and Wienke (2010). References "],
["smre-univariate-frailty.html", "2.1 Univariate frailty models", " 2.1 Univariate frailty models In those settings where risk factors are not measured, their relevance is unknown, or it is not known whether such risk factors exist at all or not, it is useful to consider two sources of variability in survival analysis: variability accounted for by observable risk factors included in the model and heterogeneity caused by unknown covariates. The unobserved heterogeneity is described by the frailty term, which is assumed to follow some distribution. Formally: \\[ h(t|\\alpha) = \\alpha h_0(t), \\] where \\(\\alpha\\) is a non-observed frailty effect and \\(h_0(t)\\) is the baseline hazard function. The random variable \\(\\alpha\\), the frailty term, is chosen to have a distribution \\(f(\\alpha)\\) with expectation \\(E(\\alpha) = 1\\) and variance \\(V(\\alpha) = \\sigma ^ 2\\). \\(V(\\alpha)\\) is interpretable as a measure of heterogeneity across the population in baseline risk: as \\(\\sigma ^ 2\\) increases the values of \\(\\alpha\\) are more dispersed, with greater heterogeneity in \\(\\alpha h_0(t)\\). Underlying assumptions are: the frailty is time independent, and it acts multiplicatively on the underlying baseline hazard function. Introducing observed covariates into the model and inducing proportional hazards: \\[ h(t|X,\\alpha) = \\alpha h_0(t) \\exp(X \\beta) = \\alpha h(t | X), \\] with \\(X\\) and \\(\\beta\\) covariates and regression coefficients, respectively. Given the relationship between hazard and survival function, it can be showed that the individual survival function conditional on the frailty is \\(S(t | \\alpha) = S(t) ^ \\alpha\\). The population (i.e. marginal, or unconditional) survival function is obtained by integrating out the frailty from the conditional survival function: \\[ S(t) = \\int_0^{+\\infty} \\left[ S(t) \\right] ^ \\alpha f(\\alpha) \\ d\\alpha \\] The individual contribution to the likelihood (assuming no delayed entry) is conditional on the unobserved frailty \\(\\alpha\\) \\[ L_i = \\prod_{i = 1} ^ {n} \\left( \\alpha h_0(t_i)\\exp(X_i \\beta) \\right) ^ {d_i} \\exp(-\\alpha H_0(t_i) \\exp(X_i \\beta)), \\] with \\(d_i\\) event indicator variable, \\(H_0(t_i)\\) cumulative baseline hazard, and \\(t_i\\) observed survival time - all relative to the \\(i\\)-th individual. Different choices for the frailty distribution are possible. Assigning a probability distribution implies that the frailty can be integrated out of the likelihood function. After this integration, the likelihood can be maximized in the usual way if an explicit form exists. Otherwise, more sophisticated approaches like numerical integration or Markov Chain Monte Carlo methods are required. The most often used frailty distributions are the gamma and the log-normal distribution; the positive stable and the inverse Gaussian distribution are also common. Assuming that the frailty \\(\\alpha\\) has a Gamma distribution is convenient: it has the appropriate range \\((0, \\infty)\\) and it is mathematically tractable. A Gamma distribution with parameters \\(a\\) and \\(b\\) has density \\[ f(x) = \\frac{x ^ {a - 1} \\exp(- x / b)}{\\Gamma(a)b ^ a}; \\] by choosing \\(a = 1 / \\theta\\) and \\(b = \\theta\\) the resulting distribution has expectation \\(1\\) and finite variance \\(\\theta\\). In these settings, the model is analytically tractable: the population survival function has the form \\[ S(t) = (1 - \\theta \\log(S(t))) ^ {-1/\\theta}; \\] the likelihood follows by substitution. Estimating such model becomes therefore straightforward, which likely contributed to the popularity of Gamma frailty models. Together with the Gamma distribution, the log-normal distribution is the most commonly used frailty distribution, given its strong ties to random effect models; more on that in Section 2.3. Hence, assuming a log-normal distribution with a single parameter \\(\\theta &gt; 0\\) (for comparison with the mathematically tractable Gamma frailty model) with density \\[ f(x) = (2 \\pi \\theta) ^ {-\\frac{1}{2}} x ^ {-1} \\exp \\left( -\\frac{(\\log x) ^ 2}{2 \\theta} \\right), \\] the resulting model has a frailty whose expectation is finite. Nevertheless, this frailty distribution cannot be integrated out of the survival function analytically to obtain the population survival function or the likelihood. "],
["smre-shared-frailty.html", "2.2 Shared frailty models", " 2.2 Shared frailty models Further generalising the model presented in Section 2.1, it is possible for the frailty effect \\(\\alpha\\) to be shared between clusters of study subjects. Specifically, for the \\(j\\)-th observation in the \\(i\\)-th cluster: \\[ h_{ij}(t | \\alpha_i) = \\alpha_i h(t|X_{ij}). \\] The conditional survival function is: \\[ S_{ij}(t | \\alpha_i) = S_{ij}(t) ^ {\\alpha_i}. \\] In this setting, the cluster-specific contribution to the likelihood is obtained by calculating the cluster-specific likelihood conditional on the frailty, consequently integrating out the frailty itself: \\[ L_i = \\int_A L_i(\\alpha_i) f(\\alpha_i) \\, d\\alpha, \\] with \\(f(\\alpha)\\) the distribution of the frailty, \\(A\\) its domain, and \\(L_i(\\alpha_i)\\) the cluster-specific contribution to the likelihood, conditional on the frailty. The cluster-specific contribution to the likelihood is \\[ L_i(\\alpha_i) = \\alpha_i ^ {D_i} \\prod_{j = 1} ^ {n_i} S_{ij}(t_{ij}) ^ {\\alpha_i} h_{ij}(t_{ij}) ^ {d_{ij}}, \\] with \\(D_i = \\sum_{j = 1} ^ {n_i} d_{ij}\\). Analogously as before, analytical formulae can be obtained when \\(\\alpha_i\\) follows a Gamma distribution: \\[ L_i = \\left[ \\prod_{j = 1} ^ {n_i} h_{ij}(t_{ij}) ^ {d_{ij}} \\right] \\frac{\\Gamma (1 / \\theta + D_i)}{\\Gamma (1 / \\theta)} \\left[ 1 - \\theta \\sum_{j = 1} ^ {n_i} \\log S_{ij}(t_{ij}) \\right] ^ {-1 / \\theta - D_i}; \\] further details in Gutierrez (2002). As in the univariate frailty model, assuming a log-normal distribution requires some numerical approximation to be performed, being the resulting model analytically intractable. References "],
["smre-random-effects.html", "2.3 Alternative formulation", " 2.3 Alternative formulation A shared frailty model assuming a log-normal distribution for the frailty term has strong ties to random-effects models. A log-normal frailty model is formulated as \\[ h_{ij}(t | \\alpha_i) = \\alpha_i h(t | X_{ij}) = \\alpha_i h_0(t) \\exp(X_{ij} \\beta), \\] with \\(\\alpha_i\\) following a log-normal distribution. On the log-hazard scale: \\[ h_{ij}(t | \\alpha_i) = h_0(t) \\exp(X_{ij} \\beta + \\eta_i), \\] with \\(\\eta_i = \\log \\alpha_i\\). \\(\\eta_i\\) results being normally distributed with parameters \\(\\mu\\) and \\(\\sigma ^ 2\\) related to those of the log-normal distribution by the relationship \\[ E(\\alpha_i) = \\exp(\\mu + \\sigma ^ 2 / 2) \\] and \\[ Var(\\alpha_i) = \\exp(2 \\mu + \\sigma ^ 2) (\\exp(\\sigma ^ 2) - 1) \\] By formulating the model on the log-hazard scale, the frailty term has a direct interpretation as a random intercept in the model. It is possible to further extend this model by allowing random covariates effects, potentially ranging over multiple levels of clustering. Using the usual mixed-effects model notation: \\[ h_{ij}(t | b_i) = h_0(t) \\exp(X_{ij} \\beta + Z_{i} b_i), \\] with \\(X_{ij}\\) representing the design matrix for the fixed effects \\(\\beta\\) and \\(Z_i\\) representing the design matrix for the random effects \\(b_i\\). Any distribution or functional form can be assumed for \\(h_0(t)\\) (Crowther, Look, and Riley 2014), or it is possible to leave it unspecified altogether yielding a semi-parametric Cox model with random effects (Ripatti and Palmgren 2000; Therneau, Grambsch, and Pankratz 2003). References "],
["jm.html", "3 Joint models for longitudinal and survival data", " 3 Joint models for longitudinal and survival data It is increasingly common for observational studies and trials to follow participants over time, recording abundant data on clinical features throughout the duration of the study. Moreover, routinely collected healthcare consumption data and population registries are being used more and more for research purposes, after being linked with other data sources. As a consequence, applied researchers often encounter longitudinally recorded covariates to account for when studying the clinical outcome of interest (e.g. time to an event, that is what I will focus on). Researchers then face two options: (1) select only one of the multiple values per individual and analyse as such, ignoring much of the available data, or (2) take into account the potential dependency and association between the repeatedly measured covariates and the outcome interest. The latter is usually the most sensible choice, as the longitudinal data can contain important predictors or surrogates of the time to event outcome. A powerful tool to achieve so is given by joint models for longitudinal and time to event data, in which the longitudinal and survival processes are modelled jointly into a single model allowing to infer their association. The development of such models was motivated by HIV/AIDS clinical trials, in which immune response was recorded over the duration of the trial and the association with survival was of interest. Seminal works on the topic are the papers by Wulfsohn and Tsiatis (1997), Tsiatis and Davidian (2004), Henderson, Diggle, and Dobson (2000), Pawitan and Self (1993); a more recent tractation of the topic is in Ibrahim, Chu, and Chen (2010), Rizopoulos (2012), Gould et al. (2015). Previous attempts to tackle this problem consisted in (1) fitting a time-dependent Cox model (Cox 1972) by splitting individual rows every time a new observation from the longitudinal covariate becomes available, and (2) by using two-stages methods in which the longitudinal and survival data were modelled separately (Tsiatis, Degruttola, and Wulfsohn 1995). Nevertheless, it has been showed that joint modelling increases efficiency and reduces bias (Hogan and Laird 1998), while improving predictions at the same time (Rizopoulos et al. 2014). Applications of joint models for longitudinal data to answer complex study questions using complex clinical data are increasingly common in medical literature, in a variety of settings: among others, cardiology (Sweeting and Thompson 2011), nephrology (Asar et al. 2015), and intensive care medicine (Andrinopoulou et al. 2017). In this Chapter, I will focus on the basic joint model for longitudinal and survival data, with a single longitudinal process. I will present its formulation in Section 3.1, and the estimation process in Section 3.2. However, several extensions of the basic joint model presented in this Chapter have been proposed during the years, as the topic has received considerable attention. A review of the state of the art in joint models with a single longitudinal process is given by Gould et al. (2015). Furthermore, the joint model has been extended to allow incorporating multiple longitudinal processes at once, measured intermittently and not necessarily at the same time or with the same association structure with the survival component; a recent review on the topic is given by Hickey et al. (2016). References "],
["jm-formulation.html", "3.1 Model formulation", " 3.1 Model formulation A joint model for longitudinal and survival data consists of two components: a model for the longitudinal part (I will be assuming a single longitudinal trajectory from now on for simplicity) and a model for the survival part. These two components will then share a latent structure that will describe the association between the two processes. In literature, the dominant approach seems to be allowing the two components to share random effects; I will follow this approach. Building on the notation from Section 1, let \\(y_{ij} = \\{ y_{ij}(t_{ij}) \\ \\forall \\ j = 1, \\dots, n_i \\}\\) be the observed longitudinal response for the \\(i\\)th subject, with \\(y_{ij}(t_{ij})\\) the observed response at time \\(t_{ij}\\) and \\(n_i\\) the number of longitudinal observations. The longitudinal component of the joint model is modelled within the mixed-effects framework (Diggle et al. 2013), as longitudinal data is likely measured intermittently and with error. Therefore: \\[ y_i(t) = m_i(t) + \\epsilon_i(t), \\ \\epsilon_i(t) \\sim N(0, \\sigma^2) \\] and \\[ m_i(t) = X_i(t) \\beta + Z_i(t) b_i, \\ b_i \\sim N(0, \\Sigma) \\] with \\(X_i(t)\\) and \\(Z_i(t)\\) the time-dependent design matrices for the fixed and random effects, respectively, \\(\\beta\\) the fixed effects, and \\(b_i\\) the random effects for the ith individual. \\(y_i(t)\\) represents the observed longitudinal trajectory at time \\(t\\), which could be decomposed into the true longitudinal trajectory \\(m_i(t)\\) plus the measurement error \\(\\epsilon_i(t)\\). The survival component of the joint model is modelled using a proportional hazards time to event model, given the true unobserved longitudinal trajectory up to time \\(t\\), i.e. \\(M_i(t) = \\{m_i(s) \\ \\forall \\ 0 \\le s \\le t\\}\\): \\[ h(t | M_i(t)) = h_0(t) \\exp(W \\psi + \\alpha m_i(t)), \\] where \\(h_0(t)\\) is the baseline hazard function and \\(W\\) is a vector of time-fixed covariates with their regression parameters \\(\\psi\\). \\(\\alpha\\) is the association parameter that links the longitudinal component and the survival component of the joint model; it can be interpreted as the log-hazard ratio for a unit increase in the true longitudinal trajectory \\(m_i(t)\\), at time \\(t\\). This specific form of the association parameter is also known as the current value parametrisation; additional association structures are available, allowing for instance interactions, association with the slope of the trajectory or its cumulative effect, and so on. Further details in Rizopoulos (2012). The survival function follows as \\[ S(t | M_i(t)) = \\exp \\left( -\\int_0 ^ t h_0(u) \\exp(W \\psi + \\alpha m_i(u)) \\ du \\right) \\] Finally, regarding \\(h_0(t)\\): the choice of the baseline hazard function follows the usual rationale. It can be left unspecified, therefore resulting in a Cox model for the survival component of the joint model, or it can be specified using a parametric distribution (e.g. a Weibull distribution) or some flexible alternative (Crowther, Abrams, and Lambert 2012). Nevertheless, Hsieh, Tseng, and Wang (2006) showed that choosing the Cox model for the survival component yields standard errors that are underestimated; consequently, bootstrapping is required to obtain correct standard errors in that situation. References "],
["jm-estimation.html", "3.2 Estimation process", " 3.2 Estimation process Estimation of a joint model for longitudinal and survival data is a non-trivial task. The complexity of jointly modelling the longitudinal component and the survival component motivated the use of two-stages procedures as mentioned in Section 3. With that approach, the longitudinal component is modelled and estimated separately; consequently, subject-specific predictions from the longitudinal model are produced and plugged into the survival model as time-varying covariates. Despite the simplicity of this approach, though, it has been showed that it produces substantial bias and poor coverage (Tsiatis and Davidian 2001; Sweeting and Thompson 2011). Therefore, an approach that models both processes jointly is required. in particular, two approaches are predominant: a full likelihood approach, and a Bayesian approach; both have appealing characteristics, but they share the feature of being computationally intensive. Focusing on the full likelihood approach, it is possible to formulate the joint likelihood (Rizopoulos 2012) for the overall parameter vector \\(\\theta = \\{\\theta_t, \\theta_y, \\theta_b\\}\\), formed by the parameters of the survival component, the parameters of the longitudinal component, and the elements of the variance-covariance matrix of the random effects, respectively. The joint distribution of the survival time \\(T_i\\), the event indicator \\(d_i\\), and the longitudinal response \\(y_i\\), conditional on the random effects \\(b_i\\), can be expressed as: \\[ f(T_i, d_i, y_i | b_i, \\theta) = f(T_i, d_i | b_i, \\theta) f(y_i | b_i, \\theta), \\] with \\[ f(y_i | b_i, \\theta) = \\prod_{j = 1} ^ {n_i} f(y_i(t_{ij}) | b_i, \\theta). \\] It is important to note that the survival process and the longitudinal process are assumed to be independent, conditionally on the random effects \\(b_i\\). It follows that the contribution to the log-likelihood for the \\(i\\)th patient is \\[ \\begin{aligned} \\log L(\\theta) &amp;= \\log \\int_{-\\infty} ^ {+\\infty} f(T_i, d_i, y_i, b_i; \\theta) \\ db_i \\\\ &amp;= \\log \\int_{-\\infty} ^ {+\\infty} f(T_i, d_i | b_i, \\theta_t) \\left[ \\prod_{j = 1} ^ {n_i} f(y_i(t_{ij}) | b_i, \\theta_y) \\right] f(b_i | \\theta_b) \\ db_i \\end{aligned} \\] with \\(f(T_i, d_i | b_i, \\theta_t)\\) the contribution to the likelihood relative to the survival component of the model: \\[ \\begin{aligned} f(T_i, d_i | b_i, \\theta_t) &amp;= h_i(T_i | M_i(T_i), \\theta_t) ^ {d_i} S_i(T_i | M_i(T_i), \\theta_t) \\\\ &amp;= \\left[ h_0(T_i) \\exp(W \\psi + \\alpha m_i(T_i)) \\right] ^ {d_i} \\exp \\left[ -\\int_0^{T_i} h_0(u) \\exp(W \\psi + \\alpha m_i(u)) \\ du \\right], \\end{aligned} \\] \\(f(y_i(t_{ij}) | b_i, \\theta_y)\\) the contribution to the likelihood of the longitudinal process at time \\(t_{ij}\\): \\[ f(y_i(t_{ij}) | b_i, \\theta_y) = (2 \\pi \\sigma ^ 2) ^ {-1/2} \\exp \\left[ -\\frac{(y_i(t_{ij}) - m_i(t_{ij})) ^ 2}{2 \\sigma ^ 2} \\right], \\] and \\(f(b_i | \\theta_b)\\) the density of the random effects: \\[ f(b_i | \\theta_b) = (2 \\pi) ^ {-q_b / 2} | \\Sigma | ^ {-1 / 2} \\exp \\left[- \\frac{b_i^T \\Sigma ^ {-1} b_i}{2}\\right], \\] with \\(q_b\\) being the dimension of the random effects. Historically, the predominant method for maximising the full joint likelihood has been the Expectation-Maximisation algorithm (Dempster, Laird, and Rubin 1977); alternatively, it is possible to use general purpose optimisers to maximise the full joint likelihood via algorithms such as the Newton-Raphson algorithm. Nevertheless, significant computational challenges persist. References "],
["compch.html", "4 Computational challenges in survival models with random effects", " 4 Computational challenges in survival models with random effects The models I presented in Chapter 2 and 3 present significant computational challenges during the estimation process. I showed how frailty models with a Gamma frailty are analytically tractable, as it is possible to obtain closed-form expressions for the marginal survival function and therefore the likelihood; conversely, including a log-normal frailty (or, correspondingly, random effects) in a survival model yields a survival function - and likelihood - that does not have a closed form. Analogously, the joint likelihood of joint models for longitudinal and survival data \\(\\log L(\\theta) = \\log \\int_{-\\infty} ^ {+\\infty} f(T_i, d_i, y_i, b_i; \\theta) \\ db_i\\) requires evaluating an analytically intractable integral over a possibly multi-dimensional integral over the infinite domain; it is therefore necessary to use some method to approximate it numerically. Methods for approximating intractable integrals form the majority of this Chapter, with more details in Section 4.1. I will also be introducing numerical methods for differentiating a function and for root-finding in Sections 4.2 and 4.3. "],
["compch-numintgr.html", "4.1 Numerical integration", " 4.1 Numerical integration The term numerical integration implies the approximation of the integral of a function; generally, it aims to use the minimum number of function evaluations possible as it tends to be numerically expensive. There is a variety of methods being proposed in the literature to perform numerical integration; throughout this Section, I will focus on quadrature rules, i.e. any method that evaluates the function to be integrated at some points over the integration domain and combines the resulting values to obtain an approximation of the integral. Quadrature rules vary in complexity and accuracy, and generally accuracy improves as rules get more complex. Additionally, integration of functions in few dimensions is generally not too problematic; the task becomes more difficult when integrating over many dimensions as obtaining an acceptable level of accuracy often requires an unfeasible number of function evaluations. 4.1.1 Unidimensional functions The simplest method to approximate numerically the integral of a unidimensional function \\(f(x)\\) over an integration domain \\([a, b]\\) is given by the Riemann sum. A particular form of Riemann sum is given by the midpoint rule, which approximates the integral of a continuous function by the area under a set of \\(N\\) step functions, with the midpoint of each matching \\(f\\): \\[ \\int_a^b f(x) \\ dx \\approx \\frac{b - a}{N} \\sum_{i = 1}^N f(a + (i - 0.5)(b - a) / N) \\] Alternativeley, the trapezoidal rule approximates the area under a continuous function as a trapezoid and then computes its area: \\[ \\int_a^b f(x) \\ dx \\approx (b - a) \\left[ \\frac{f(a) + f(b)}{2} \\right] \\] it works best when partitioning the integration area into many subinterval, applying the trapezoidal rule to all of them, and then sum the results: \\[ \\int_a^b f(x) \\ dx \\approx \\sum_{i = 1} ^ N \\frac{f(x_{k - 1}) + f(x)}{2} \\Delta(x_k), \\] with \\({x_k}\\) a partition of \\([a, b]\\) such that \\(a = x_0 &lt; x_1 &lt; x_2 &lt; \\dots &lt; X_{N-1} &lt; x_N = b\\) and \\(\\Delta(x_k) = x_k - x_{k - 1}\\) the length of the \\(k\\)th subinterval. The accuracy of the midpoint and trapezoidal rules depends on the number of steps (subintervals) \\(N\\) used to approximate the function, but so does complexity (computationally speaking). The only requirement for applying these rules is that one needs to be able to evaluate the function \\(f(x)\\) at a given point in its domain. If \\(f(x)\\) is cheap to evaluate than the midpoint and trapezoidal rules may be just fine; otherwise, it would be better to move onto more complicated methods that yield more accurate results. A first method that is only slightly more complicated but yields better results is the Simpson’s rule. It works analogously to the midpoint and trapezoidal rule, but using a smooth quadratic interpolant which takes the same values as \\(f(x)\\) at the extremities of the integration interval \\([a, b]\\) and at the midpoint \\(m = (a + b) / 2\\): \\[ \\int_a^b f(x) \\ dx \\approx \\frac{b - a}{6} \\left[ f(a) + 4f((a + b) / 2) + f(b) \\right] \\] Analogously as the trapezoidal rule, it is possible to obtain greater accuracy by splitting the integration interval into many subintervals, applying the Simpson’s rule to each subinterval, and adding up the results. Second, it is possible to show that by carefully choosing the points at which to evaluate \\(f(x)\\) and the weights assigned to each point it is possible to obtain an exact approximation of the integral of any polynomial of degree \\(2N - 1\\) or less with \\(N\\) function evaluations (proof in Monahan (2011)). Methods that exploit this feature are commonly named Gaussian quadrature methods. Let \\(f(x)\\) be a function of order \\(2N - 1\\) or less to integrate over a conventional domain \\([-1, 1]\\); let \\(w(x)\\) be a weight function. The quadrature formula is defined as: \\[ \\int_{-1}^{1} f(x) \\ dx = \\sum_{i = 1} ^ N w_i f(x_i) \\] Before applying Gaussian quadrature, any integral over the domain \\([a, b]\\) needs to be changed to to the interval \\([-1, 1]\\). The change of interval is applied as \\[ \\int_a^b f(x) \\ dx = \\frac{b - a}{2} \\int_{-1}^{1} f\\left( \\frac{b - a}{2} x + \\frac{a + b}{2} \\right) \\ dx, \\] and the Gaussian quadrature rule is then \\[ \\int_a^b f(x) \\ dx = \\frac{b - a}{2} \\sum_{i = 1} ^ N w_i f\\left( \\frac{b - a}{2} x_i + \\frac{a + b}{2} \\right). \\] If \\(f(x) = w(x) g(x)\\) can be written as the product of a polynomial function \\(g(x)\\) and a known weighting function \\(w(x)\\), alternative weights \\(w_i&#39;\\) and nodes \\(x_i&#39;\\) give better results: \\[ \\int_{-1}^{1} f(x) \\ dx = \\int_{-1}^{1} w(x) g(x) \\ dx = \\sum_{i = 1} ^ N w_i&#39; f(x_i&#39;) \\] Depending on the choice of the weighting function \\(w(x)\\), different Gaussian quadrature rules can be obtained that allow integrating over different domains \\([a, b]\\). When \\(w(x) = 1\\), the associated polynomials \\(g(x)\\) are Legendre polynomials, the quadrature rule is then named Gauss-Legendre quadrature rule, and it allows integrating over the interval \\([-1, 1]\\). The integration points are then obtained as the the \\(N\\) roots of the Legendre polynomials: \\(x = \\{x_1, x_2, \\dots, x_N\\}\\). When choosing the weight function \\(\\exp(-x)\\) the associated polynomials are Laguerre polynomials, the quadrature rule is named Gauss-Laguerre quadrature rule, and the integration domain is \\([0, +\\infty)\\). Finally, when choosing the weight function \\(\\exp(-x^2)\\) the associated polynomials are Hermite polynomials, the quadrature rule is named Gauss-Hermite quadrature rule, and the integration domain is \\((-\\infty, +\\infty)\\). Finally, a slightly more complicated version of Gaussian quadrature is given by the Gauss–Kronrod quadrature formula. In the Gauss-Kronrod quadrature rule, the evaluation points are chosen dynamically so that an accurate approximation can be computed by reusing the information produced by the computation of a less accurate approximation. In practice, integration points from previous iterations can be reused as part of the new set of points, whereas usual Gaussian quadrature would require recomputation of all abscissas at each iteration. This is particularly important when some specified degree of accuracy is needed but the number of points needed to achieve this accuracy is not known ahead of time. Gauss-Kronrod quadrature rule is implemented in R as the integrate() function. 4.1.2 Multidimensional functions All the methods presented so far only apply to the integration of unidimensional functions. It is clearly possible to extend quadrature rules to multidimensional settings, by recursively applying unidimensional quadrature rules. Say I want to approximate the integral of a bi-dimensional function \\(f(x, y)\\); the bi-dimensional Gaussian quadrature rule has the form: \\[ \\int_X \\int_Y f(x, y) \\ dx \\ dy \\approx \\sum_j \\sum_i w_j w_i f(x_j, y_i) \\] This can be extended to any number of dimensions \\(d\\), but it gets very computationally expensive very quickly as a \\(N\\)-points rule requires \\(N^d\\) function evaluations. Two simple tricks for improving accuracy and efficiency are given in Jäckel (2005) and consist in pruning and/or rotating the matrix of location nodes (Figure ??). With pruning, nodes with extremely low weights that contribute very little to the total integral value can be removed to spare some computational cost. With rotating, the correlation between the \\(d\\) dimensions is taken into account. For instance, assume I am integrating over the random effects of a joint model for longitudinal and survival data; the random effects variance-covariance matrix can be decomposed using either the Cholesky decomposition or the spectral decomposition, and consequently the matrix of location nodes can be pre-multiplied by such decomposition to rotate the grid of integration points, better adapting to the shape of the multidimensional function to integrate. A better option when the number of dimensions \\(d\\) to integrate over is high is given by Monte Carlo integration. Consider integrating a multidimensional function \\(f(x)\\) over some region \\(\\Omega\\) of volume \\(V(\\Omega)\\): \\[ I_{\\Omega} = \\int_{\\Omega} f(x) \\ dx = E[f(U)] V(\\Omega), \\] with \\(U \\sim\\) uniform over \\(\\Omega\\). Drawing \\(N\\) uniform random vectors \\(u_i\\) an estimator for \\(I_{\\Omega}\\) is \\[ \\hat{I}_{\\Omega} = \\frac{V(\\Omega)}{N} \\sum_{i = 1} ^ N f(u_i); \\] this estimator defines Monte Carlo integration. More details in Monahan (2011). Both Gaussian quadrature and Monte Carlo integration can be further tweaked to improve accuracy and convergence rates: two appealing options are, respectively, adaptive Gaussian quadrature and importance sampling. Adaptive Gaussian quadrature works best when using the Gauss-Hermite rule with the normal density kernel as weighting function. Recall using the spectral or Cholesky decomposition of the random effects variance-covariance to pre-multiply the matrix of location nodes. This will not place the nodes in optimum locations for each subject: if between-subject variability is large, then a common matrix of location nodes is likely to be quite inefficient. Adaptive Gauss-Hermite quadrature, first proposed by Pinheiro and Bates (1995), aims to solve this problem. In brief, the idea consists in applying subject-specific (or cluster-specific) centring and scaling of the quadrature nodes to place the quadrature nodes at an optimal position for each subject. That is achieved by using an alternative normal kernel distribution \\(\\phi(x; \\hat{M}_i, \\hat{\\Sigma}_i)\\) with subject-specific mean vector \\(\\hat{M}_i\\) and variance-covariance matrix \\(\\hat{\\Sigma}_i\\): \\[ f(x) = \\frac{f(x)}{\\phi(x; \\hat{M}_i, \\hat{\\Sigma}_i)} \\phi(x; \\hat{M}_i, \\hat{\\Sigma}_i) \\] This yields a quadrature rule based on the subject-specific normal kernel: the location nodes and weights then depend on \\(\\hat{M}_i\\) and \\(\\hat{\\Sigma}_i\\). Further, \\(\\hat{M}_i\\) and \\(\\hat{\\Sigma}_i\\) can be updated iteratively (e.g. using empirical Bayes estimates) to better adapt the grid of quadrature points to the actual shape of the integral to approximate. Conversely, Monte Carlo integration works best when it is possible to draw a sample from the target distribution (i.e. the distribution of the integral to approximate); unfortunately, that is rarely the case in practice. The idea of importance sampling consists then in drawing a sample from a proposal distribution and then re-weight the estimated integral using importance weights to better adapt to the target distribution. References "],
["compch-numdiff.html", "4.2 Numerical differentiation", " 4.2 Numerical differentiation Numerical differentiation is a series of algorithms to numerically estimate the derivative of a function. They tend to be computationally less demanding than numerical integration methods, but they are more sensitive to cancellation error. The easiest method for approximating the derivative of a function is to use finite difference approximation. The finite difference approximation of the derivative of a continuous function \\(f(x)\\) at \\(x\\), \\(f&#39;(x)\\), is calculated as \\[ f&#39;(x) \\approx \\frac{f(x + h) - f(x)}{h}, \\] for a small \\(h\\). This formula is affected by both truncation error (as it derives from a truncated Taylor series expansion of \\(f(x)\\)) and cancellation error (as a machine works with finite-precision arithmetic). It is necessary to choose a value \\(h\\) that gives a good balance between the two errors: it can be showed that a good choice in most cases is \\(h = \\sqrt{\\epsilon}\\), with \\(\\epsilon\\) being the machine precision. This formula for finite different approximation is also known as forward differencing; other options are central differencing (\\([f(x + h) - f(x - h)] / 2h\\), more accurate but more computationally expensive), backward differencing (\\([f(x) - f(x - h)] / h\\)), the complex method (extremely powerful but with limited applicability), and the Richardson’s extrapolation method, which is more accurate but slower than finite differencing. "],
["compch-numroot.html", "4.3 Numerical root finding", " 4.3 Numerical root finding Root-finding algorithms are algorithms for finding the values \\(x\\) such that \\(f(x) = 0\\), for a given continuous function \\(f(\\cdot)\\). Such values \\(x\\) are named roots (or zeros) of a function. Most root-finding algorithms are based on the intermediate value theorem, which states that if a continuous function has values of opposite sign at the end points of an interval then the function has at least one root in the interval. For instance, the easiest root-finding method is the bisection method: let \\(f(x)\\) be a continuous function, for which one knows an interval \\([a, b]\\) such that \\(f(a)\\) and \\(f(b)\\) have opposite sign. Let \\(c = (a + b) / 2\\) be the midpoint the bisect the interval: now, either \\(f(a)\\) and \\(f(c)\\) or \\(f(c)\\) and \\(f(b)\\) have opposite sign, and one has in fact divided by two the size of the interval. One can iterate this method until the difference between the extremities of the interval is small enough (e.g. \\(&lt;1 \\times 10^{-8}\\)). Another well established method is the secant method: it uses a succession of roots of secant lines to approximate the root of a function \\(f(x)\\). Starting with values \\(x_0\\) and \\(x_1\\), a line is constructed between \\((x_0, f(x_0))\\) and \\((x_1, f(x_1))\\): \\[ y = \\frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_1) + f(x_1) \\] The root of this line is \\[ x = x_1 - f(x_1) \\frac{x_1 - x_0}{f(x_1) - f(x_0)} \\] Now, we set \\(x_2 = x\\) and we iterate this method until the difference between the extremities of the interval is small enough (e.g. \\(&lt;1 \\times 10^{-8}\\)). The secant method is also known as a linear interpolation method; it is also possible to use higher order interpolation, specifically quadratic interpolation, to find the root of a function using the same rationale presented for the secant method. Finally, a well-established and robust method is the Brent-Dekker method, implemented in R with the uniroot() function. It combines the three methods presented before, trying to use the secant or quadratic interpolation method first - as they tend to converge faster to a solution - but falling back to the bisection method if necessary, for its robustness properties. More details on the Brent-Dekker method in Brent (1973). References "],
["simst1.html", "5 Simulation study: accuracy of Gaussian quadrature", " 5 Simulation study: accuracy of Gaussian quadrature In this Chapter I will present the first, small simulation study I run during my first year, on accuracy on Gaussian quadrature methods. The aim of this simulation study is two-fold. First, I want to assess the accuracy of Gaussian quadrature using available analytical formulae as a control method. Second, I aim to assess the accuracy of Gaussian quadrature when analytical formulae are not available and therefore quadrature is indeed required. I presented part of this work as an oral presentation at the 2017 Survival Analysis for Junior Researchers conference (more details in Chapter 10 and slides available in Appendix C.1). "],
["simst1-dgms.html", "5.1 Data-generating mechanisms", " 5.1 Data-generating mechanisms I generated survival data from a Weibull distribution with shape parameter \\(\\lambda\\) = 0.5 and scale parameter \\(p\\) = 0.6 using the method of Bender, Augustin, and Blettner (2005), and applying administrative censoring at time \\(t\\) = 5; I used the following parametrisation for the Weibull distribution: \\[ h(t) = \\lambda p t ^ {p - 1} \\] I included a binary covariate (e.g. a treatment) simulated by drawing from a Bernoulli random variable with parameter \\(\\pi\\) = 0.5, and a frailty term shared between individuals in a cluster by drawing first from a Gamma distribution with shape parameter \\(1 / \\theta\\) and scale parameter \\(\\theta\\) (for identifiability purposes) and then by drawing from a normal distribution with mean \\(\\mu\\) = 0 and standard deviation \\(\\sigma = \\sqrt{\\theta}\\). I varied \\(\\theta\\): \\(\\theta\\) = {0.25, 0.75, 1.25}. I also varied the regression coefficient (e.g. the log-treatment effect) \\(\\beta\\) associated with the binary covariate: \\(\\beta\\) = {-0.50, 0.00, 0.50}. I simulated data for six different sample sizes: 15 clusters of 30, 100, or 500 individuals each, 50 clusters of 30 or 100 individuals, and 1000 clusters of 2 individuals; sample size then varied between 450 and 7500 individuals. Finally, I used a fully factorial design combining different frailty variances, frailty distributions, treatment effects, and sample sizes; it resulted in 3 times 2 times 3 times 6 = 108 different data-generating mechanisms, and for each of them I generated 1000 datasets. The 54 simulated scenarios with a Gamma frailty will be used to answer the first aim of the simulation study, as it is possible to derive an analytical formulation of the likelihood. The remaining 54 scenarios with a log-normal frailty will be used to answer the second aim. References "],
["simst1-methods-est-pm.html", "5.2 Methods, estimands, and performance measures", " 5.2 Methods, estimands, and performance measures I fitted a set of models for each simulated dataset under each data-generating mechanism. Specifically, for the data generated assuming a Gamma frailty, I compared the following models: a shared Gamma frailty model with a baseline Weibull hazard using the analytical formulation of the likelihood (method AF); a shared Gamma frailty model with a baseline Weibull hazard using the likelihood approximated numerically via Gaussian quadrature (specifically, a Gauss-Laguerre quadrature rule) with 15, 35, 75, and 105 nodes (methods GQ15, GQ35, GQ75, GQ105); a shared Gamma frailty model with a baseline Weibull hazard using the likelihood approximated numerically via Gauss-Kronrod quadrature (as implemented in R’s integrate() function; method IN). Then, for data generated assuming a log-normal frailty, I fitted a Weibull model with a random intercept using the likelihood approximated via Gauss-Hermite quadrature using 15, 35, 75, and 105 nodes (methods GQ15, GQ35, GQ75, GQ105). For each model I compared the estimated parameters of the Weibull distribution \\(\\hat{\\lambda}\\) and \\(\\hat{p}\\), the estimated log-treatment effect \\(\\hat{\\beta}\\), and the estimated variance of the frailty component \\(\\hat{\\theta}\\). In terms of performance measures, I am interested first of all in the performance of the maximum likelihood estimation procedure; that is, how precise is the maximum likelihood estimator. I will assess this by computing bias for each estimand, defined as \\(b = E(\\hat{\\beta}) - \\beta\\). Next, I am interested in coverage, i.e. the proportion of times the \\(100 \\times (1 - \\alpha)\\%\\) confidence interval \\(\\hat{\\beta} \\pm Z_{1 - \\alpha / 2} \\times SE(\\hat{\\beta})\\) includes the true value \\(\\beta\\). This allow to assess whether the empirical coverage rate approaches the nominal coverage rate (\\(100 \\times (1 - \\alpha)\\%\\)), to properly control the type I error rate for testing a null hypotesis of no effect. Finally, I am interested in overall accuracy and therefore I will compute the mean squared error, defined as the sum of bias and variability: \\((\\bar{\\hat{\\beta}} - \\beta) ^ 2 + (SE(\\hat{\\beta})) ^ 2\\). Summary measures for \\(\\lambda\\), \\(p\\), and \\(\\theta\\) are computed on the log-scale. For bias and coverage, I will further include Monte Carlo standard errors to quantify the uncertainty in estimating the performance measures (further details in White (2010)). References "],
["simst1-res.html", "5.3 Results", " 5.3 Results I selected a single scenario for each aim of this simulation study (out of 54) to present, for conciseness. Specifically, I will present results for the setting of a small frailty variance (0.25) with a negative regression coefficient (-0.50). The full results can be explored online interactively by clicking here. 5.3.1 Aim 1: accuracy compared to analytical formulae Bias, coverage, and mean squared error are presented in Tables A.1, A.2, A.3 and Figures B.1, B.2, B.3. Bias, coverage, and overall accuracy were optimal for all methods and across all sample sizes for the scale parameter of the Weibull distribution \\(p\\) and the regression coefficient \\(\\beta\\); conversely, the methods performed quite differently for the shape parameter \\(\\lambda\\) and the frailty variance \\(\\theta\\). The shape parameter estimated using analytical formulae or Gauss-Kronrod quadrature was generally unbiased, with good coverage and accuracy; vice versa, using Gauss-Laguerre quadrature produced underestimated coefficients when using a small number of nodes and required at least 75 nodes to yield unbiased results. As the number of nodes increased, coverage and mean squared error improved considerably. Also, sample sizes with a higher number of clusters generally yielded better estimates for the shape parameter in terms of bias, coverage, and mean squared error. The frailty variance \\(\\theta\\) was the parameter estimated with the greatest variability in the results. Analytical formulae required a high number of clusters to produce unbiased results (50 or 1000), yielding underestimated coefficients otherwise. Gauss-Kronrod performed similarly to analytical formulae, as did Gauss-Laguerre quadrature with a sufficiently high number of nodes. Coverage was generally good, above 90% (except Gauss-Laguerre with 15 nodes, where coverage fell to 60-70% in some settings), symptom of overestimated standard errors for the frailty variance; this inflation of the standard errors was reflected in the mean squared error, which was generally greater than the other estimated parameters for all methods under all sample sizes explored in this scenario. 5.3.2 Aim 2: accuracy when analytical formulae are not available Bias, coverage, and mean squared error are presented in Tables A.4, A.5, A.6, and Figures B.4, B.5, B.6. Bias is generally negligible for the parameters of the Weibull distribution \\(\\lambda\\) and \\(p\\) and the regression coefficient \\(\\beta\\): between 0.0059 and 0.0193 for \\(\\lambda\\), between -0.0424 and -0.0332 for \\(p\\), between 0.0040 and 0.0867 for \\(\\beta\\). Conversely, estimates for \\(\\sigma\\) were negatively biased for a sample size of 15 clusters - 100 individuals, 1000 clusters - 2 individuals, 15 clusters - 30 individuals (between -0.3057 and -0.0854) and positively biased for a sample size of 15 clusters - 500 individuals (between and 0.2427 and 0.4020). Bias was negligible for a sample size of 50 clusters - 30 individuals and 50 clusters - 100 individuals (between -0.0536 and -0.0095). Coverage of all estimated coefficients was poor (&lt; 75%) for a sample size of 15 clusters - 500 individuals. For the regression coefficient \\(\\beta\\) and the frailty variance \\(\\sigma\\) coverage was good or superoptimal for the remaining sample sizes, with the exception of \\(\\sigma\\) estimated using Gauss-Hermite quadrature with 15 nodes that resulted in slight undercoverage for sample sizes of 15 clusters - 100 individuals and 50 clusters - 100 individuals. The parameters of the Weibull distribution were generally undercovered (&lt; 80%) across sample sizes, except \\(\\lambda\\) with a sample size of 1000 clusters - 2 individuals and \\(p\\) with a sample size of 15 clusters - 30 individuals for which coverage was in the range 90-95%. Finally, mean squared error was low for \\(\\lambda\\), \\(p\\), and \\(\\beta\\), comparable for \\(\\sigma\\) with a sample size of 50 clusters - 30 individuals and 50 clusters - 100 individuals, much higher for \\(\\sigma\\) with all the remaining sample sizes (i.e. overall accuracy was lower in these settings). "],
["simst1-conclusions.html", "5.4 Conclusions", " 5.4 Conclusions I showed in the previous Section how Gaussian quadrature performs (1) compared to analytical formulae and (2) when it is not possible to obtain analytical formulae. Overall, Gaussian quadrature performs well with a sufficient number of quadrature nodes but the variability is great. The regression coefficient \\(\\beta\\) is the most robust estimand across different scenarios, it is mostly unbiased (or with little bias) and with good coverage and accuracy (in terms of mean squared error). The frailty variance is the least robust estimand, with precision and accuracy greatly depending on many factors such as the number of quadrature nodes and the number of clusters. The latter makes sense theoretically: with more clusters, it should be easier to properly estimate a variance term. Accuracy and precision of the maximum likelihood estimator for the parameters of the Weibull baseline hazard also varied greatly. In conclusion, using a shared frailty model to do inference on a regression coefficient seems to be robust to the accuracy of numerical integration methods; nevertheless, if the principal research interest lays in relative risk estimates, using a parametric model may not be the best choice after all. A semi parametric Cox model - even with frailty terms if necessary - could be utilised instead. If the research objectives include absolute risk estimations, though, a parametric model is immediately more appealing. However, checking the convergence, precision, and accuracy of numerical integration by evaluating and comparing an increasing number of quadrature knots appears to be fundamental. "],
["simst2.html", "6 Simulation study: impact of misspecification in survival models with shared frailty terms", " 6 Simulation study: impact of misspecification in survival models with shared frailty terms In this Chapter, I will present the second simulation study I set up and run during my first year. It investigates the impact of model misspecification in survival models with shared frailty terms, and part of this work was presented in oral form at the 2017 Statistical Analysis of Multi-Outcome Data (SAM) Conference and at the 38th Annual Conference of the International Society for Clinical Biostatistics (more details in Chapter 10 and slides available in Appendix C.2). I am also currently writing up this project into a manuscript for submission to a journal; a current draft is attached as well in Appendix D. This Chapter is arranged as follows. First, I introduce the aim of the simulation study in greater detail in Section 6.1. Then, in Section 6.2 I will introduce the data-generating mechanisms, in Section 6.3 I will describe the different models I included in the comparison, in Section 6.4 I will define the estimands of interest, in Section 6.5 I will present the performance measures used to compare the different models, in Section 6.6 I will present some results, and finally I will conclude the Chapter in Section 6.7. "],
["simst2-aim.html", "6.1 Aim", " 6.1 Aim The de-facto standard method used in medical research when dealing with time to event data is the Cox proportional hazards model. It is best suited when relative risk estimates are the quantities of interest. However, often the focus is on absolute measures of risk: in that context, modelling the baseline hazard is necessary, and it can be achieved by using standard parametric survival models with a simple parametric distribution (such as the exponential, Weibull, or Gompertz distribution) or by using the flexible parametric modelling approach (Royston and Parmar 2002) to better capture the shape of complex hazard functions. The latter approach requires choosing the number of degrees of freedom for the spline term used to approximate the baseline hazard: in practice, sensitivity analyses and information criteria (AIC, BIC) have been used to select the best model. Recently, Rutherford, Crowther, and Lambert (2015) showed via simulation studies that, assuming a sufficient number of degrees of freedom is used, the approximated hazard function given by restricted cubic splines fit well for a number of complex hazard shapes and the hazard ratios estimation is insensitive to the correct specification of the baseline hazard. Moreover, it is common to encounter clustered survival data where the overall study population can be divided into heterogeneous clusters of homogeneous observations; examples are given in Chapter 2. As a consequence, survival times of individuals within a cluster are likely to be correlated and need to be analysed as such by including a random effect, e.g. a frailty term. Flexible parametric survival models are a robust alternative to standard parametric survival models when the shape of the hazard function is complex; using a sufficient number of degrees of freedom, e.g. 2 or more, the spline-based approach is able to capture the underlying shape of the hazard function with minimal bias. AIC and BIC can guide the choice of the best fitting model, but they tend to agree to within 1 or 2 degrees of freedom in practice (Rutherford, Crowther, and Lambert 2015). Analogously, the impact of the choice of a particular parametric frailty distribution on the regression coefficients is minimal (Pickles and Crouchley 1995). Conversely, little is know about the impact of misspecifying the baseline hazard in survival models with frailty terms. My aim with this work is to assess the impact of misspecifying the baseline hazard or the frailty distribution on the estimated regression coefficients, frailty variance, and absolute, marginal risk measures such as the integrated difference of survival curves and the survival difference at given time points. I will simulate data under a variety of data-generating mechanisms, and then compare a set of models that include the Cox model with frailties, fully parametric survival models with frailty, models with flexible baseline hazard, and models with flexible baseline hazard and a penalty for the complexity of the spline term. References "],
["simst2-dgms.html", "6.2 Data-generating mechanisms", " 6.2 Data-generating mechanisms I simulate data under five different baseline hazard functions using the approaches of Bender, Augustin, and Blettner (2005) and Crowther and Lambert (2013): Exponential, Weibull, Gompertz, and a two different two-components mixture Weibull-Weibull with turning points. In practice, I choose the following hazard functions: exponential with scale \\(\\lambda\\) = 0.3, Weibull with scale \\(\\lambda\\) = 0.5 and shape \\(p\\) = 0.6, Gompertz with scale \\(\\lambda\\) = 0.1 and shape \\(\\gamma\\) = 0.5, two-components mixture Weibull with scale parameters \\(\\lambda_1\\) = 0.5, \\(\\lambda_2\\) = 0.3, shape parameters \\(p_1\\) = 2.5 and \\(p_2\\) = 1.3, and mixing parameter \\(\\pi\\) = 0.8, and two-components mixture Weibull-Weibull with scale parameters \\(\\lambda_1\\) = \\(\\lambda_2\\) = 1.0, shape parameters \\(p_1\\) = 1.5 and \\(p_2\\) = 0.5, and mixing parameter \\(\\pi\\) = 0.5 (Figure 6.1). Then, for all possible baseline hazard function, I generated clustered data assuming 15 clusters of (30, 100) individuals each, 50 clusters of (30, 100) individuals each, or 1000 clusters of 2 individuals each. I included a binary treatment variable \\(X \\sim Bin(1, 0.5)\\) with associated log-hazard ratio of \\(-0.5\\) and cluster-specific frailty terms \\(\\alpha_i\\) following either a Gamma or a log-normal distribution with variance \\(\\theta\\) (\\(\\theta\\) = {0.25, 0.75, 1.25}, Figure 6.2). Finally, I generated an event indicator variable \\(d\\) by applying administrative censoring at 5 years. The true marginal survival functions corresponding to these simulated settings are depicted in Figure 6.3. I applied a fully factorial design: this resulted in 150 simulation scenarios, 5 sample sizes \\(\\times\\) 5 baseline hazards \\(\\times\\) 2 frailty distributions \\(\\times\\) 3 true frailty variances. Figure 6.1: Baseline hazard functions chosen for this simulation study. Figure 6.2: Frailty distributions chosen for this simulation study. Figure 6.3: Marginal survival depending on baseline hazard and frailty distribution chosen for this simulation study. References "],
["simst2-methods.html", "6.3 Methods", " 6.3 Methods In this Section I will introduce the models I fitted in this simulation study. First, I fit a Cox model with a shared frailty term: \\[ h_{ij}(t | \\alpha_i) = \\alpha_i h_0(t_{ij}) \\exp(X_{ij} \\beta), \\] with \\(h_0(\\cdot)\\) left unspecified. The Cox model with a shared Gamma frailty is implemented in the R package frailtyEM, while the Cox model with a shared log-normal frailty is implemented in the R package coxme. Since the coxme package does not return a standard error for the estimated frailty variance by default, I used bootstrap with 1000 replications to estimate it; I resampled clusters of individuals rather than individuals to preserve the correlation within a cluster. Then, I fitted fully parametric survival models with a shared frailty term, using the same model formulation of the Cox model but specifying the baseline hazard function. I fitted six models, for each combination of baseline hazard (Exponential, Weibull, or Gompertz) and frailty distribution (Gamma, log-normal). These models are implemented in the R package parfm. Finally, I fit flexible Royston-Parmar models with a shared frailty term, either Gamma or log-normal: \\[ \\log H(t_{ij}|\\alpha_i) = s(z; \\gamma) + X \\beta + \\log(\\alpha_i), \\] with \\(s(\\cdot)\\) a restricted cubic spline function of log-time that smooths the logarithm of the baseline cumulative hazard \\(H_0(\\cdot)\\). This model requires choosing the number of degrees of freedom of the spline term, hence I varied between 3, 5, 7, and 9 degrees of freedom. I also fitted the same model using penalised likelihood (X. Liu, Pawitan, and Clements 2016), applying a penalty to the likelihood to avoid both overfitting and having to choose the number of degrees of freedom for the spline term. These flexible parametric models are implemented in the R package rstpm2. References "],
["simst2-est.html", "6.4 Estimands", " 6.4 Estimands The first estimand of interest is the regression coefficient \\(\\beta\\) associated with the simulated binary treatment, to see if and how misspecification of the baseline hazard or frailty distribution affects relative risk estimates. Second, I am interested in comparing estimates of the frailty variance - and therefore of the unobserved heterogeneity - obtained from each model. Finally, I am going to compare two measures of absolute risk: marginal survival difference at time \\(t\\), defined as \\(S(t)_{\\text{diff}} = S(t | x = 1) - S(t | x = 0)\\); integrated marginal survival difference, defined as \\(iS_{\\text{diff}} = iS(x = 1) - iS(x = 0)\\). is obtained by fixing the time \\(t\\) (I am using 1, 2, 3, and 4 years), and then integrating out the frailty term from the conditional survival function as explained in Chapter 2. Conversely, (2) requires integrating the marginal survival function for both treatment groups and then computing their difference. I estimate it as follows: I estimate marginal survival for a treatment group at 1000 equally spaced values over the follow-up time \\(t\\); I fit an interpolating natural spline over the 1000 estimates from step (1) using the splinefun R function; I integrate the resulting spline function using Gauss-Kronrod quadrature as implemented in the integrate function; I compute the difference of the integrated marginal survival for the two treatment groups. The integral of a survival function can be interpreted as life expectancy; hence, the quantity I am computing can be interpreted as the difference in 5-years life expectancy between treated and untreated individuals. "],
["simst2-pm.html", "6.5 Performance measures", " 6.5 Performance measures Performance measures of interest are bias, coverage, and mean squared error - as in the previous simulation study of Chapter 5 (more details in Section 5.2). I will also include Monte Carlo standard errors for bias and coverage to quantify uncertainty in estimating such performance measures. "],
["simst2-res.html", "6.6 Results", " 6.6 Results Among the 150 different data-generating mechanisms I simulated data from, I chose (for conciseness) to present results only for the settings of 15 clusters of 100 individuals each, with a frailty variance of 0.25. I will also exclude Royston-Parmar models with 3 or 7 degrees of freedom from this comparison, again for conciseness. Additional results can be explored interactively by clicking here. Bias, coverage, and mean squared error of the estimated regression coefficient are presented in Tables A.7, A.8, A.9 and Figures B.7, B.8, B.9. With a true exponential baseline hazard, all models produced unbiased estimates under all scenarios; with a true Weibull baseline hazard, all models performed well except the parametric models with an exponential or Gompertz baseline hazard, which yielded underestimated regression coefficients (approximately -0.09 on the log-hazard rate scale). Analogously, with a true Gompertz baseline hazard the parametric Gompertz model, the flexible parametric models, and the Cox models performed well with unbiased estimates; the parametric exponential and Weibull models yielded overestimated results (approximately 0.13). With the first Weibull-Weibull baseline hazard, the flexible parametric models and the Cox model performed well; conversely, the parametric models yielded overestimated results (exponential and Gompertz, approximately -0.07) or underestimated results (Weibull, approximately 0.10). Similarly, with the second Weibull-Weibull, the flexible parametric models and the Cox model returned unbiased estimates; the Weibull model returned unbiased results too. The exponential and Gompertz parametric models, on the other side, return underestimated results (approximately -0.11). Coverage followed a similar pattern; when the model yielded unbiased results, coverage was optimal at approximately 95%. Conversely, when the estimates were biased and the parametric distribution was misspecified or failed to capture a complex hazard shape, coverage dropped up to 35% (with the exponential model performing worst). Mean squared error of the estimated coefficients was the smallest when the model was well specified, or when using the Cox model or Royston-Parmar models. Bias, coverage, and mean squared error of the estimated frailty variance are presented in Tables A.10, A.11, A.12 and Figures B.10, B.11, B.12. With a true exponential baseline hazard, all models yielded slightly biased results irrespectively of the frailty distribution: models with a well-specified frailty distribution yielded slightly negatively biased results (-0.03 to -0.01; the Cox model with a Gamma frailty performed worse with a negative bias of -0.09). When assuming a Gamma frailty in place of a log-normal frailty, negative bias was somewhat greater (around -0.05, with the Cox model once again performing worse with a negative bias of -0.11); when assuming a log-normal frailty in place of a Gamma frailty, results were slightly positively biased (approximately 0.01). With more complex true baseline hazard functions, the flexible parametric models performed the best with performance similar to the exponential setting; conversely, fully parametric models performed worse when the baseline hazard was misspecified (with both negative and positive bias depending on the setting, up to -0.15 and 0.10). With a complex baseline hazard, negative and positive bias for the fully parametric models further increased up to -0.15 and 0.15, approximately. The Cox model with a Gamma frailty performed the worst, severely underestimating the frailty variance (up to -0.18, approximately). Coverage was generally suboptimal, in the range 70-90%, with a few exceptions; the fully parametric models showed good coverage at times, a symptom of overestimated standard errors (given that they returned biased estimates). Mean squared errors reflected the pattern observed for bias, with the flexible parametric models performing better than the other models across the range of frailty distributions and baseline hazards examined; the parametric models performed similarly when well specified, slightly worse otherwise. The Cox model with a log-normal frailty performed similarly to the Royston-Parmar models, while the Cox model with a Gamma frailty performed worse, especially with a complex baseline hazard (where it performed even worse than fully parametric models). Finally, bias and mean squared error of the estimated difference in 5-years life expectancy are presented in Tables A.13, A.14 and Figures B.13, B.14. With a true Gamma frailty, the flexible parametric models perform well with estimates of the difference in 5-years life expectancy that are practically unbiased; the parametric models showed good performance when well specified, returned slightly biased results otherwise (both negative and positive bias, up to -0.04 and 0.08 respectively - an absolute difference of 0.5 to 1.0 months in terms of time). With a true log-normal frailty distribution, the Royston-Parmar models produced slightly overestimated results (0.01 to 0.05), while the remaining models performed similarly to the setting with a true Gamma distribution. Bias slightly increased with a complex baseline hazard when using parametric models, up to 0.12 (i.e. approximately 1.5 months). Mean squared errors showed a similar pattern, with all models performing comparably with a true exponential or Gompertz baseline hazard, and the flexible parametric models performing best otherwise (compared to misspecified models). The Cox model generally performed similarly or slightly worse than the flexible parametric models. "],
["simst2-conclusions.html", "6.7 Conclusions", " 6.7 Conclusions I showed that estimates of regression coefficients, frailty variance, and difference in expectation of life are relatively insensitive to misspecification of the frailty distribution of the model. Conversely, misspecifying the baseline hazard has serious consequences as it impacts both relative and absolute measures of risk, and estimates of heterogeneity. This seems to be particularly important with respect to the regression coefficients, as bias on the log-hazard ratio scale of up to 0.13 corresponds to a difference of approximately 15% on the relative risk scale, a clinically meaningful difference. All models seemed to produced biased estimates of the frailty variance, which may be due to the small number of cluster examined here; exploring additional scenarios will provide a greater insight on the topic. The bias in the difference of 5-years expectation of life seems to be less clinically relevant (bias up to 1.5 months), but it is something to bear in mind nonetheless. The fully parametric models perform well (as expected) when well specified, but relatively simple hazard forms may be too restrictive and unrealistic in practice; conversely, flexible parametric models showed robustness to all different shapes of the baseline hazards and generally performed best, even compared to the Cox model. Further to that, this robustness seemed to be independent of the number of knots for modelling the baseline hazard and on the estimation method (full or penalised likelihood). "],
["sirex.html", "7 Exploring results from simulation studies interactively", " 7 Exploring results from simulation studies interactively The simulation studies I presented in Chapters 5 and 6 presented multiple challenges, one of them being how to present the results effectively given the amount of simulated scenarios (108 and 150 scenarios for simulation 1 and 2, respectively). Each scenario would require producing a variety of tables and plot for bias, coverage, and any other summary statistics I may be interested in: the amount of tables and plots would grow dramatically to an unsustainable number. A straightforward (and often used in practice) option could be selecting a handful of scenarios to present, limiting the number of tables and plots to what is believed to be most interesting. However, a reader may find other scenarios more interesting, or would like to compare different factors or even deep down more into the results; presenting only a subset of results may result limiting to some extent then. Therefore, facing this problem myself, I set out to develop an interactive tool to aid exploration and dissemination of results from simulation studies. Part of this work was presented in oral form at the Students’ Day of the 38th Conference of the International Society for Clinical Biostatistics; slides are attached in Appendix C.3. Simulation studies represent a powerful tool with a multiplicity of aims: among others, evaluating new or existing statistical methods, comparing them, assessing the impact of modelling assumption violations, and helping with the understanding of statistical concepts. The increased availability of powerful computational tools (both personal and high-performance cluster computers) to the average researcher surely contributed to the rise of simulation studies in the current literature. Searching on PubMed and Scopus with the query “simulation study” it is indeed possible to appreciate the greater use of this tool (Figure 7.1). Additionally, increased computational capabilities allow researchers to simulate an ever-growing number of scenarios, exploring multiple data-generating mechanisms, factors, and methods at once - making reporting results a non-trivial task. Figure 7.1: Numer of results querying ‘simulation study’ on Pubmed and Scopus. It is necessary to bear in mind that dissemination of results plays a focal role in simulation studies: it can drive practitioners and applied statisticians to methods that have been shown to perform well in their practical settings (e.g.: small sample size, high proportion of missing values); it can guide researchers to develop new methods in a promising direction; it can provide insights into less established methods. As a consequence, several design and reporting guidelines emerged, often tailor-made to a specific research area (e.g. health technology assessment, medical statistics, social sciences). Despite that, challenges still persist and further research is needed into methods to help reporting of results. To bridge the gap between the number of scenarios a researcher can simulate from and dissemination of results, I developed a tool for exploring results interactively. The tool is developed using R and shiny (https://shiny.rstudio.com/), a web application framework for R that allows creating interactive web applications in a straightforward way. I named the interactive tool SiReX, acronym for Simulation Results eXplorer. It requires the researcher to upload a dataset in a standardised, tidy format (observations are in rows, variables are in columns) containing results from a simulation study. Then, it computes performance measures such as bias, coverage probability, Monte Carlo errors, and empirical standard errors automatically. Finally, it presents results and performance summaries both in tabular and graphical fashion (via bar plots and lolly plots) and allows the reader to vary simulation parameters and choose estimands of interest for further investigations. A typical workflow with SiReX would consist of the following steps: Upload a dataset with results from a simulation study in a tidy format compatible with the tool (Figure 7.2); Figure 7.2: SiReX: load a dataset Summary statistics are computed automatically (Figure 7.3); Figure 7.3: SiReX: table with summary statistics, computed automatically Factors identifying different data-generating mechanisms are identified automatically and drop-down menus are populated appropriately; Now, it is possible to select and change data-generating mechanisms: summary tables and plots are updated automatically (Figure 7.4); Figure 7.4: SiReX: plots are created and updated automatically Exporting summary statistics, tables, and plots for later use is supported (Figures 7.5 and 7.6). Figure 7.5: SiReX: export tables in LaTeX format Figure 7.6: SiReX: export plots A current live demo of the tool is available at https://ag475.shinyapps.io/sirex-demo/, using an example dataset from a simulation study on multiple imputation. "],
["infvp.html", "8 Informative visiting process", " 8 Informative visiting process Health care consumption data is being used increasingly often in medical research, a successful example being the CArdiovascular disease research using LInked Bespoke studies and Electronic health Records (CALIBER) programme (Denaxas et al. 2012). CALIBER was constructed by extracting and linking electronic health records from primary care, hospital care, and nationwide registries (including information such as social deprivation and living status), and it consists of more than 10 million adults with 400 million person-years of follow-up; this vast amount of data allows researchers to answer more relevant and detailed clinical questions, but poses new methodological challenges. First and foremost, in observational, health care consumption data observation times are likely to be correlated with the underlying disease severity. For instance, individuals tend to have irregular observation times as patients with more severe conditions (or showing early symptoms of a disease) tend to visit their GP or go to the hospital more often than those with milder conditions (and no symptoms). Their worse disease status is also likely to be reflected in worse biomarkers being recorded as such visits, causing abnormal values of such biomarkers to be overrepresented and normal values to be underrepresented. Additionally, for diseases with a high mortality rate, a terminal event that truncates observation of the longitudinal process is likely to be informative in the sense that it likely correlates with disease severity. That is, dropout is likely to be informative as the tendency to drop out after the occurrence of a terminal event is related to the current level of the longitudinally recorded biomarker. Traditional methods used to analyse longitudinal data rely on the assumption that the underlying mechanism that controls the observation time is independent of disease severity; however, as I mentioned before, that is unlikely with health care consumption data. It can be showed that failing to account for informative dropout in a longitudinal study could yield biased estimates of the model parameters (Wu and Carroll 1988). Analogously, an explicit assumption when jointly modelling longitudinal and survival data (using the framework presented in Chapter 3) is that the timing and number of measurements for each subject should be non-informative, i.e. it does not associate with the survival part of the model. However, it is currently unknown whether violations of this assumption lead to invalid inference or not in the context of joint models. The topic of informative observation times and informative censoring has been the object of recent investigations. L. Liu, Huang, and O’Quigley (2008) developed a model that analyses repeated measures by taking into account informative observation times and an informative terminal event at the same time; they proposed a joint model formed by three submodels, a frailty model for the observation times, a mixed effects model for the longitudinal process, and a proportional hazards model for the terminal event. P. Ghosh, Ghosh, and Tiwari (2011) proposed a joint longitudinal and survival model that handles multiple change points in the longitudinal profile by including random spline coefficients; the survival part of the models handles an informative terminal event by using a semiparametric Cox model. Han, Song, and Sun (2014) proposed a model for the joint distribution of the longitudinal process, the observation process, and the dropout process that uses, respectively, a semiparametric linear regression model for the longitudinal data and two accelerated failure time models for the observation and dropout process; their model is semiparametric in the sense that it leaves the distributional form and the dependence structure unspecified. Lesperance et al. (2015) developed a joint model within the multi-state framework that handles examination times correlated with disease progression; they link transition intensities of a Markov model with a log-linear mixed model governing observation times through shared random effects. However, they do not integrate out the shared random effects from the model as their target of inference is the transition intensities conditional on the random effects. Analogously, there has been quite a lot of developments in the multi-state framework to handle informative visiting times and/or dropout. Sweeting, Farewell, and De Angelis (2010) developed a model (similar to a hidden Markov model) in the setting of a response variable irregularly and infrequently observed by conditioning on regularly collected auxiliary data. Lange et al. (2015) generalise the work of Sweeting, Farewell, and De Angelis (2010) to better fit observational data settings rather than clinical trial data with informative missingness or observation times by modelling the disease process with a Markov model and the observation process with a Poisson process that depends on the underlying disease status. Lawless and Nazeri Rad (2015) consider the effect of intermittent, irregular observation times on the estimation of Markov models; they show that it is not possible to estimate transition intensities in bi-directional Markov models with good precision when the gap between observation times is too big. They also show how the correlation between visit times and observed disease status can bias model assessment procedures, and propose an inverse intensity weighted estimation procedure for state prevalence. In brief, this approach consists in weighting each individual by their probability of being observed (or measured) at a given point in time; they discuss and develop this further in (Nazeri Rad and Lawless 2017). Finally, Li and Su (2017) proposed a joint model for a longitudinal outcome and semicompeting risk data such as study dropout and death; they model the longitudinal process using a mixed model, and the semicompeting risks using two separate probit models. In conclusion, joint models for longitudinal and survival data can handle effectively informative dropout in longitudinal study by modelling the longitudinal trajectory and the dropout process jointly. However, it is not clear whether presence of an informative visiting process affects inference from joint models. Further investigating this topic will form a good part of work planned for the second year of my PhD, as I will discuss in Chapter 9. This project will have important practical implications, as it aims to provide guidance to practitioners and applied researchers using joint models for longitudinal and survival data with their observational data. References "],
["future.html", "9 Future research developments", " 9 Future research developments In this Chapter I will present my plans for research during the second year of my PhD. I also include a Gantt chart laying out the projects planned for years 2 and 3 of my PhD Figure 9.1. First of all, I plan to conclude the simulation study on the impact of model misspecification in survival models with frailty terms. Simulations for some scenarios are currently still running on the University high-performance computing (HPC) facilities at the time of writing, and should finish soon. Consequently, I will summarise relevant results and I will finish writing up the project into a paper that will be submitted to a journal for publication by the end of the year. Current potential target journals are the Biometrical Journal and Statistics in Medicine. At the same time, I will be planning the next project on modelling the visiting process and investigating the impact of an informative visiting process on inference using longitudinal data originating from healthcare records. This project aims to shed light on how an informative visiting process affects the analysis of longitudinal data that are intermittently and irregularly measured and recorded, in order to provide practical advice to applied researchers. In practice, I will be simulating complex survival data along with one or more longitudinal biomarkers under a variety of biologically plausible data-generating mechanisms; for instance, I will vary: the underlying risk of event, in terms of magnitude and shape of the baseline hazard; the number and frequency of longitudinal measurements; features of the longitudinal process such as functional form over time; the strength of the informative observation process, i.e. the magnitude of the association between the underlying disease process and the observation process; the shape of the association between the observation process and the disease process, that is, the parametrisation that links the two processes (e.g. current value parametrisation, intercept and slope, cumulative effect, etc.). Then, I will compare different analytical approaches proposed in the literature to tackle the problem, starting with simple approaches such as including the number of preceding measurements in the model as a proxy of disease severity moving onto more complex methods such as those introduced in Section 8. Some methods will be directly applicable using existing software, while others will require developing ad-hoc software if an existing implementation is not available in standard statistical software. Throughout this project we will be collaborating with Dr. Jessica Barrett from the MRC Biostatistic Unit in Cambridge to discuss factors that may affect the results of this study and what methods to include and compare. Once the planning phase is completed, I will code the simulation study and run it using the HPC facilities of the University. Finally, I plan to write up the project into a manuscript approximately at the end of my second year. During my second year I will also work on two additional projects. First, an applied project in the area of cardiovascular epidemiology using joint models for multiple longitudinal biomarkers and survival data using CALIBER data (Denaxas et al. 2012). CALIBER includes a wide variety of biomarkers such as systolic and diastolic blood pressure, body-mass index, high- and low-density cholesterol, and so on. These biomarkers are likely to be correlated as they change over time, and they may improve cardiovascular risk prediction and clear up the complex relationship between changes in the biomarkers and the risk of adverse events. Specifically, I will select a cohort of individuals with stable angina and type 2 diabetes (for whom regular monitoring of blood pressure is recommended) and evaluate the association between multiple, longitudinally measured biomarkers and the risk of adverse coronary events, fatal and non-fatal. Second, I will be collaborating Dr. Michael Harhay from the Department of Epidemiology and Biostatistics at the University of Pennsylvania to develop a tutorial on using joint models for longitudinal and survival data in applied settings; specifically, we will focus on the setting of intensive care medicine and we will include practical examples using real data from clinical trials. The applied project using CALIBER data will inform further projects if time allows it. A possible project could consist in studying and developing discrimination and calibration tools to use with multiple longitudinal biomarkers. Such tools are really important, as it is fundamental to be able to discern whether the addition of longitudinal biomarkers improves predictions or not. This project would have wide reaching consequences, as it would provide guidance in the use of joint models for longitudinal and survival data with multiple longitudinal biomarkers when the aim is prediction. I aim to work on this during the final year of my PhD. Finally, an ongoing task will be the continuous development and expansion of the interactive tool for exploring results from simulation studies. Potential developments will include: polishing the underlying engine used to computed summary statistics; including more plots; allowing custom faceted plots and tables comparing multiple factors at once. I aim to produce a polished version of SiReX to publish on-line to a wider audience as soon as possible, and write a manuscript presenting the tool for submission to a journal such as the Journal of Statistical Software. Figure 9.1: Gantt chart for current and future projects. References "],
["pdevelop.html", "10 Personal development", " 10 Personal development In this chapter I will introduce and briefly discuss the personal development activities I carried out during the first year of my PhD; specifically, I will present the supervisory meetings, training courses, and conferences I attended. "],
["supervisory-meetings.html", "10.1 Supervisory meetings", " 10.1 Supervisory meetings I have been having frequent meetings with my supervisors, formally and informally. Formal supervisory meetings, recorded on PROSE (https://prose.le.ac.uk), have been held on average every other week, with summaries of what was discussed produced and shared between us. A comprehensive list is available on PROSE. Additionally, we held more frequent informal meetings to discuss developments and more urgent matters, whenever it was necessary. "],
["training-and-courses.html", "10.2 Training and courses", " 10.2 Training and courses I have attended a wide variety of courses during my first year, both externally and internally to the University of Leicester. The external courses I attended are: Efficient R Programming, on November 8th 2016, organised by the Royal Statistical Society in London. The instructor was Dr. Colin Gillespie, from the University of Newcastle, United Kingdom, and Jumping Rivers. The course covered how to program efficiently with R; in particular, it covered common pitfalls when writing R code, code profiling, interfacing with C++, and parallel programming. General hints and tips were provided. Introduction to causal inference, on April 25th and 26th 2017, organised by the Biostatistics Research Group at the University of Leicester and delivered by Dr. Arvid Sjölander from Karolinska Institutet, Stockholm, Sweden. The course provided foundational concepts of causal inference such as the difference between association and causation, the counterfactual framework, exchangeability, directed acyclic graphs, methods for estimating a causal effect, etc. Additionally, it provided an introduction to more advanced methods such as instrumental variables and Mendelian randomisation. Using simulation studies to evaluate statistical methods, on May 22nd 2017, organised by University College London. The course was delivered by Dr. Tim Morris, Prof. Ian White and Dr. Michael Crowther, and it covered the rationale for using simulation studies, important concepts to keep in mind when planning a simulation study, computational tools, estimates of uncertainty, and tools for improving reporting and dissemination. Workshop on Joint modelling of longitudinal and time-to-event data with R, on July 5th, 2017, organised by the Department of Biostatistics of the University of Liverpool. The course was delivered by Dr. Graeme Hickey and provided an introduction to joint models of longitudinal and survival data, including extensions to incorporate competing risks and multiple longitudinal processes. The course included a practical session using R. I have attended a few courses within the University and not offered on PROSE; specifically, I attended a course on Time series analysis with R (November 10th, 2016), a course on Data visualisation (November 15th, 2016), and a course on High performance computing at Leicester (February 8th, 2017). The latter was particularly important, as it allowed me to take advantage of the high-performance computing facilities offered by the University more efficiently. I also attended the Preparing to teach in higher education workshop, strand A (July 24th and 27th 2017). Finally, I attended the following PROSE training sessions to develop personal and communication skills in research settings: Planning your literature search, October 21st 2016; Conducting your literature search, October 25th 2016 ; Assertiveness, November 14th 2016; Introduction to critical thinking, December 15th 2016; Presentations A: Fundamentals of an effective presentation, January 30th 2017; Communication in research and other work settings, January 31st 2017; Enhancing your digital profile, February 2nd 2017; Saying it with your abstract, February 10th 2017; Designing a poster, February 27th 2017; Leadership in research and other work environments, February 28th 2017; Preparing for the probation review (Physical natural and medical sciences), May 30th 2017. "],
["conferences.html", "10.3 Conferences", " 10.3 Conferences I attended a number of conferences during this year, in which I delivered the following oral presentations: Survival Analysis for Junior Researchers conference, held in Leicester, UK, on April 5th and 6th 2017. I delivered a talk titled Direct likelihood maximisation using numerical quadrature to approximate intractable terms; Statistical Analysis of Multi-Outcome Data (SAM) conference, held in Liverpool, UK, on July 3rd and 4th 2017. I delivered a talk titled Impact of model misspecification in survival models with frailties; Annual Conference of the International Society for Clinical Biostatistics conference, held in Vigo, Spain, on July 9th to July 13th 2017. I delivered two talks: a titled Impact of model misspecification in survival models with frailties during one of the contributed sessions on survival analysis, and a talk titled Exploring results from simulation studies interactively during the Students’ Day organised on July 13th. Additionally, I delivered an oral presentation on previous research work external to my PhD project during the 54th ERA-EDTA Congress held in Madrid, Spain, between June 3rd and June 6th. The ERA-EDTA Congress is the main conference in the field of Nephrology in Europe, with approximately 10,000 participants in 2017. I delivered my presentation, titled Inappropriate prescription of nephrotoxic drugs to individuals with chronic kidney disease, to an audience of clinicians, epidemiologists, clinical researchers, and other stakeholders. "],
["ax-tables.html", "A Tables", " A Tables Table A.1: Bias, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 0.002 (0.002) 0.001 (0.001) -0.003 (0.002) -0.017 (0.008) 1000c. of 2i. IN 0.000 (0.002) 0.000 (0.001) -0.003 (0.002) -0.032 (0.008) 1000c. of 2i. GQ15 0.011 (0.002) 0.005 (0.001) -0.007 (0.002) 0.086 (0.003) 1000c. of 2i. GQ35 0.002 (0.002) 0.001 (0.001) -0.003 (0.002) -0.011 (0.008) 1000c. of 2i. GQ75 0.002 (0.002) 0.001 (0.001) -0.003 (0.002) -0.017 (0.008) 1000c. of 2i. GQ105 0.002 (0.002) 0.001 (0.001) -0.003 (0.002) -0.017 (0.008) 15c. of 100i. AF -0.008 (0.004) 0.002 (0.001) -0.001 (0.002) -0.146 (0.013) 15c. of 100i. IN -0.007 (0.004) 0.002 (0.001) -0.001 (0.002) -0.146 (0.013) 15c. of 100i. GQ15 -0.165 (0.008) 0.003 (0.001) -0.002 (0.002) 0.223 (0.013) 15c. of 100i. GQ35 -0.086 (0.006) 0.002 (0.001) -0.001 (0.002) -0.028 (0.013) 15c. of 100i. GQ75 -0.040 (0.005) 0.001 (0.001) -0.001 (0.002) -0.114 (0.013) 15c. of 100i. GQ105 -0.008 (0.005) 0.002 (0.001) -0.001 (0.002) -0.130 (0.013) 15c. of 30i. AF -0.010 (0.005) 0.002 (0.002) -0.007 (0.004) -0.215 (0.017) 15c. of 30i. IN -0.008 (0.005) 0.002 (0.002) -0.008 (0.004) -0.215 (0.017) 15c. of 30i. GQ15 -0.053 (0.007) 0.003 (0.002) -0.007 (0.004) -0.025 (0.014) 15c. of 30i. GQ35 -0.003 (0.005) 0.002 (0.002) -0.008 (0.004) -0.194 (0.021) 15c. of 30i. GQ75 -0.010 (0.005) 0.002 (0.002) -0.007 (0.004) -0.216 (0.017) 15c. of 30i. GQ105 -0.010 (0.005) 0.002 (0.002) -0.007 (0.004) -0.216 (0.017) 15c. of 500i. AF -0.011 (0.004) 0.000 (0.000) 0.001 (0.001) -0.142 (0.012) 15c. of 500i. IN -0.049 (0.005) 0.000 (0.000) 0.001 (0.001) -0.067 (0.013) 15c. of 500i. GQ15 -0.225 (0.009) 0.001 (0.000) 0.001 (0.001) 0.317 (0.015) 15c. of 500i. GQ35 -0.145 (0.006) 0.001 (0.000) 0.001 (0.001) 0.073 (0.013) 15c. of 500i. GQ75 -0.109 (0.005) 0.000 (0.000) 0.001 (0.001) -0.033 (0.012) 15c. of 500i. GQ105 -0.101 (0.005) 0.000 (0.000) 0.001 (0.001) -0.057 (0.012) 50c. of 100i. AF 0.000 (0.002) 0.000 (0.001) -0.002 (0.001) -0.044 (0.007) 50c. of 100i. IN 0.001 (0.002) 0.000 (0.001) -0.002 (0.001) -0.045 (0.007) 50c. of 100i. GQ15 -0.207 (0.006) 0.001 (0.001) -0.003 (0.001) 0.316 (0.009) 50c. of 100i. GQ35 -0.088 (0.004) 0.000 (0.001) -0.002 (0.001) 0.062 (0.007) 50c. of 100i. GQ75 -0.020 (0.003) 0.000 (0.001) -0.002 (0.001) -0.025 (0.007) 50c. of 100i. GQ105 -0.002 (0.003) 0.000 (0.001) -0.002 (0.001) -0.036 (0.007) 50c. of 30i. AF -0.002 (0.003) 0.000 (0.001) -0.002 (0.002) -0.046 (0.008) 50c. of 30i. IN -0.002 (0.003) 0.000 (0.001) -0.004 (0.002) -0.043 (0.008) 50c. of 30i. GQ15 -0.045 (0.004) 0.001 (0.001) -0.003 (0.002) 0.082 (0.007) 50c. of 30i. GQ35 -0.002 (0.003) 0.000 (0.001) -0.002 (0.002) -0.038 (0.008) 50c. of 30i. GQ75 -0.002 (0.003) 0.000 (0.001) -0.002 (0.002) -0.046 (0.008) 50c. of 30i. GQ105 -0.002 (0.003) 0.000 (0.001) -0.002 (0.002) -0.046 (0.008) Table A.2: Coverage, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 94.20 (0.74) 95.50 (0.66) 95.00 (0.69) 96.50 (0.58) 1000c. of 2i. IN 93.42 (0.78) 95.70 (0.64) 93.02 (0.81) 77.45 (1.32) 1000c. of 2i. GQ15 93.60 (0.77) 95.40 (0.66) 95.10 (0.68) 98.70 (0.36) 1000c. of 2i. GQ35 94.29 (0.73) 95.10 (0.68) 95.30 (0.67) 95.90 (0.63) 1000c. of 2i. GQ75 94.20 (0.74) 95.50 (0.66) 95.00 (0.69) 96.50 (0.58) 1000c. of 2i. GQ105 94.20 (0.74) 95.50 (0.66) 95.00 (0.69) 96.50 (0.58) 15c. of 100i. AF 92.40 (0.84) 95.50 (0.66) 94.70 (0.71) 92.60 (0.83) 15c. of 100i. IN 90.65 (0.92) 95.48 (0.66) 94.45 (0.72) 92.39 (0.84) 15c. of 100i. GQ15 27.80 (1.42) 94.60 (0.71) 94.90 (0.70) 86.70 (1.07) 15c. of 100i. GQ35 44.04 (1.57) 94.79 (0.70) 94.89 (0.70) 93.99 (0.75) 15c. of 100i. GQ75 66.00 (1.50) 95.10 (0.68) 95.10 (0.68) 93.00 (0.81) 15c. of 100i. GQ105 76.20 (1.35) 95.50 (0.66) 94.80 (0.70) 92.50 (0.83) 15c. of 30i. AF 92.70 (0.82) 95.00 (0.69) 94.70 (0.71) 94.50 (0.72) 15c. of 30i. IN 92.21 (0.85) 94.91 (0.69) 94.08 (0.75) 93.15 (0.80) 15c. of 30i. GQ15 68.10 (1.47) 94.70 (0.71) 94.70 (0.71) 93.10 (0.80) 15c. of 30i. GQ35 88.91 (0.99) 94.61 (0.71) 94.20 (0.74) 95.23 (0.67) 15c. of 30i. GQ75 92.50 (0.83) 95.00 (0.69) 94.70 (0.71) 94.10 (0.75) 15c. of 30i. GQ105 92.70 (0.82) 95.00 (0.69) 94.60 (0.71) 94.60 (0.71) 15c. of 500i. AF 92.50 (0.83) 95.40 (0.66) 95.70 (0.64) 91.60 (0.88) 15c. of 500i. IN 31.74 (1.47) 95.02 (0.69) 95.63 (0.65) 91.25 (0.89) 15c. of 500i. GQ15 11.00 (0.99) 92.50 (0.83) 94.90 (0.70) 80.40 (1.26) 15c. of 500i. GQ35 14.20 (1.10) 93.80 (0.76) 95.70 (0.64) 91.50 (0.88) 15c. of 500i. GQ75 18.40 (1.23) 94.80 (0.70) 95.90 (0.63) 93.40 (0.79) 15c. of 500i. GQ105 22.00 (1.31) 95.20 (0.68) 95.40 (0.66) 93.20 (0.80) 50c. of 100i. AF 95.40 (0.66) 94.90 (0.70) 94.30 (0.73) 93.30 (0.79) 50c. of 100i. IN 93.07 (0.80) 94.56 (0.72) 93.82 (0.76) 91.68 (0.87) 50c. of 100i. GQ15 17.20 (1.19) 94.30 (0.73) 94.70 (0.71) 60.60 (1.55) 50c. of 100i. GQ35 37.80 (1.53) 94.60 (0.71) 94.30 (0.73) 91.50 (0.88) 50c. of 100i. GQ75 69.80 (1.45) 94.80 (0.70) 94.50 (0.72) 93.70 (0.77) 50c. of 100i. GQ105 82.90 (1.19) 94.90 (0.70) 94.20 (0.74) 93.60 (0.77) 50c. of 30i. AF 93.80 (0.76) 95.30 (0.67) 95.30 (0.67) 95.40 (0.66) 50c. of 30i. IN 92.65 (0.83) 95.28 (0.67) 94.73 (0.71) 93.30 (0.79) 50c. of 30i. GQ15 71.90 (1.42) 95.00 (0.69) 95.10 (0.68) 92.60 (0.83) 50c. of 30i. GQ35 92.00 (0.86) 95.10 (0.68) 95.10 (0.68) 93.90 (0.76) 50c. of 30i. GQ75 93.80 (0.76) 95.40 (0.66) 95.20 (0.68) 95.40 (0.66) 50c. of 30i. GQ105 93.70 (0.77) 95.30 (0.67) 95.30 (0.67) 95.40 (0.66) Table A.3: Mean squared error, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 0.0026 0.0008 0.0042 0.0670 1000c. of 2i. IN 0.0024 0.0008 0.0041 0.0635 1000c. of 2i. GQ15 0.0024 0.0007 0.0042 0.0159 1000c. of 2i. GQ35 0.0025 0.0008 0.0042 0.0571 1000c. of 2i. GQ75 0.0026 0.0008 0.0042 0.0668 1000c. of 2i. GQ105 0.0026 0.0008 0.0042 0.0669 15c. of 100i. AF 0.0190 0.0009 0.0045 0.1886 15c. of 100i. IN 0.0193 0.0009 0.0046 0.1880 15c. of 100i. GQ15 0.0867 0.0009 0.0046 0.2116 15c. of 100i. GQ35 0.0432 0.0009 0.0046 0.1621 15c. of 100i. GQ75 0.0274 0.0009 0.0045 0.1816 15c. of 100i. GQ105 0.0242 0.0009 0.0045 0.1877 15c. of 30i. AF 0.0262 0.0029 0.0157 0.3341 15c. of 30i. IN 0.0259 0.0029 0.0158 0.3353 15c. of 30i. GQ15 0.0517 0.0029 0.0161 0.2038 15c. of 30i. GQ35 0.0283 0.0029 0.0159 0.4801 15c. of 30i. GQ75 0.0263 0.0029 0.0157 0.3365 15c. of 30i. GQ105 0.0262 0.0029 0.0157 0.3414 15c. of 500i. AF 0.0169 0.0002 0.0009 0.1691 15c. of 500i. IN 0.0284 0.0002 0.0009 0.1835 15c. of 500i. GQ15 0.1315 0.0002 0.0009 0.3364 15c. of 500i. GQ35 0.0599 0.0002 0.0009 0.1638 15c. of 500i. GQ75 0.0394 0.0002 0.0009 0.1486 15c. of 500i. GQ105 0.0345 0.0002 0.0009 0.1477 50c. of 100i. AF 0.0053 0.0003 0.0014 0.0487 50c. of 100i. IN 0.0054 0.0003 0.0014 0.0488 50c. of 100i. GQ15 0.0816 0.0003 0.0014 0.1827 50c. of 100i. GQ35 0.0242 0.0003 0.0014 0.0555 50c. of 100i. GQ75 0.0095 0.0003 0.0014 0.0480 50c. of 100i. GQ105 0.0076 0.0003 0.0014 0.0491 50c. of 30i. AF 0.0073 0.0009 0.0047 0.0628 50c. of 30i. IN 0.0074 0.0009 0.0047 0.0629 50c. of 30i. GQ15 0.0180 0.0009 0.0048 0.0560 50c. of 30i. GQ35 0.0082 0.0009 0.0047 0.0647 50c. of 30i. GQ75 0.0073 0.0009 0.0047 0.0627 50c. of 30i. GQ105 0.0073 0.0009 0.0047 0.0628 Table A.4: Bias, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 0.0187 (0.0012) -0.0415 (0.0008) 0.0149 (0.0019) -0.3010 (0.0250) 1000c. of 2i. GQ35 0.0188 (0.0012) -0.0415 (0.0008) 0.0144 (0.0019) -0.2842 (0.0232) 1000c. of 2i. GQ75 0.0188 (0.0012) -0.0415 (0.0008) 0.0146 (0.0019) -0.3057 (0.0254) 1000c. of 2i. GQ105 0.0187 (0.0012) -0.0415 (0.0008) 0.0147 (0.0019) -0.2946 (0.0243) 15c. of 100i. GQ15 0.0102 (0.0040) -0.0332 (0.0010) 0.0552 (0.0040) 0.0282 (0.0113) 15c. of 100i. GQ35 0.0102 (0.0040) -0.0333 (0.0010) 0.0173 (0.0024) -0.0854 (0.0085) 15c. of 100i. GQ75 0.0102 (0.0040) -0.0333 (0.0010) 0.0088 (0.0022) -0.1015 (0.0075) 15c. of 100i. GQ105 0.0102 (0.0040) -0.0333 (0.0010) 0.0088 (0.0022) -0.1015 (0.0075) 15c. of 30i. GQ15 0.0193 (0.0045) -0.0348 (0.0017) 0.0047 (0.0039) -0.2510 (0.0272) 15c. of 30i. GQ35 0.0192 (0.0045) -0.0348 (0.0017) 0.0041 (0.0039) -0.2455 (0.0256) 15c. of 30i. GQ75 0.0191 (0.0045) -0.0348 (0.0017) 0.0041 (0.0039) -0.2593 (0.0275) 15c. of 30i. GQ105 0.0195 (0.0045) -0.0347 (0.0017) 0.0040 (0.0039) -0.2522 (0.0265) 15c. of 500i. GQ15 0.0113 (0.0037) -0.0411 (0.0006) 0.0867 (0.0050) 0.2427 (0.0104) 15c. of 500i. GQ35 0.0157 (0.0038) -0.0423 (0.0006) 0.0860 (0.0040) 0.4011 (0.0091) 15c. of 500i. GQ75 0.0078 (0.0036) -0.0405 (0.0005) 0.0707 (0.0033) 0.4020 (0.0112) 15c. of 500i. GQ105 0.0059 (0.0036) -0.0410 (0.0006) 0.0600 (0.0028) 0.3119 (0.0125) 50c. of 100i. GQ15 0.0135 (0.0022) -0.0365 (0.0005) 0.0244 (0.0020) -0.0095 (0.0049) 50c. of 100i. GQ35 0.0135 (0.0022) -0.0365 (0.0005) 0.0102 (0.0012) -0.0350 (0.0038) 50c. of 100i. GQ75 0.0135 (0.0022) -0.0365 (0.0005) 0.0105 (0.0012) -0.0350 (0.0037) 50c. of 100i. GQ105 0.0135 (0.0022) -0.0365 (0.0005) 0.0105 (0.0012) -0.0350 (0.0037) 50c. of 30i. GQ15 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0535 (0.0052) 50c. of 30i. GQ35 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0536 (0.0052) 50c. of 30i. GQ75 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0536 (0.0052) 50c. of 30i. GQ105 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0536 (0.0052) Table A.5: Coverage, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 92.5662 (0.8295) 64.8676 (1.5096) 94.5010 (0.7209) 99.0835 (0.3013) 1000c. of 2i. GQ35 92.4413 (0.8359) 64.7600 (1.5107) 94.6885 (0.7092) 99.0807 (0.3018) 1000c. of 2i. GQ75 92.4720 (0.8343) 64.6999 (1.5113) 94.6083 (0.7142) 99.0844 (0.3012) 1000c. of 2i. GQ105 92.5586 (0.8299) 64.8318 (1.5100) 94.5973 (0.7149) 99.0826 (0.3015) 15c. of 100i. GQ15 52.1000 (1.5797) 76.4000 (1.3428) 86.0000 (1.0973) 70.2000 (1.4464) 15c. of 100i. GQ35 52.1000 (1.5797) 76.5000 (1.3408) 98.8000 (0.3443) 88.9000 (0.9934) 15c. of 100i. GQ75 52.1000 (1.5797) 76.4000 (1.3428) 99.9000 (0.0999) 94.0000 (0.7510) 15c. of 100i. GQ105 52.1000 (1.5797) 76.4000 (1.3428) 99.9000 (0.0999) 94.1000 (0.7451) 15c. of 30i. GQ15 75.4016 (1.3619) 90.0602 (0.9461) 99.0964 (0.2992) 98.3936 (0.3976) 15c. of 30i. GQ35 75.3769 (1.3624) 90.2513 (0.9380) 99.0955 (0.2994) 99.1960 (0.2824) 15c. of 30i. GQ75 75.4263 (1.3614) 90.0702 (0.9457) 99.0973 (0.2991) 99.2979 (0.2640) 15c. of 30i. GQ105 75.5020 (1.3600) 90.0602 (0.9461) 99.0964 (0.2992) 99.2972 (0.2642) 15c. of 500i. GQ15 30.0813 (1.4503) 21.3415 (1.2956) 27.6423 (1.4143) 23.1707 (1.3342) 15c. of 500i. GQ35 27.8826 (1.4180) 17.8197 (1.2101) 37.1069 (1.5277) 15.7233 (1.1511) 15c. of 500i. GQ75 30.1053 (1.4506) 18.9474 (1.2392) 55.5789 (1.5713) 16.6316 (1.1775) 15c. of 500i. GQ105 29.1028 (1.4364) 18.3807 (1.2248) 67.8337 (1.4771) 32.1663 (1.4771) 50c. of 100i. GQ15 51.9000 (1.5800) 37.8000 (1.5333) 94.8000 (0.7021) 85.6000 (1.1102) 50c. of 100i. GQ35 51.9000 (1.5800) 37.5000 (1.5309) 100.0000 (0.0000) 95.0000 (0.6892) 50c. of 100i. GQ75 51.9000 (1.5800) 37.5000 (1.5309) 100.0000 (0.0000) 94.9000 (0.6957) 50c. of 100i. GQ105 51.9000 (1.5800) 37.5000 (1.5309) 100.0000 (0.0000) 94.9000 (0.6957) 50c. of 30i. GQ15 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) 50c. of 30i. GQ35 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) 50c. of 30i. GQ75 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) 50c. of 30i. GQ105 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) Table A.6: Mean squared error, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 0.0019 0.0024 0.0040 0.7180 1000c. of 2i. GQ35 0.0019 0.0024 0.0039 0.6198 1000c. of 2i. GQ75 0.0019 0.0024 0.0039 0.7394 1000c. of 2i. GQ105 0.0019 0.0024 0.0039 0.6752 15c. of 100i. GQ15 0.0164 0.0021 0.0189 0.1285 15c. of 100i. GQ35 0.0164 0.0021 0.0063 0.0790 15c. of 100i. GQ75 0.0164 0.0022 0.0048 0.0665 15c. of 100i. GQ105 0.0164 0.0022 0.0048 0.0665 15c. of 30i. GQ15 0.0208 0.0041 0.0152 0.8029 15c. of 30i. GQ35 0.0208 0.0040 0.0149 0.7164 15c. of 30i. GQ75 0.0208 0.0041 0.0149 0.8228 15c. of 30i. GQ105 0.0207 0.0041 0.0149 0.7636 15c. of 500i. GQ15 0.0141 0.0020 0.0326 0.1666 15c. of 500i. GQ35 0.0146 0.0021 0.0234 0.2438 15c. of 500i. GQ75 0.0131 0.0019 0.0160 0.2881 15c. of 500i. GQ105 0.0129 0.0020 0.0113 0.2542 50c. of 100i. GQ15 0.0048 0.0016 0.0046 0.0237 50c. of 100i. GQ35 0.0048 0.0016 0.0015 0.0153 50c. of 100i. GQ75 0.0048 0.0016 0.0015 0.0153 50c. of 100i. GQ105 0.0048 0.0016 0.0015 0.0153 50c. of 30i. GQ15 0.0066 0.0022 0.0043 0.0301 50c. of 30i. GQ35 0.0066 0.0022 0.0043 0.0300 50c. of 30i. GQ75 0.0066 0.0022 0.0043 0.0300 50c. of 30i. GQ105 0.0066 0.0022 0.0043 0.0300 Table A.7: Bias with Monte Carlo standard error of estimated regression coefficient, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox 0.001 (0.002) 0.007 (0.002) 0.009 (0.002) 0.013 (0.002) -0.007 (0.002) Gamma Gamma Exp -0.002 (0.002) -0.083 (0.003) 0.133 (0.002) 0.102 (0.002) -0.116 (0.002) Gamma Gamma Wei -0.002 (0.002) 0.002 (0.002) 0.034 (0.002) -0.067 (0.002) 0.000 (0.002) Gamma Gamma Gom -0.006 (0.002) -0.086 (0.003) -0.002 (0.002) 0.028 (0.002) -0.114 (0.002) Gamma Gamma RP(5) -0.002 (0.002) -0.001 (0.002) -0.002 (0.002) -0.002 (0.002) -0.001 (0.002) Gamma Gamma RP(9) -0.002 (0.002) -0.001 (0.002) -0.002 (0.002) -0.001 (0.002) -0.002 (0.002) Gamma Gamma RP(P) -0.002 (0.002) -0.001 (0.002) 0.003 (0.002) 0.000 (0.002) -0.000 (0.002) Log-Normal Gamma Cox 0.004 (0.002) -0.002 (0.002) 0.000 (0.002) -0.006 (0.002) 0.002 (0.002) Log-Normal Gamma Exp 0.001 (0.002) -0.085 (0.002) 0.136 (0.001) 0.100 (0.002) -0.115 (0.002) Log-Normal Gamma Wei 0.001 (0.002) -0.002 (0.002) 0.038 (0.002) -0.066 (0.002) 0.001 (0.002) Log-Normal Gamma Gom 0.001 (0.002) -0.085 (0.002) 0.003 (0.002) 0.022 (0.002) -0.115 (0.002) Log-Normal Gamma RP(5) 0.002 (0.002) -0.002 (0.002) 0.003 (0.002) 0.001 (0.002) 0.008 (0.002) Log-Normal Gamma RP(9) 0.002 (0.002) -0.002 (0.002) 0.003 (0.002) 0.002 (0.002) 0.007 (0.002) Log-Normal Gamma RP(P) 0.002 (0.002) -0.002 (0.002) 0.008 (0.002) 0.003 (0.002) 0.009 (0.002) Gamma Log-Normal Cox -0.002 (0.002) 0.001 (0.002) -0.002 (0.002) -0.001 (0.002) 0.001 (0.002) Gamma Log-Normal Exp -0.002 (0.002) -0.083 (0.003) 0.133 (0.002) 0.102 (0.002) -0.116 (0.002) Gamma Log-Normal Wei -0.002 (0.002) 0.001 (0.002) 0.034 (0.002) -0.067 (0.002) 0.000 (0.002) Gamma Log-Normal Gom -0.005 (0.002) -0.084 (0.003) -0.002 (0.002) 0.027 (0.002) -0.115 (0.002) Gamma Log-Normal RP(5) -0.002 (0.002) -0.001 (0.002) -0.002 (0.002) -0.002 (0.002) -0.001 (0.002) Gamma Log-Normal RP(9) -0.002 (0.002) -0.001 (0.002) -0.002 (0.002) -0.001 (0.002) -0.002 (0.002) Gamma Log-Normal RP(P) -0.002 (0.002) -0.001 (0.002) 0.003 (0.002) 0.000 (0.002) -0.000 (0.002) Log-Normal Log-Normal Cox 0.001 (0.002) -0.002 (0.002) 0.003 (0.002) 0.002 (0.002) 0.002 (0.002) Log-Normal Log-Normal Exp 0.001 (0.002) -0.086 (0.002) 0.136 (0.001) 0.100 (0.002) -0.115 (0.002) Log-Normal Log-Normal Wei 0.001 (0.002) -0.002 (0.002) 0.038 (0.002) -0.066 (0.002) 0.001 (0.002) Log-Normal Log-Normal Gom 0.001 (0.002) -0.084 (0.002) 0.003 (0.002) 0.022 (0.002) -0.114 (0.002) Log-Normal Log-Normal RP(5) 0.001 (0.002) -0.002 (0.002) 0.003 (0.002) 0.001 (0.002) 0.008 (0.002) Log-Normal Log-Normal RP(9) 0.001 (0.002) -0.002 (0.002) 0.003 (0.002) 0.002 (0.002) 0.007 (0.002) Log-Normal Log-Normal RP(P) 0.001 (0.002) -0.002 (0.002) 0.007 (0.002) 0.003 (0.002) 0.009 (0.002) Table A.8: Coverage with Monte Carlo standard error of estimated regression coefficient, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox 93.68 (0.77) 96.17 (0.61) 93.33 (0.79) 93.33 (0.79) 93.33 (0.79) Gamma Gamma Exp 95.20 (0.68) 72.60 (1.41) 35.30 (1.51) 53.80 (1.58) 49.20 (1.58) Gamma Gamma Wei 95.20 (0.68) 95.50 (0.66) 91.70 (0.87) 72.80 (1.41) 94.59 (0.72) Gamma Gamma Gom 95.67 (0.64) 70.21 (1.45) 94.80 (0.70) 89.98 (0.95) 49.87 (1.58) Gamma Gamma RP(5) 95.09 (0.68) 95.27 (0.67) 94.89 (0.70) 94.27 (0.73) 93.64 (0.77) Gamma Gamma RP(9) 95.09 (0.68) 95.27 (0.67) 95.00 (0.69) 94.35 (0.73) 93.64 (0.77) Gamma Gamma RP(P) 95.17 (0.68) 95.42 (0.66) 94.59 (0.72) 94.36 (0.73) 93.64 (0.77) Log-Normal Gamma Cox 95.55 (0.65) 94.79 (0.70) 97.18 (0.52) 92.68 (0.82) 92.59 (0.83) Log-Normal Gamma Exp 95.30 (0.67) 74.80 (1.37) 35.40 (1.51) 53.30 (1.58) 47.50 (1.58) Log-Normal Gamma Wei 95.50 (0.66) 95.00 (0.69) 91.50 (0.88) 74.10 (1.39) 94.68 (0.71) Log-Normal Gamma Gom 95.28 (0.67) 76.13 (1.35) 95.40 (0.66) 93.17 (0.80) 49.22 (1.58) Log-Normal Gamma RP(5) 95.30 (0.67) 96.20 (0.60) 95.49 (0.66) 95.38 (0.66) 95.73 (0.64) Log-Normal Gamma RP(9) 95.30 (0.67) 96.35 (0.59) 95.39 (0.66) 95.05 (0.69) 95.73 (0.64) Log-Normal Gamma RP(P) 95.60 (0.65) 96.20 (0.60) 95.60 (0.65) 94.92 (0.69) 95.30 (0.67) Gamma Log-Normal Cox 95.20 (0.68) 95.30 (0.67) 95.00 (0.69) 94.30 (0.73) 94.70 (0.71) Gamma Log-Normal Exp 95.19 (0.68) 72.37 (1.41) 35.30 (1.51) 53.80 (1.58) 48.95 (1.58) Gamma Log-Normal Wei 95.19 (0.68) 95.50 (0.66) 91.69 (0.87) 73.22 (1.40) 94.68 (0.71) Gamma Log-Normal Gom 95.94 (0.62) 71.47 (1.43) 94.99 (0.69) 90.42 (0.93) 49.48 (1.58) Gamma Log-Normal RP(5) 95.09 (0.68) 95.27 (0.67) 94.90 (0.70) 94.20 (0.74) 93.64 (0.77) Gamma Log-Normal RP(9) 95.09 (0.68) 95.27 (0.67) 94.90 (0.70) 94.30 (0.73) 93.64 (0.77) Gamma Log-Normal RP(P) 95.09 (0.68) 95.42 (0.66) 94.60 (0.71) 94.50 (0.72) 93.64 (0.77) Log-Normal Log-Normal Cox 95.20 (0.68) 95.40 (0.66) 95.30 (0.67) 95.10 (0.68) 94.80 (0.70) Log-Normal Log-Normal Exp 95.30 (0.67) 74.87 (1.37) 35.40 (1.51) 53.20 (1.58) 47.59 (1.58) Log-Normal Log-Normal Wei 95.49 (0.66) 95.10 (0.68) 91.59 (0.88) 73.92 (1.39) 94.78 (0.70) Log-Normal Log-Normal Gom 94.75 (0.71) 76.04 (1.35) 95.39 (0.66) 93.17 (0.80) 48.99 (1.58) Log-Normal Log-Normal RP(5) 95.20 (0.68) 96.50 (0.58) 95.49 (0.66) 95.40 (0.66) 95.73 (0.64) Log-Normal Log-Normal RP(9) 95.30 (0.67) 96.35 (0.59) 95.39 (0.66) 95.10 (0.68) 95.73 (0.64) Log-Normal Log-Normal RP(P) 95.40 (0.66) 96.05 (0.62) 95.70 (0.64) 94.90 (0.70) 95.73 (0.64) Table A.9: Mean squared error of estimated regression coefficient, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox 0.005 0.005 0.004 0.005 0.004 Gamma Gamma Exp 0.004 0.013 0.020 0.013 0.019 Gamma Gamma Wei 0.004 0.005 0.005 0.009 0.003 Gamma Gamma Gom 0.004 0.014 0.004 0.004 0.018 Gamma Gamma RP(5) 0.004 0.004 0.004 0.003 0.004 Gamma Gamma RP(9) 0.004 0.004 0.004 0.003 0.004 Gamma Gamma RP(P) 0.004 0.004 0.004 0.003 0.004 Log-Normal Gamma Cox 0.004 0.005 0.004 0.003 0.003 Log-Normal Gamma Exp 0.004 0.013 0.020 0.012 0.018 Log-Normal Gamma Wei 0.004 0.005 0.005 0.009 0.003 Log-Normal Gamma Gom 0.004 0.013 0.004 0.004 0.019 Log-Normal Gamma RP(5) 0.004 0.004 0.004 0.003 0.003 Log-Normal Gamma RP(9) 0.004 0.004 0.004 0.003 0.003 Log-Normal Gamma RP(P) 0.004 0.004 0.004 0.003 0.003 Gamma Log-Normal Cox 0.004 0.005 0.004 0.003 0.003 Gamma Log-Normal Exp 0.004 0.014 0.020 0.013 0.019 Gamma Log-Normal Wei 0.004 0.005 0.005 0.009 0.003 Gamma Log-Normal Gom 0.004 0.013 0.004 0.004 0.018 Gamma Log-Normal RP(5) 0.004 0.004 0.004 0.003 0.004 Gamma Log-Normal RP(9) 0.004 0.004 0.004 0.003 0.004 Gamma Log-Normal RP(P) 0.004 0.004 0.004 0.003 0.004 Log-Normal Log-Normal Cox 0.004 0.005 0.004 0.003 0.003 Log-Normal Log-Normal Exp 0.004 0.014 0.020 0.012 0.018 Log-Normal Log-Normal Wei 0.004 0.005 0.005 0.009 0.003 Log-Normal Log-Normal Gom 0.004 0.013 0.004 0.004 0.018 Log-Normal Log-Normal RP(5) 0.004 0.004 0.004 0.003 0.003 Log-Normal Log-Normal RP(9) 0.004 0.004 0.004 0.003 0.003 Log-Normal Log-Normal RP(P) 0.004 0.004 0.004 0.003 0.003 Table A.10: Bias with Monte Carlo standard error of estimated frailty variance, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox -0.096 (0.002) -0.087 (0.002) -0.144 (0.001) -0.177 (0.001) -0.164 (0.001) Gamma Gamma Exp -0.014 (0.003) 0.056 (0.004) -0.116 (0.002) -0.090 (0.002) 0.091 (0.004) Gamma Gamma Wei -0.014 (0.003) -0.020 (0.003) -0.043 (0.003) 0.049 (0.003) -0.015 (0.003) Gamma Gamma Gom -0.018 (0.003) 0.063 (0.004) -0.013 (0.003) -0.042 (0.002) 0.089 (0.004) Gamma Gamma RP(5) -0.014 (0.003) -0.020 (0.003) -0.013 (0.003) -0.011 (0.003) -0.024 (0.003) Gamma Gamma RP(9) -0.014 (0.003) -0.020 (0.003) -0.013 (0.003) -0.012 (0.003) -0.023 (0.003) Gamma Gamma RP(P) -0.015 (0.003) -0.020 (0.003) -0.018 (0.003) -0.016 (0.003) -0.024 (0.003) Log-Normal Gamma Cox -0.113 (0.001) -0.102 (0.001) -0.148 (0.001) -0.182 (0.001) -0.168 (0.001) Log-Normal Gamma Exp -0.048 (0.003) 0.016 (0.004) -0.147 (0.001) -0.130 (0.001) 0.047 (0.004) Log-Normal Gamma Wei -0.048 (0.003) -0.057 (0.003) -0.081 (0.002) -0.009 (0.003) -0.056 (0.002) Log-Normal Gamma Gom -0.054 (0.003) 0.022 (0.004) -0.053 (0.003) -0.080 (0.002) 0.040 (0.004) Log-Normal Gamma RP(5) -0.048 (0.003) -0.058 (0.003) -0.053 (0.003) -0.057 (0.002) -0.065 (0.002) Log-Normal Gamma RP(9) -0.048 (0.003) -0.058 (0.003) -0.053 (0.003) -0.057 (0.002) -0.065 (0.002) Log-Normal Gamma RP(P) -0.048 (0.003) -0.058 (0.003) -0.057 (0.002) -0.059 (0.002) -0.066 (0.002) Gamma Log-Normal Cox 0.037 (0.004) 0.027 (0.004) 0.038 (0.004) 0.039 (0.004) 0.033 (0.004) Gamma Log-Normal Exp 0.016 (0.004) 0.091 (0.005) -0.101 (0.003) -0.067 (0.003) 0.144 (0.005) Gamma Log-Normal Wei 0.016 (0.004) 0.006 (0.004) -0.018 (0.003) 0.104 (0.005) 0.019 (0.004) Gamma Log-Normal Gom 0.014 (0.004) 0.089 (0.005) 0.017 (0.004) -0.009 (0.003) 0.138 (0.005) Gamma Log-Normal RP(5) 0.016 (0.004) 0.007 (0.004) 0.017 (0.004) 0.019 (0.004) 0.007 (0.004) Gamma Log-Normal RP(9) 0.016 (0.004) 0.007 (0.004) 0.017 (0.004) 0.018 (0.004) 0.007 (0.004) Gamma Log-Normal RP(P) 0.016 (0.004) 0.007 (0.004) 0.012 (0.004) 0.017 (0.004) 0.006 (0.004) Log-Normal Log-Normal Cox -0.023 (0.003) -0.032 (0.003) -0.028 (0.003) -0.032 (0.003) -0.029 (0.003) Log-Normal Log-Normal Exp -0.039 (0.003) 0.027 (0.004) -0.142 (0.002) -0.121 (0.002) 0.066 (0.004) Log-Normal Log-Normal Wei -0.039 (0.003) -0.048 (0.003) -0.073 (0.002) 0.014 (0.003) -0.043 (0.003) Log-Normal Log-Normal Gom -0.049 (0.003) 0.031 (0.004) -0.043 (0.003) -0.064 (0.002) 0.066 (0.004) Log-Normal Log-Normal RP(5) -0.039 (0.003) -0.049 (0.003) -0.043 (0.003) -0.047 (0.002) -0.056 (0.002) Log-Normal Log-Normal RP(9) -0.039 (0.003) -0.049 (0.003) -0.043 (0.003) -0.047 (0.002) -0.056 (0.002) Log-Normal Log-Normal RP(P) -0.039 (0.003) -0.048 (0.003) -0.047 (0.003) -0.048 (0.002) -0.057 (0.002) Table A.11: Coverage with Monte Carlo standard error of estimated frailty variance, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox 59.48 (1.55) 64.26 (1.52) 5.71 (0.73) 0.00 (0.00) 2.22 (0.47) Gamma Gamma Exp 85.70 (1.11) 94.50 (0.72) 38.10 (1.54) 54.90 (1.57) 96.80 (0.56) Gamma Gamma Wei 86.10 (1.09) 83.30 (1.18) 77.20 (1.33) 95.80 (0.63) 85.60 (1.11) Gamma Gamma Gom 82.35 (1.21) 96.28 (0.60) 85.70 (1.11) 83.06 (1.19) 96.42 (0.59) Gamma Gamma RP(5) 85.97 (1.10) 83.36 (1.18) 85.77 (1.10) 86.83 (1.07) 80.45 (1.25) Gamma Gamma RP(9) 85.87 (1.10) 83.36 (1.18) 85.80 (1.10) 86.90 (1.07) 81.36 (1.23) Gamma Gamma RP(P) 86.02 (1.10) 83.36 (1.18) 85.09 (1.13) 86.37 (1.08) 80.45 (1.25) Log-Normal Gamma Cox 47.66 (1.58) 55.37 (1.57) 6.78 (0.79) 0.00 (0.00) 0.00 (0.00) Log-Normal Gamma Exp 76.80 (1.33) 89.00 (0.99) 17.80 (1.21) 28.30 (1.42) 93.50 (0.78) Log-Normal Gamma Wei 77.10 (1.33) 72.60 (1.41) 61.70 (1.54) 90.20 (0.94) 73.62 (1.39) Log-Normal Gamma Gom 74.20 (1.38) 90.98 (0.91) 74.70 (1.37) 66.47 (1.49) 91.41 (0.89) Log-Normal Gamma RP(5) 76.48 (1.34) 72.04 (1.42) 75.05 (1.37) 73.69 (1.39) 71.37 (1.43) Log-Normal Gamma RP(9) 76.78 (1.34) 72.04 (1.42) 74.92 (1.37) 73.54 (1.40) 71.37 (1.43) Log-Normal Gamma RP(P) 76.98 (1.33) 72.04 (1.42) 73.17 (1.40) 72.36 (1.41) 71.37 (1.43) Gamma Log-Normal Cox 84.50 (1.14) 82.20 (1.21) 83.30 (1.18) 84.40 (1.15) 82.50 (1.20) Gamma Log-Normal Exp 87.78 (1.04) 96.00 (0.62) 45.70 (1.58) 63.30 (1.52) 96.80 (0.56) Gamma Log-Normal Wei 87.88 (1.03) 85.89 (1.10) 80.88 (1.24) 96.99 (0.54) 88.23 (1.02) Gamma Log-Normal Gom 86.29 (1.09) 96.60 (0.57) 88.08 (1.02) 86.84 (1.07) 97.38 (0.50) Gamma Log-Normal RP(5) 87.68 (1.04) 85.95 (1.10) 88.10 (1.02) 89.60 (0.97) 85.91 (1.10) Gamma Log-Normal RP(9) 87.68 (1.04) 85.95 (1.10) 88.20 (1.02) 89.50 (0.97) 85.91 (1.10) Gamma Log-Normal RP(P) 87.78 (1.04) 85.95 (1.10) 87.50 (1.05) 89.20 (0.98) 85.91 (1.10) Log-Normal Log-Normal Cox 76.70 (1.34) 73.80 (1.39) 76.00 (1.35) 74.00 (1.39) 72.60 (1.41) Log-Normal Log-Normal Exp 80.18 (1.26) 91.09 (0.90) 21.70 (1.30) 36.50 (1.52) 94.38 (0.73) Log-Normal Log-Normal Wei 79.84 (1.27) 77.48 (1.32) 66.87 (1.49) 92.08 (0.85) 77.89 (1.31) Log-Normal Log-Normal Gom 77.34 (1.32) 92.97 (0.81) 77.56 (1.32) 71.36 (1.43) 92.42 (0.84) Log-Normal Log-Normal RP(5) 80.08 (1.26) 76.44 (1.34) 77.66 (1.32) 77.58 (1.32) 77.35 (1.32) Log-Normal Log-Normal RP(9) 79.98 (1.27) 76.44 (1.34) 77.66 (1.32) 77.38 (1.32) 77.35 (1.32) Log-Normal Log-Normal RP(P) 79.88 (1.27) 76.60 (1.34) 76.78 (1.34) 77.20 (1.33) 77.35 (1.32) Table A.12: Mean squared error of estimated frailty variance, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox 0.012 0.010 0.021 0.032 0.027 Gamma Gamma Exp 0.009 0.017 0.017 0.013 0.023 Gamma Gamma Wei 0.009 0.009 0.009 0.013 0.008 Gamma Gamma Gom 0.009 0.017 0.009 0.006 0.022 Gamma Gamma RP(5) 0.009 0.009 0.009 0.008 0.008 Gamma Gamma RP(9) 0.009 0.009 0.009 0.008 0.008 Gamma Gamma RP(P) 0.009 0.009 0.009 0.008 0.008 Log-Normal Gamma Cox 0.014 0.012 0.023 0.033 0.028 Log-Normal Gamma Exp 0.009 0.014 0.024 0.019 0.015 Log-Normal Gamma Wei 0.009 0.010 0.011 0.007 0.009 Log-Normal Gamma Gom 0.009 0.015 0.009 0.010 0.015 Log-Normal Gamma RP(5) 0.009 0.010 0.009 0.009 0.009 Log-Normal Gamma RP(9) 0.009 0.010 0.009 0.009 0.009 Log-Normal Gamma RP(P) 0.009 0.010 0.009 0.009 0.009 Gamma Log-Normal Cox 0.018 0.017 0.019 0.016 0.017 Gamma Log-Normal Exp 0.014 0.030 0.017 0.012 0.048 Gamma Log-Normal Wei 0.014 0.014 0.012 0.032 0.015 Gamma Log-Normal Gom 0.014 0.030 0.015 0.009 0.045 Gamma Log-Normal RP(5) 0.014 0.014 0.015 0.013 0.012 Gamma Log-Normal RP(9) 0.014 0.014 0.015 0.013 0.012 Gamma Log-Normal RP(P) 0.014 0.014 0.015 0.013 0.012 Log-Normal Log-Normal Cox 0.009 0.009 0.009 0.008 0.009 Log-Normal Log-Normal Exp 0.009 0.015 0.023 0.017 0.020 Log-Normal Log-Normal Wei 0.009 0.009 0.011 0.010 0.009 Log-Normal Log-Normal Gom 0.009 0.015 0.009 0.009 0.022 Log-Normal Log-Normal RP(5) 0.009 0.010 0.009 0.008 0.009 Log-Normal Log-Normal RP(9) 0.009 0.010 0.009 0.008 0.009 Log-Normal Log-Normal RP(P) 0.009 0.010 0.009 0.008 0.009 Table A.13: Bias with Monte Carlo standard error of difference in 5-years life expectancy, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox 0.008 (0.003) -0.000 (0.003) 0.003 (0.002) 0.009 (0.003) 0.053 (0.003) Gamma Gamma Exp 0.001 (0.003) 0.024 (0.003) -0.038 (0.002) -0.008 (0.002) 0.072 (0.003) Gamma Gamma Wei 0.001 (0.003) -0.001 (0.003) -0.023 (0.002) 0.061 (0.002) 0.019 (0.003) Gamma Gamma Gom 0.005 (0.003) 0.030 (0.003) 0.001 (0.002) 0.037 (0.002) 0.069 (0.003) Gamma Gamma RP(5) 0.001 (0.003) 0.000 (0.003) 0.000 (0.002) -0.005 (0.002) 0.013 (0.003) Gamma Gamma RP(9) 0.001 (0.003) 0.000 (0.003) 0.001 (0.002) -0.002 (0.002) 0.012 (0.003) Gamma Gamma RP(P) 0.001 (0.003) 0.001 (0.003) -0.003 (0.002) -0.001 (0.002) 0.014 (0.003) Log-Normal Gamma Cox 0.001 (0.003) 0.008 (0.003) 0.006 (0.002) 0.032 (0.002) 0.033 (0.003) Log-Normal Gamma Exp -0.000 (0.003) 0.028 (0.003) -0.040 (0.002) -0.009 (0.002) 0.064 (0.003) Log-Normal Gamma Wei -0.000 (0.003) 0.006 (0.003) -0.025 (0.002) 0.060 (0.002) 0.013 (0.003) Log-Normal Gamma Gom -0.000 (0.003) 0.027 (0.003) -0.002 (0.002) 0.040 (0.002) 0.066 (0.003) Log-Normal Gamma RP(5) -0.000 (0.003) 0.005 (0.003) -0.002 (0.002) -0.011 (0.002) -0.009 (0.002) Log-Normal Gamma RP(9) -0.000 (0.003) 0.005 (0.003) -0.002 (0.002) -0.009 (0.002) -0.010 (0.002) Log-Normal Gamma RP(P) -0.000 (0.003) 0.005 (0.003) -0.006 (0.002) -0.007 (0.002) -0.008 (0.002) Gamma Log-Normal Cox -0.012 (0.003) -0.016 (0.003) -0.004 (0.002) 0.066 (0.002) 0.045 (0.003) Gamma Log-Normal Exp -0.022 (0.003) -0.004 (0.003) -0.048 (0.002) 0.003 (0.002) 0.118 (0.003) Gamma Log-Normal Wei -0.022 (0.003) -0.025 (0.003) -0.035 (0.002) 0.094 (0.002) 0.042 (0.003) Gamma Log-Normal Gom -0.020 (0.003) -0.000 (0.004) -0.016 (0.002) 0.056 (0.002) 0.116 (0.003) Gamma Log-Normal RP(5) 0.014 (0.003) 0.016 (0.003) 0.011 (0.002) 0.017 (0.002) 0.045 (0.003) Gamma Log-Normal RP(9) 0.014 (0.003) 0.016 (0.003) 0.011 (0.002) 0.020 (0.002) 0.044 (0.003) Gamma Log-Normal RP(P) 0.014 (0.003) 0.016 (0.003) 0.007 (0.002) 0.021 (0.002) 0.046 (0.003) Log-Normal Log-Normal Cox -0.009 (0.003) -0.006 (0.003) -0.005 (0.002) 0.053 (0.002) 0.036 (0.003) Log-Normal Log-Normal Exp -0.017 (0.003) 0.008 (0.003) -0.047 (0.002) 0.001 (0.002) 0.109 (0.003) Log-Normal Log-Normal Wei -0.017 (0.003) -0.012 (0.003) -0.034 (0.002) 0.088 (0.002) 0.035 (0.003) Log-Normal Log-Normal Gom -0.016 (0.003) 0.005 (0.003) -0.015 (0.002) 0.056 (0.002) 0.106 (0.003) Log-Normal Log-Normal RP(5) 0.013 (0.003) 0.020 (0.003) 0.007 (0.002) 0.006 (0.002) 0.019 (0.003) Log-Normal Log-Normal RP(9) 0.013 (0.003) 0.020 (0.003) 0.007 (0.002) 0.009 (0.002) 0.018 (0.003) Log-Normal Log-Normal RP(P) 0.013 (0.003) 0.020 (0.003) 0.004 (0.002) 0.010 (0.002) 0.020 (0.003) Table A.14: Mean squared error of difference in 5-years life expectancy, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. True frailty Model frailty Model baseline Exponential Weibull Gompertz Weibull-Weibull (1) Weibull-Weibull (2) Gamma Gamma Cox 0.008 0.010 0.005 0.009 0.010 Gamma Gamma Exp 0.007 0.012 0.005 0.005 0.014 Gamma Gamma Wei 0.007 0.010 0.004 0.010 0.008 Gamma Gamma Gom 0.007 0.013 0.004 0.007 0.014 Gamma Gamma RP(5) 0.007 0.009 0.005 0.005 0.009 Gamma Gamma RP(9) 0.007 0.009 0.004 0.005 0.009 Gamma Gamma RP(P) 0.007 0.009 0.004 0.005 0.009 Log-Normal Gamma Cox 0.008 0.010 0.004 0.006 0.008 Log-Normal Gamma Exp 0.007 0.011 0.005 0.005 0.014 Log-Normal Gamma Wei 0.007 0.009 0.005 0.010 0.008 Log-Normal Gamma Gom 0.007 0.011 0.004 0.007 0.014 Log-Normal Gamma RP(5) 0.007 0.009 0.004 0.005 0.006 Log-Normal Gamma RP(9) 0.007 0.009 0.004 0.005 0.006 Log-Normal Gamma RP(P) 0.007 0.009 0.004 0.005 0.006 Gamma Log-Normal Cox 0.007 0.010 0.004 0.009 0.009 Gamma Log-Normal Exp 0.008 0.012 0.006 0.005 0.023 Gamma Log-Normal Wei 0.008 0.011 0.005 0.015 0.009 Gamma Log-Normal Gom 0.008 0.013 0.005 0.009 0.022 Gamma Log-Normal RP(5) 0.008 0.010 0.005 0.006 0.011 Gamma Log-Normal RP(9) 0.008 0.010 0.005 0.006 0.011 Gamma Log-Normal RP(P) 0.008 0.010 0.005 0.006 0.011 Log-Normal Log-Normal Cox 0.007 0.009 0.004 0.008 0.008 Log-Normal Log-Normal Exp 0.007 0.011 0.006 0.005 0.021 Log-Normal Log-Normal Wei 0.007 0.010 0.005 0.014 0.009 Log-Normal Log-Normal Gom 0.008 0.010 0.005 0.009 0.020 Log-Normal Log-Normal RP(5) 0.008 0.009 0.005 0.005 0.007 Log-Normal Log-Normal RP(9) 0.008 0.009 0.005 0.005 0.007 Log-Normal Log-Normal RP(P) 0.008 0.009 0.004 0.005 0.007 "],
["ax-plots.html", "B Plots", " B Plots Figure B.1: Bias, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.2: Coverage, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.3: Mean squared error, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.4: Bias, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.5: Coverage, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.6: Mean squared error, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.7: Bias of estimated regression coefficient, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. Figure B.8: Coverage of estimated regression coefficient, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. Figure B.9: Mean squared error of estimated regression coefficient, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. Figure B.10: Bias of estimated frailty variance, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. Figure B.11: Coverage of estimated frailty variance, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. Figure B.12: Mean squared error of estimated frailty variance, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. Figure B.13: Bias of estimated difference in 5-years life expectancy, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. Figure B.14: Mean squared error of estimated difference in 5-years life expectancy, simulation study on model misspecification in survival models with shared frailty terms, scenario with 15 clusters of 100 individuals each and a small frailty variance. "],
["ax-slides.html", "C Slides ", " C Slides "],
["ax-slides-safjr.html", "C.1 2017 SAfJR Conference", " C.1 2017 SAfJR Conference "],
["ax-slides-sam-iscb.html", "C.2 2017 SAM Conference and ISCB Conference", " C.2 2017 SAM Conference and ISCB Conference "],
["ax-slides-students-day.html", "C.3 Students’ Day at the 2017 ISCB Conference", " C.3 Students’ Day at the 2017 ISCB Conference "],
["ax-manuscript.html", "D Manuscript draft", " D Manuscript draft "],
["references.html", "References", " References "]
]
