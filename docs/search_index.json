[
["index.html", "Probation review report Introduction", " Probation review report Alessandro Gasparini 2017-08-14 Introduction This report presents the work I have done during my first year as a PhD student at the Department of Health Sciences, University of Leicester, under the supervision of Dr. Michael Crowther and Prof. Keith Abrams. I will begin by briefly introducing the topic of survival analysis in Chapter 1. Second, I will introduce survival models with random effects (e.g. frailties, in the simplest form) and joint models for longitudinal and time-to-event data in Chapters 2 and 3, respectively. Computational challenges that survival models with random effects and joint models pose are presented in Chapter 4. In Chapter 5, I will present a method for simulating survival data. I will then present the results of two simulation studies in Chapters 6 and 7; the first simulation study investigates the accuracy of quadrature methods when approximating analytically intractable terms, while the second simulation study investigates the impact of model misspecification in survival models with shared frailty terms. I will introduce an interactive tool I have been developing to aid the dissemination of results from simulation studies and motivated by the simulation studies of Chapter 6 and 7 in Chapter 8. Next, I will introduce the problem of informative visiting process in clinical research using healthcare consumption data in Chapter 9, and how I aim to evaluate and compare the different approaches that have been proposed and utilised in literature to tackle such problem in Chapter 10. Finally, I will briefly summarise the training and personal development activities I have participated to during the first year of my PhD in Chapter 11. The text of this report is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License, while the underlying code is licensed under the GPLv3. The report is written using bookdown (Xie 2016), and can be accessed online at https://ellessenne.github.io/prr/. References "],
["sa.html", "1 Introduction to survival analysis", " 1 Introduction to survival analysis Survival analysis is a branch of statistics in which the main outcome consists in the time until the occurrence of a given event. Time could be years, months, weeks, or any amount of calendar time or even age time; event could be death, disease occurrence or relapse, or any other experience of interest. Survival analysis is also known as reliability theory in engineering, duration analysis in economics, and event history analysis in sociology. A broad overview of survival analysis is given in Kalbfleisch and Prentice (2011) and in Kleinbaum and Klein (2012). Some examples of time to event data are: disease remission in leukemia patients. In this study, leukemia patients are followed over several weeks to study how long they stay in remission status; heart disease occurrence. In this study, healthy subjects are followed over several years until occurrence of heart disease, or end of the study; renal failure. In this study, individuals with kidney disease are followed until renal failure, or end of the study; reliability of complex technical installations. For instance, studies assessing failure rates of components such as bulbs and valves. In this Chapter I will define survival data and its peculiarities in Section 1.1 and 1.2. Terminology and notation used throughout this report will be introduced in Section 1.3. I will introduce common non-parametric and parametric methods in survival analysis in Sections 1.4 and 1.5. I will introduce the widely used semi-parametric Cox model in Section 1.6. Finally, I will provide a brief overview on advances in survival analysis in Section 1.7. References "],
["sa-survdata.html", "1.1 Survival data", " 1.1 Survival data Survival data generally consists - as previously mentioned - in an event of interest and time until its occurrence. In the leukemia remission example, time to event would be how many weeks it takes before a given patient experiences disease relapse and the event would be whether the individual relapsed or not before the end of the study. Nevertheless, in certain situations we may have some information about the survival time but the actual survival time may be unknown. This problem is know as censoring and it is presented in Section 1.2. "],
["sa-censoring.html", "1.2 Censoring", " 1.2 Censoring Censoring is a mechanisms that causes survival times to be unobserved. There are many reasons why censoring may occur; among others: a person does not experience the event before the end of the study; a person drops out of the study before the occurrence of the event of interest; a person experience a competing event that impedes the occurrence of the event of interest (e.g.: death, when death is not the study outcome). Figure 1.1: Simulated right censored survival data, plotted by their calendar time in panel A and by their study time in panel B. I simulated survival data for illustration purposes: I assumed a clinical trial with 10 individuals enrolled during a recruitment window of 1 year, and followed for up to 5 years. Not all individuals experience the event of interest during the study period, and are therefore censored after five years from the start of the study. The observation time for each individual is depicted in Figure 1.1 with a solid dark grey line, a cross represents the occurrence of the study event, and a circle represents censoring. Individuals A, E, and J all have censored survival time: I know that they were still event-free at the end of follow-up, i.e. their real survival time is greater than the observed one, but the former is unknown. The simulated data is presented in Figure 1.1: in panel A, survival data is plotted against the calendar time; conversely, in panel B, survival data is plotted against the study time, e.g. each individual is assigned a time zero corresponding to their enrollment in the study, and survival time is counted from there. This example represents a particular form of censoring: right censoring. The defining characteristic of right-censored data is that it is censored (or incomplete) at the right side of the follow-up time, hence the true survival time is greater than the observed time. This example represents administrative censoring as well, as individuals are censored at the end of the study to artificially restrict follow-up time (e.g. for financial reasons). It is also possible to encounter data that is left censored or interval censored. In the former case, the true survival time is shorter that the observed one, e.g. I know that the event occurred before the observation time, but I do not know when - imagine onset of a viral infection, which can be detected only at a visit time. In the latter, I know that the event occurred within a certain interval of time but I do not know when; using the same example of infection onset, if infection was detected at a visit date but the individual was known to be infection-free at the previous visit, the true infection onset time is unknown and the event time is said to be interval censored. Finally, another important concept related to right censoring is that of left truncation (or delayed entry). Left truncation occurs when an individual enrolls in the study some time after the inclusion criteria are satisfied; individuals that die (or emigrate, …) before the start of observation time will never enter the study, and inclusion time may differ between individuals. Data arising from such phenomenon is therefore said to be left truncated. "],
["sa-terminology-notation.html", "1.3 Terminology and notation", " 1.3 Terminology and notation I denote the random variable for an individual’s survival time with \\(S\\); since it denotes time, \\(S\\) can assume any non-negative value. The lower-case \\(s\\) represent a specific value of interest drawn from \\(S\\) for a given individual. In the case of right censoring, I denote with \\(C\\) the random variable representing censoring time, and \\(c\\) its realisation. The observed time is denoted with \\(T = \\min(S, C)\\), and its realisation is \\(t\\). Finally, I denote with \\(D = I(S \\le C)\\) the random variable indicating either occurrence of the event of interest or censorship; analogously as before, its realisation is lower-case \\(d\\). Next, I defined two quantities of interest in survival analysis, the survival function and the hazard function. They are both functions of the observed time \\(t\\) and are denoted by \\(S(t)\\) and \\(h(t)\\), respectively. The survival function is the complement of the cumulative distribution function of the observed time \\(T\\) and represent the probability that a given individual survives1 longer than a specified time \\(t\\): \\[ S(t) = 1 - F_T(t) = 1 - P(T \\le t) = P(T &gt; t) \\] \\(t\\) ranges (theoretically) between 0 and infinity, hence the survival function can be plotted as a smooth, continuous function that tends to 0 as \\(t\\) goes to infinity. In practice, though, the survival function appears as a step function as (1) individuals can be observed at discrete times only and (2) not all individuals may experience the event before the end of the study. Figure 1.2 depicts this difference: in panel A I plotted a theoretical survival function, restricted to 15 years of follow-up for comparison purposes, while in panel B I plotted the survival function relative to the survival data simulated in Section 1.2. The former is a smooth function of time, and should we extend the x-axis to infinity the function would eventually reach zero. Conversely, the latter is a step function with steps at each event time, and should we extend the x-axis to infinity the function would remain flat after the last observed event. Figure 1.2: Theoretical survival function (A) and observed survival function for simulated data (B). The hazard function \\(h(t)\\) is the limit of the probability of the survival time \\(T\\) laying within an interval \\([t, t + \\Delta(t))\\) given that an individual survived up to time \\(t\\) divided by the length of the interval \\(\\Delta(t)\\), for \\(\\Delta(t)\\) approaching zero: \\[ h(t) = \\lim_{\\Delta(t) \\to 0} \\frac{P(t \\le T &lt; t + \\Delta(t) | T \\ge t)}{\\Delta(t)} \\] It represent the instantaneous potential (e.g. risk) for the event to occur within the interval \\([t, t + \\Delta(t))\\) (with \\(\\Delta(t) \\to 0\\)), given that the individual survived up to time \\(t\\). The hazard function is always non-negative, it can assume different shapes over time, and it has no upper bound. In Figure 1.3 I present a simple hazard function; it increases over time, which means that the instantaneous risk of event increases over time. Figure 1.3: Example of hazard function. The survival function from Figure 1.2, panel A, and the hazard function from Figure 1.3 are strictly related. In fact, there is a clearly defined mathematical relationship between the survival and the hazard function: it is possible to derive the form of \\(S(t)\\) when knowing the form of \\(h(t)\\), and vice versa. Formally: \\[ S(t) = \\exp \\left[ -\\int_0^t h(u) \\ du \\right] \\] \\[ h(t) = -\\left[ \\frac{d S(t) / dt}{S(t)} \\right] \\] Finally, a third quantity of interest in survival analysis that is strictly related to the survival and hazard functions is the cumulative hazard function \\(H(t)\\). The cumulative hazard function represents the accumulation of hazard (e.g. \\(h(t)\\)) over time, and can be defined as \\[ H(t) = \\int_0^t h(u) \\ du; \\] it can conveniently be expressed in terms of survival function via the relationship \\(H(t) = - \\log S(t)\\), or alternatively with \\(S(t) = \\exp(-H(t))\\). I use the term survives loosely speaking, for conciseness - formally, I refer to not experiencing the event of interest.↩ "],
["sa-estsurv.html", "1.4 Estimation of the survival function", " 1.4 Estimation of the survival function The survival function presented in Figure 1.2, panel B, is a non-parametric estimate of the true survival function based on the data only. The estimator employed is this case is the Kaplan-Meier estimator of the survival function (Kaplan and Meier 1958), with which the estimated survival probabilities are obtained using a product limit formula. The general form for the Kaplan-Meier estimator at time \\(t_{(i)}\\) is \\[ \\hat{S}(t_{(i)}) = \\hat{S}(t_{(i - 1)}) \\times \\hat{P}(T &gt; t_{(i)} | T \\ge t_{(i)}), \\] with \\(t_{(i)}\\) being the \\(i\\)th ordered failure time. The interpretation is straightforward: it is the product of the probability of surviving past the previous event-time (\\(\\hat{S}(t_{(i - 1)})\\)) times the conditional probability of surviving past the current time \\(t_{(i)}\\) given survival to at least the current time (\\(\\hat{P}(T &gt; t_{(i)} | T \\ge t_{(i)})\\)). The product limit formula is: \\[ \\hat{S}(t_{(i)}) = \\prod_{j = 1}^i \\hat{P}(T &gt; t_{(j)} | T \\ge t_{(j)}) \\] The conditional probability in the product limit formula can be estimated from the observed data as: \\[ \\hat{P}(T &gt; t_{(i)} | T \\ge t_{(i)}) = \\frac{r_{(i)} - e_{(i)}}{r_{(i)}}, \\] where \\(r_{(i)}\\) and \\(e_{(i)}\\) are the number of individuals at risk and the number of events at time \\(t_{(i)}\\), respectively. The Kaplan-Meier estimator can be computed using R and the function survfit from the survival package. An example using the simulated data from Section 1.2 (stored in a data frame named data): # library(survival) fit = survfit(Surv(time = t0, event = d) ~ 1, data = data) summary(fit) ## Call: survfit(formula = Surv(time = t0, event = d) ~ 1, data = data) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0.687 10 1 0.9 0.0949 0.7320 1.000 ## 0.975 9 1 0.8 0.1265 0.5868 1.000 ## 1.047 8 1 0.7 0.1449 0.4665 1.000 ## 2.580 7 1 0.6 0.1549 0.3617 0.995 ## 2.780 6 1 0.5 0.1581 0.2690 0.929 ## 3.143 5 1 0.4 0.1549 0.1872 0.855 ## 4.606 2 1 0.2 0.1612 0.0412 0.971 By doing so, I obtain an estimate of the survival function (column survival) at each distinct failure time (column time). For instance, the survival probability at \\(t\\) = 1.047 is 0.700, with 95% confidence interval (0.467 - 1.000). Finally, plotting the estimated survival curve I obtain Figure 1.4, which is exactly the same survival curve presented in panel B of Figure 1.2. # library(ggfortify) autoplot(fit, conf.int = FALSE, censor = FALSE) + theme_bw() + coord_cartesian(ylim = c(0, 1)) + labs(x = &quot;Time&quot;, y = expression(hat(S)[KM](t))) Figure 1.4: Estimated survival function using the Kaplan-Meier estimator on the simulated data. An alternative way of estimating the survival function is to use the Nelson-Aalen estimator for the cumulative hazard \\[ \\hat{H}(t) = \\sum_{t_i &lt; t} \\frac{e_i}{r_i} = \\sum_{t_i &lt; t} \\hat{h}_i, \\] and then use the relationship presented in Section 1.3 to obtain the survival function. References "],
["sa-parsa.html", "1.5 Parametric survival models", " 1.5 Parametric survival models In applied settings it is often of interest to assess the association between observed covariates and the survival time of interest. For instance, it may be of interest to study whether a treatment is effective in slowing disease relapse (e.g. relapse of leukemia), whether there are difference between genders or age categories. A common way of assessing the effect of covariates on a time to event outcome, while adjusting for potentially confounding factors at the same time, consists in using a regression model. In the context of survival data, two models are commonly used: the accelerated failure time model (AFT), and the proportional hazards (PH) model. In the former, the natural logarithm of the observed survival time \\(\\log t\\) is expressed as a linear function of the covariates \\(X\\): \\[ \\log t = X \\beta + \\epsilon, \\] with \\(\\beta\\) a vector of regression coefficients and \\(\\epsilon\\) a vector of residual error terms. Assuming a parametric distribution for \\(\\epsilon\\) determines the regression model: log-normal, log-logistic, Weibull, etc. In the AFT model, a positive association of the covariates with survival time implies an increased expected time to event. In the PH model, the covariates have a multiplicative effect on the hazard function: \\[ h(t; X) = h_0(t) g(X), \\] for some \\(h_0(t)\\) and \\(g(X)\\), with \\(g(\\cdot)\\)) a non-negative function of the covariates. A popular choice for the latter is \\(g(X) = \\exp(X \\beta)\\); conversely, is is possible to either left the former unspecified, or assume a parametric distribution. The focus of this Section is on specifying a parametric distribution for \\(h_0(t)\\), yielding the so-called parametric survival regression models; I will present commonly assumed parametric distributions in Section 1.5.1, the estimation procedure in Section 1.5.2, and an example using data from the International Stroke Trial (IST) (International Stroke Trial Collaborative Group 1997; Sandercock, Niewada, and Czlonkowska 2011) in Section 1.5.3. Leaving \\(h_0(t)\\) unspecified yields the semi-parametric Cox model, that I will present in Section 1.6. From now on I will focus on the proportional hazards formulation of the survival model. 1.5.1 Failure time distributions I mentioned in Section 1 that the random variable representing the survival time is non-negative; hence, we can choose any non-negative distribution to assign to \\(h_0(t)\\). Commonly used distribution are the Exponential, Weibull, log-Normal, and Gompertz distributions; other possible distributions are the inverse Weibull, the inverse Gamma, the positive stable, the log-skew-Normal, the log-logistic, and complex mixture distributions (such as the two components mixture Weibull distribution, McLachlan and McGiffin (1994)). Each distribution yields a different shape for the Survival and hazard functions; in particular, focusing on the distribution I will be utilising in the rest of this report: Exponential distribution: \\(h_0(t) = \\lambda\\) \\(S(t) = \\exp(-\\lambda t)\\) \\(\\lambda &gt; 0\\) Weibull distribution: \\(h_0(t) = \\lambda p t ^ {p - 1}\\) \\(S(t) = \\exp(-\\lambda t ^ p)\\) \\(\\lambda, p &gt; 0\\) log-Normal distribution: \\(h_0(t) = \\frac{\\phi(\\frac{\\log t - \\mu}{\\sigma})}{\\sigma t \\left(1 - \\Phi \\left[ \\frac{\\log t - mu}{\\sigma} \\right] \\right)}\\) \\(S(t) = 1 - \\Phi\\left( \\frac{\\log t - \\mu}{\\sigma} \\right)\\) \\(\\mu \\in R; \\sigma &gt; 0\\) Gompertz distribution: \\(h_0(t) = \\lambda \\exp(\\gamma t)\\) \\(S(t) = \\exp \\left[ -\\frac{\\lambda}{\\gamma} (\\exp(\\gamma t) - 1) \\right]\\) \\(\\lambda, \\gamma &gt; 0\\) Two components mixture Weibull distribution: \\(h_0(t) = \\frac{\\lambda_1 p_1 t ^ {p_1 - 1} \\pi \\exp(-\\lambda_1 t ^ {p_1}) + \\lambda_2 p_2 t ^ {p_2 - 1} (1 - \\pi) \\exp(-\\lambda_2 t ^ {p_2})}{\\pi \\exp(-\\lambda_1 t ^ {p_1}) + (1 - \\pi) \\exp(-\\lambda_2 t ^ {p_2})}\\) \\(S(t) = \\pi \\exp(-\\lambda_1 t ^ {p_1}) + (1 - \\pi) \\exp(-\\lambda_2 t ^ {p_2})\\) \\(\\lambda_1, \\lambda_2, p_1, p_2 &gt; 0; \\pi \\in [0, 1]\\) 1.5.2 Estimation procedure Assume \\(n\\) observations with the bivariate response \\((t_i, d_i)\\), with \\(i = 1, \\dots, n\\). For a given survival function \\(S(t)\\) the density function is given by \\[ f(t) = - \\frac{d S(t)}{dt}, \\] and the hazard function by \\[ h(t) = \\frac{f(t)}{S(t)}. \\] The parameters of the parametric proportional hazards survival model presented in Section 1.5 can be estimated via the maximum likelihood method. A subject that experiences the event of interest at time \\(t_i\\) contributes to the likelihood the density at time \\(t_i\\), i.e. \\(f(t_i)\\); conversely, a censored observation know to survive until tile \\(t_i\\) contributes \\(S(t_i)\\) to the likelihood. The individual contribution to the likelihood \\(L_i\\) can therefore be written as \\[ L_i = h(t_i) ^ d_i S(t_i), \\] where \\(d_i\\) is the event indicator variable. The overall likelihood is the product of the individual contributions: \\[ L = \\prod_{i = 1} ^ n L_i. \\] Taking the natural logarithm of the likelihood for ease of computation: \\[ \\begin{aligned} \\log L &amp;= \\sum_{i = 1} ^ n \\left[ d_i \\log f_i(t_i) + (1 - d_i) \\log S_i(t_i) \\right] = \\\\ &amp;= \\sum_{i = 1} ^ n \\left[ d_i \\log h_i(t_i) + \\log S_i(t_i) \\right] \\end{aligned} \\] Implicit in the above log-likelihood are the regression parameters \\(\\beta\\) and the parameters of the parametric distribution of choice for \\(h_0(t)\\). The log-likelihood function \\(\\log L\\) has a closed-form; maximum likelihood estimates for \\(\\beta\\) and the distribution parameters can hence be obtained by maximising \\(\\log L\\), e.g. using one of the many general purpose optimisers available in R (optim, nlm, …). 1.5.3 Data analysis example The International Stroke Trial (IST) was a large, prospective, randomised controlled trial conducted between 1991 and 1996. The aim of the trial was to assess whether early administration of aspirin, heparin, both or neither influenced clinical outcomes in patients with acute ischaemic stroke (International Stroke Trial Collaborative Group 1997; Sandercock, Niewada, and Czlonkowska 2011). As illustration, I will evaluate the association between tretment with aspirin and/or heparin and survival after acute ischaemic stroke. I will start by reading the data, stored in the ist.csv file. This file is a subset of the full IST dataset containing information on age, gender, Country, treatment, and survival; further, individuals with missing values and individuals with a survival time of zero were dropped. # library(readr) ist = read_csv(&quot;data/ist.csv.gz&quot;, col_names = c(&quot;gender&quot;, &quot;age&quot;, &quot;rxasp&quot;, &quot;rxhep&quot;, &quot;country&quot;, &quot;d&quot;, &quot;t&quot;), col_types = &quot;cicccii&quot;, skip = 1) attr(ist, &quot;spec&quot;) = NULL # removing &quot;spec&quot; attribute # turn treatments into factors ist$rxasp = factor(ist$rxasp, levels = c(&quot;N&quot;, &quot;Y&quot;)) ist$rxhep = factor(ist$rxhep, levels = c(&quot;N&quot;, &quot;L&quot;, &quot;H&quot;)) I fit first a parametric survival model assuming a Weibull distribution for \\(h_0(t)\\). The hazard function, including covariates and the imposing proportional hazards, has the form \\[ h(t; X) = \\lambda p t ^ {p - 1} \\exp(X \\beta), \\] while the survival function has the form \\[ S(t; X) = \\exp(-\\lambda t ^ p \\exp(X \\beta)). \\] \\(X\\) is the model design matrix, and \\(\\beta\\) is the vector of regression coefficients. The log-likelihood has the form \\[ \\log L = \\sum_{i = 1} ^ n \\left[ d_i \\log h_i(t_i) + \\log S_i(t_i) \\right] \\] First, I code a function with the model log-likelihood. The function depends on (1) the model parameters \\(\\beta\\), \\(\\lambda\\), and \\(p\\) (pars argument), (2) the model design matrix \\(X\\) (X argument), and (3) survival time \\(t\\) and event indicator \\(d\\) (t and d arguments): ll = function(pars, X, t, d) { lambda = exp(pars[1]) p = exp(pars[2]) beta = pars[-(1:2)] log_hi = log(lambda) + log(p) + (p - 1) * log(t) + c(X %*% beta) log_Si = -lambda * t ^ p * exp(c(X %*% beta)) ll = sum(d * log_hi + log_Si + log(t)) # + sum(log(t)) is the same adjustment that Stata # does to remove the time units from log L return(-ll) } The function ll() returns the negative log-likelihood as most optimisers minimise a target function (and so does optim); however, minimising the negative log-likelihood function is equivalent to maximising the log-likelihood. I define the model matrix \\(X\\) for a model with aspirin treatment, heparin treatment, and their interactions. The first column is removed to avoid collinearity: X = with(ist, model.matrix(t ~ rxasp * rxhep - 1))[,-1] Next, I define the starting values for the optmisation routine. I choose the value 1 for the parameters of the Weibull distribution and the value 0 for the regression coefficients: start = c(1, 1, rep(0, ncol(X))) names(start) = c(&quot;lambda&quot;, &quot;p&quot;, colnames(X)) The value of the log-likelihood function at the starting values is -92861140101.392. Finally, I use the robust-variance modification of the Marquard algorithm, which is more efficient than Gauss-Newton-like algorithms when starting from points very far from the optimum (Marquardt 1963; Commenges et al. 2006): # library(marqLevAlg) fit = marqLevAlg(b = start, fn = function(x) ll(x, X = X, t = ist$t, d = ist$d)) ## ## Be patient. The program is computing ... ## The program took 7.35 seconds Assess convergency: fit$istop ## [1] 1 The convergence status indicator is equal to 1, hence the convergence criteria were satisfied. The log-likelihood at the maximum likelihood estimates is 61566.567. The optimising routine returns the upper triangle matrix of variance-covariance estimates at the stopping point, which can be used to obtain standard errors of the estimated coefficients: fit$vcov = matrix(0, nrow = length(fit$b), ncol = length(fit$b)) fit$vcov[upper.tri(fit$vcov, diag = TRUE)] = fit$v fit$vcov[lower.tri(fit$vcov)] = t(fit$vcov)[lower.tri(fit$vcov)] Finally, I build a table of results: res = data.frame( coef = fit$b, hr = exp(fit$b), se = sqrt(diag(fit$vcov))) res$z = res$coef / res$se res$p = 2 * pmin(pnorm(-abs(res$z)), 1 - pnorm(abs(res$z))) kable(res, digits = 3, align = &quot;rrrrr&quot;, booktabs = TRUE, col.names = c(&quot;Beta&quot;, &quot;Hazard ratio&quot;, &quot;SE (Beta)&quot;, &quot;Z&quot;, &quot;P &gt; |Z|&quot;), linesep = &quot;&quot;, caption = &quot;Results from a parametric Weibull model.&quot;) Table 1.1: Results from a parametric Weibull model. Beta Hazard ratio SE (Beta) Z P &gt; |Z| lambda -3.852 0.021 0.047 -82.665 0.000 p -0.759 0.468 0.015 -52.050 0.000 rxaspY -0.037 0.964 0.044 -0.850 0.395 rxhepL 0.085 1.088 0.052 1.640 0.101 rxhepH 0.044 1.045 0.052 0.843 0.399 rxaspY:rxhepL -0.095 0.910 0.075 -1.269 0.204 rxaspY:rxhepH 0.037 1.038 0.074 0.503 0.615 I test the interaction term using the Wald test to assess whether combining aspirin and heparin alter their association with time to event. I use the Wald \\(\\chi^2\\) test statistic as the sample size is big enough for it to be equivalent to its \\(F\\) counterpart: # identify the interaction terms idx = grepl(&quot;:&quot;, names(fit$b)) # compute the W statistic W = t(fit$b[idx]) %*% solve(fit$vcov[idx, idx]) %*% fit$b[idx] # produce the test c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx))) ## W df p-value ## 2.6104650 2.0000000 0.2711095 The interaction terms seem to be not statistically significant. We can conclude the association of aspirin and heparin treatments with survival are not dependent on one another. I can then re-fit the model excluding the interaction terms: X = with(ist, model.matrix(t ~ rxasp + rxhep - 1))[,-1] start = c(1, 1, rep(0, ncol(X))) names(start) = c(&quot;lambda&quot;, &quot;p&quot;, colnames(X)) re_fit = marqLevAlg(b = start, fn = function(x) ll(x, X = X, t = ist$t, d = ist$d)) ## ## Be patient. The program is computing ... ## The program took 5.99 seconds re_fit$istop ## [1] 1 The routine converged. I produce the variance-covariance matrix: re_fit$vcov = matrix(0, nrow = length(re_fit$b), ncol = length(re_fit$b)) re_fit$vcov[upper.tri(re_fit$vcov, diag = TRUE)] = re_fit$v re_fit$vcov[lower.tri(re_fit$vcov)] = t(re_fit$vcov)[lower.tri(re_fit$vcov)] Finally, I build a new table of results: re_res = data.frame( coef = re_fit$b, hr = exp(re_fit$b), se = sqrt(diag(re_fit$vcov))) re_res$z = re_res$coef / re_res$se re_res$p = 2 * pmin(pnorm(-abs(re_res$z)), 1 - pnorm(abs(re_res$z))) kable(re_res, digits = 3, align = &quot;rrrrr&quot;, col.names = c(&quot;Beta&quot;, &quot;Hazard ratio&quot;, &quot;SE (Beta)&quot;, &quot;Z&quot;, &quot;P &gt; |Z|&quot;), booktabs = TRUE, linesep = &quot;&quot;, caption = &quot;Results from a parametric Weibull model with no interactions.&quot;) Table 1.2: Results from a parametric Weibull model with no interactions. Beta Hazard ratio SE (Beta) Z P &gt; |Z| lambda -3.845 0.021 0.044 -87.441 0.000 p -0.759 0.468 0.015 -52.056 0.000 rxaspY -0.051 0.950 0.030 -1.689 0.091 rxhepL 0.039 1.040 0.037 1.043 0.297 rxhepH 0.062 1.064 0.037 1.684 0.092 I now test the significance of the two coefficients related to heparin treatment jointly: idx = grepl(&quot;^rxhep&quot;, names(re_fit$b)) W = t(re_fit$b[idx]) %*% solve(re_fit$vcov[idx, idx]) %*% re_fit$b[idx] c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx))) ## W df p-value ## 3.08001 2.00000 0.21438 Heparin treatment seems to be not statistically significantly associated with time to death in acute ischaemic stroke patients; the effect size is small, with a 4% and 7% increased risk for the L and H heparin treatment modalities versus no heparin treatment, respectively. Finally, the treatment with aspirin is also not statistically significantly associated with the outcome; effect size is small as well, approximately a 5% risk reduction for aspirin treatment compared to no treatment with aspirin (Table 1.2). This is a simple application of parametric survival models; a fully developed analysis should take further aspects into account, such as considering different hazard distributions. It is possible to estimate various models and compare their fit to a specific distribution using information criteria such as the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). References "],
["sa-cox.html", "1.6 The Cox proportional hazards model", " 1.6 The Cox proportional hazards model The parametric survival models of Section 1.5 could have both the accelerated failure time form and the proportional hazards form. Recall that the latter is formulated in terms of the hazard function: \\[ h(t; X) = h_0(t) \\exp(X \\beta) \\] As I mentioned before, this model requires specifying the baseline hazard function \\(h_0(t)\\) (e.g. using one of the parametric distributions of Section 1.5.1) and by leaving it unspecified I obtain the Cox proportional hazards model. Such model is also called semi-parametric as it is formed by a non-parametric component (the baseline hazard left unspecified) and a parametric component (the modelling assumption for the functional form of \\(g(\\cdot)\\), the usual \\(\\exp(X \\beta)\\) in this case). The survival function for a Cox model can be written as: \\[ S(t; X) = \\exp \\left[ -\\int_0^t h_0(u) \\exp(X \\beta) \\ du \\right] \\] The main problems when fitting a Cox model are related to estimation of the regression coefficients \\(\\beta\\) and of the survival function \\(S(t)\\). The former is tackled in Section 1.6.1, the latter in Section 1.6.2. 1.6.1 Estimation procedure The main method for estimating the regression coefficients is the method of partial likelihood, proposed and discussed in detail in Cox (1972) and Cox (1975). In brief, the observed data are assumed to have density function \\(f(t; \\theta, \\beta)\\) in which \\(\\beta\\) is the vector of regression coefficients of interest and \\(\\theta\\) can be considered a vector of nuisance parameters. In particular, \\(\\theta\\) represents the unspecified function \\(h_0(t)\\). It can be showed that is is possible to factorise the density into two terms, one of which only depends on \\(\\beta\\): this term is called partial likelihood. Ignoring the term that depends on \\(\\theta\\), and even if the partial likelihood is not directly interpretable as a likelihood in the ordinary sense, it can be used like an ordinary likelihood for estimation purposes as the usual asymptotic properties formulas and properties associated with the likelihood function and likelihood estimation apply. The partial likelihood applies directly to the relative risk model \\(h(t; X)\\), assuming independent right censoring. The individual contribution to the likelihood has the form \\[ L_i(\\beta) = \\frac{h(t_i; x_i) \\Delta t_i}{\\sum_{l \\in R(t_i)} h(t_i; x_l) \\Delta t_i}, \\] and provides information on failures occurrence in the interval \\([t_i, t_i + \\Delta t_i)\\); \\(R(t_i)\\) is the risk set of individuals at risk of failing at time \\(t_i^{-}\\), right before \\(t_i\\). Under the relative risk model, the baseline hazard in \\(h(t; X)\\) cancels out in the numerator and denominator; the product over \\(i\\) gives the partial likelihood for \\(\\beta\\): \\[ L(\\beta) = \\prod_{i = 1}^n \\frac{\\exp(x_i \\beta)}{\\sum_{l \\in R(t_i)} \\exp(x_l \\beta)}. \\] The values of \\(\\beta\\) that maximise the partial likelihood \\(\\hat{\\beta}\\) can be obtained by using a Newton-Raphson-like algorithm; asymptotics are fully analogous to a parametric likelihood. A caveat of the partial likelihood method is that it assumes continuous failure times: in practice, that is unrealistic and there will be tied failure times (e.g. due to rounding). In that case, several methods have been proposed to adjust the partial likelihood in order to handle ties; see for instance Peto (1972), Breslow (1974), and Efron (1977) 1.6.2 Estimating the survival function Consider deriving an estimator for the survival function from a Cox model: the form of \\(h_0(t)\\) is unspecified, hence it is not possible to directly estimate the parameters of the distribution as in fully parametric survival models. Under a Cox model, the survival function has the form \\[ S(t; X) = S_0(t) ^ {\\exp(X \\beta)} \\] The coefficients \\(\\beta\\) are estimated using the penalised likelihood procedure, and the baseline survival function \\(S_0(t)\\) is estimated by assuming that the baseline hazard function is constant between each pair of consecutive observed failure times. The resulting estimator, known as the Breslow estimator, estimates the cumulative baseline hazard function as \\[ \\hat{H}_0(t) = \\sum_{t(i) \\le t} \\frac{e_{(i)}}{\\sum_{l \\in R(t_{(i)}) \\exp(x_l \\hat{\\beta})}}, \\] with \\(e_{(i)}\\) the number of events at time \\(t_{(i)}\\). The baseline survival function follows as \\[ \\hat{S}_0(t) = \\exp \\left[ -\\hat{H}_0(t) \\right], \\] and the survival function as \\[ \\hat{S}(t; X) = \\hat{S}_0(t) ^ {\\exp(X \\hat{\\beta})}. \\] An alternative estimator based on approximating the baseline survival function as a step function and consequently solving \\(k\\) simultaneous equations has been proposed by Kalbfleisch and Prentice (2011), and is omitted here. 1.6.3 Model assumptions The Cox model relies on two main assumptions. First, the assumption of non-informative censoring, e.g. the censoring process must be independent of any covariate, observed and not. Second, the proportional hazards assumption requires hazards to be proportional across time, e.g. the hazard ratios must be constant. There are several ways of testing the proportional hazards assumption, both analytical and graphical; see Chapter 4 of Kleinbaum and Klein (2012) for further details. 1.6.4 Data analysis example In this section I re-analyse the IST data of Section 1.5.3 using a semi-parametric Cox model. I first read the dataset: # library(readr) ist = read_csv(&quot;data/ist.csv.gz&quot;, col_names = c(&quot;gender&quot;, &quot;age&quot;, &quot;rxasp&quot;, &quot;rxhep&quot;, &quot;country&quot;, &quot;d&quot;, &quot;t&quot;), col_types = &quot;cicccii&quot;, skip = 1) attr(ist, &quot;spec&quot;) = NULL # removing &quot;spec&quot; attribute # turn treatments into factors ist$rxasp = factor(ist$rxasp, levels = c(&quot;N&quot;, &quot;Y&quot;)) ist$rxhep = factor(ist$rxhep, levels = c(&quot;N&quot;, &quot;L&quot;, &quot;H&quot;)) I fit the Cox model using the coxph() function from the survival package: # library(survival) fit = coxph(Surv(t, d) ~ rxasp * rxhep, data = ist) summary(fit) ## Call: ## coxph(formula = Surv(t, d) ~ rxasp * rxhep, data = ist) ## ## n= 19378, number of events= 4315 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## rxaspY -0.03710 0.96358 0.04356 -0.852 0.394 ## rxhepL 0.08017 1.08347 0.05161 1.553 0.120 ## rxhepH 0.04215 1.04305 0.05221 0.807 0.419 ## rxaspY:rxhepL -0.09049 0.91348 0.07458 -1.213 0.225 ## rxaspY:rxhepH 0.03595 1.03661 0.07415 0.485 0.628 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## rxaspY 0.9636 1.0378 0.8847 1.049 ## rxhepL 1.0835 0.9230 0.9792 1.199 ## rxhepH 1.0431 0.9587 0.9416 1.155 ## rxaspY:rxhepL 0.9135 1.0947 0.7893 1.057 ## rxaspY:rxhepH 1.0366 0.9647 0.8964 1.199 ## ## Concordance= 0.513 (se = 0.004 ) ## Rsquare= 0 (max possible= 0.987 ) ## Likelihood ratio test= 7.98 on 5 df, p=0.1574 ## Wald test = 8.02 on 5 df, p=0.1554 ## Score (logrank) test = 8.02 on 5 df, p=0.1551 I test again the joint significancy of the interaction terms using the Wald test: idx = grepl(&quot;:&quot;, names(coef(fit))) W = t(coef(fit)[idx]) %*% solve(vcov(fit)[idx, idx]) %*% coef(fit)[idx] c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx))) ## W df p-value ## 2.3929136 2.0000000 0.3022633 Analogously as before, the interaction is not significantly different than zero. I re-fit the model without the interaction term: # library(survival) re_fit = coxph(Surv(t, d) ~ rxasp + rxhep, data = ist) summary(re_fit) ## Call: ## coxph(formula = Surv(t, d) ~ rxasp + rxhep, data = ist) ## ## n= 19378, number of events= 4315 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## rxaspY -0.05079 0.95048 0.03046 -1.668 0.0954 . ## rxhepL 0.03641 1.03708 0.03725 0.978 0.3283 ## rxhepH 0.05991 1.06174 0.03707 1.616 0.1061 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## rxaspY 0.9505 1.0521 0.8954 1.009 ## rxhepL 1.0371 0.9642 0.9641 1.116 ## rxhepH 1.0617 0.9419 0.9873 1.142 ## ## Concordance= 0.511 (se = 0.004 ) ## Rsquare= 0 (max possible= 0.987 ) ## Likelihood ratio test= 5.58 on 3 df, p=0.1337 ## Wald test = 5.59 on 3 df, p=0.1333 ## Score (logrank) test = 5.59 on 3 df, p=0.1332 Testing the significancy of the heparin treatment using the Wald test: idx = grepl(&quot;^rxhep&quot;, names(coef(fit))) W = t(coef(fit)[idx]) %*% solve(vcov(fit)[idx, idx]) %*% coef(fit)[idx] c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx))) ## W df p-value ## 2.4963250 2.0000000 0.2870317 Treatment with heparin is not statistically significantly associated with risk of death; besides that, the effect size of heparin treatment is small: approximately 4% and 6% risk increase for heparin treatment modalities L and H compared to no heparin treatment, respectively. The treatment with aspirin is also barely significantly different than zero, assuming a significancy level \\(\\alpha = 0.10\\), with a p-value of 0.0954. The effect size is comparable to the estimated effect size obtained with the Weibull model, approximately 5% risk reduction for treatment with aspirin compared to no aspirin treatment. References "],
["sa-advances.html", "1.7 Advances in survival analysis", " 1.7 Advances in survival analysis There are several extensions of the statistical methods presented in this Chapter: I will briefly introduce some of them in this Section, without going into great detail as that would be beyond the scope of this report. The proportional hazards models have been extended to include time-dependent covariates and time-dependent covariate effects; additionally, the Cox model has been extended to allow stratification by a given factor (details in Kalbfleisch and Prentice (2011) and Kleinbaum and Klein (2012)). Parametric survival models have been generalised to allow combinations of linear predictors and penalised smoothers for the effect of time and covariates, both in the proportional hazards and proportional odds framework (Liu, Pawitan, and Clements 2016). More generally, models have been developed to account for competing events and multi-state diseases, even with intermediate states, and for modelling a wide range of multivariate survival data (Geskus 2015; Crowder 2016). Finally, the main advances that I will discuss further are models with random effects and joint models for longitudinal and survival data, in Chapters 2 and 3 respectively. --> References "],
["smre.html", "2 Survival models with random effects", " 2 Survival models with random effects Random effects models are a kind of hierarchical model in which the data is assumed to have some sort of hierarchical structure: imagine having individual patients data clustered into families, cities, regions, and so on. It is also assumed that individuals are homogeneous within hierarchical unit, heterogeneous between different units. In comparison, fixed effects models do not take into account any hierarchy in the data. In biostatistics, also, the terms fixed effects and random effects have a special meaning, referring to the population-average and subject-specific effects, respectively, with the latter generally assumed to be unknown, unobserved variables. Random effects models are generally used to analyse hierarchical data with a continuous, normally distributed outcome (e.g. hemoglobin levels, inflammation markers, …); such models are referred to as linear mixed-effects models, as they can incorporate both fixed and random effects, and generalise the linear regression model. Additionally, when data consists in multiple observations for a given individual over time (and therefore the first level of clustering consists in the individual himself) the term longitudinal data is used. It is possible to encounter hierarchical data originating from a variety of ditribution from the exponential family such as the Poisson, Gamma, and Binomial distribution. Linear mixed-effects models can be generalised to include such data, and these models are generally referred to as generalised linear mixed-effects models. Practically speaking, the generalisation is analogous to generalising linear models to generalised linear models. It is also possible to relax the normality assumption for continuous, hierarchical data and model the median (or any quantile, really) rather than the mean. Such models are called linear quantile mixed-effects models, and generalise the linear mixed-effects models as quantile regression is generalising the linear model. Survival data can present a hierarchical structure too; for instance, data could be clustered in geographical areas, institutions, or patients themselves. Meta-analysis of individual-patient data are a common example of survival data (when the outcome is time to event, of course) with some hierarchical structure; another example is given by repeated-events data, such as infections or acute recurrent events, in which the first level of the hierarchical structure consists in the patient. Another example of survival data with biological cluster is given by twin data, in which siblings share some genetic factors. This heterogeneity structure often leads to violation of the implicit assumption that populations are homogeneous: sometimes it is impossible to include all relevant risk factors, or maybe such risk factors are not known at all. The result is unobserved heterogeneity. The simplest survival model with random effects is the univariate frailty model, in which a random effect - named frailty - is included in the model to account for the unobserved heterogeneity. The univariate frailty model can be generalised by allowing the frailty term to be shared between observations belonging to the same cluster of data. The resulting models are named shared frailty model. The frailty term generally acts multiplicatively on the baseline hazard, and it is modelled on the hazard scale; it is possible to alternatively formulate the model in terms of random effects rather than frailties, by including the frailty as an additive term on the log-hazard scale. I will introduce the univariate frailty model in Section 2.1, and generalise it to allow shared frailty terms in Section 2.2. Finally, I will present the alternative formulation in terms of random effects rather than frailties in Section 2.3. A comprehensive treatment of frailty models in survival analysis is given in Hougaard (2000) and Wienke (2010). References "],
["smre-univariate-frailty.html", "2.1 Univariate frailty models", " 2.1 Univariate frailty models In those settings where risk factors are not measured, their relevance is unknown, or it is not known whether such risk factor exist at all or not, it is useful to consider two sources of variability in survival analysis: variability accounted for by observable risk factors included in the model and heterogeneity caused by unknown covariates. The unobserved heterogeneity is described by the frailty term, which is assumed to follow some distribution. Formally: \\[ h(t|\\alpha) = \\alpha h_0(t), \\] where \\(\\alpha\\) is a non-observed frailty effect and \\(h_0(t)\\) is the baseline hazard function. The random variable \\(\\alpha\\), the frailty term, is chosen to have a distribution \\(f(\\alpha)\\) with expectation \\(E(\\alpha) = 1\\) and variance \\(V(\\alpha) = \\sigma ^ 2\\). \\(V(\\alpha)\\) is interpretable as a measure of heterogeneity across the population in baseline risk: as \\(\\sigma ^ 2\\) increases the values of \\(\\alpha\\) are more dispersed, with greater heterogeneity in \\(\\alpha h_0(t)\\). Underlying assumptions are: the frailty is time independent, and it acts multiplicatively on the underlying baseline hazard function. Introducing observed covariates into the model: \\[ h(t|X,\\alpha) = \\alpha h_0(t) \\exp(X \\beta) = \\alpha h(t | X), \\] with \\(X\\) and \\(\\beta\\) covariates and regression coefficients, respectively. Given the relationship between hazard and survival function, it can be showed that the individual survival function conditional on the frailty is \\(S(t | \\alpha) = S(t) ^ \\alpha\\). The population (i.e. marginal, or unconditional) survival function is obtained by integrating out the frailty from the conditional survival function: \\[ S(t) = \\int_0^{+\\infty} \\left[ S(t) \\right] ^ \\alpha f(\\alpha) \\ d\\alpha \\] The individual contribution to the likelihood (assuming no delayed entry) is conditional on the unobserved frailty \\(\\alpha\\) \\[ L_i = \\prod_{i = 1} ^ {n} \\left( \\alpha h_0(t_i)\\exp(X_i \\beta) \\right) ^ {d_i} \\exp(-\\alpha H_0(t_i) \\exp(X_i \\beta)), \\] with \\(d_i\\) event indicator variable, \\(H_0(t_i)\\) cumulative baseline hazard, and \\(t_i\\) observed survival time - all relative to the \\(i\\)-th individual. Different choices for the frailty distribution are possible. Assigning a probability distribution implies that the frailty can be integrated out of the likelihood function. After this integration, the likelihood can be maximized in the usual way if an explicit form of it exists. Otherwise, more sophisticated approaches like numerical integration or Markov Chain Monte Carlo methods need to be applied. The most often used frailty distributions are the gamma and the log-normal distribution; the positive stable and the inverse Gaussian distribution are also common. Assuming that the frailty \\(\\alpha\\) has a Gamma distribution is convenient: it has the appropriate range \\((0, \\infty)\\) and it is mathematically tractable. A Gamma distribution with parameters \\(a\\) and \\(b\\) has density \\[ f(x) = \\frac{x ^ {a - 1} \\exp(- x / b)}{\\Gamma(a)b ^ a}; \\] by choosing \\(a = 1 / \\theta\\) and \\(b = \\theta\\) the resulting distribution has expectation \\(1\\) and finite variance \\(\\theta\\). In these settings, the model is analytically tractable: the population survival function has the form \\[ S(t) = (1 - \\theta \\log(S(t))) ^ {-1/\\theta}; \\] the likelihood follows by substitution. Estimating such model becomes therefore straightforward, which likely contributed to the popularity of Gamma frailty models. Together with the Gamma distribution, the log-normal distribution is the most commonly used frailty distribution, given its strong ties to random effect models; more on that in Section 2.3. Hence, assuming a log-normal distribution with a single parameter \\(\\theta &gt; 0\\) (for comparison with the mathematically tractable Gamma frailty model), with density \\[ f(x) = (2 \\pi \\theta) ^ {-\\frac{1}{2}} x ^ {-1} \\exp \\left( -\\frac{(\\log x) ^ 2}{2 \\theta} \\right), \\] the resulting model has a frailty whose expectation is finite. Nevertheless, this frailty distribution cannot be integrated out of the survival function analytically to obtain the population survival function, and therefore requires more complex estimation procedures involving numerical integration (taking the maximum likelihood approach), mathematical approximations (such as the Laplace approximation), or Markov Chain Monte Carlo methods (Clayton 1991; Sinha et al. 2008). Further details in Chapter 4. References "],
["smre-shared-frailty.html", "2.2 Shared frailty models", " 2.2 Shared frailty models Further generalising the model presented in Section 2.1, it is possible for the frailty effect \\(\\alpha\\) to be shared between clusters of study subjects. Specifically, for the \\(j\\)-th observation in the \\(i\\)-th cluster: \\[ h_{ij}(t | \\alpha_i) = \\alpha_i h(t|X_{ij}). \\] The conditional survival function is: \\[ S_{ij}(t | \\alpha_i) = S_{ij}(t) ^ {\\alpha_i}. \\] In this setting, the cluster-specific contribution to the likelihood is obtained by calculating the cluster-specific likelihood conditional on the frailty, consequently integrating out the frailty itself: \\[ L_i = \\int_A L_i(\\alpha_i) f(\\alpha_i) \\, d\\alpha, \\] with \\(f(\\alpha)\\) the distribution of the frailty, \\(A\\) its domain, and \\(L_i(\\alpha_i)\\) the cluster-specific contribution to the likelihood, conditional on the frailty. The cluster-specific contribution to the likelihood is \\[ L_i(\\alpha_i) = \\alpha_i ^ {D_i} \\prod_{j = 1} ^ {n_i} S_{ij}(t_{ij}) ^ {\\alpha_i} h_{ij}(t_{ij}) ^ {d_{ij}}, \\] with \\(D_i = \\sum_{j = 1} ^ {n_i} d_{ij}\\). Analogously as before, analytical formulae can be obtained when \\(\\alpha_i\\) follows a Gamma distribution: \\[ L_i = \\left[ \\prod_{j = 1} ^ {n_i} h_{ij}(t_{ij}) ^ {d_{ij}} \\right] \\frac{\\Gamma (1 / \\theta + D_i)}{\\Gamma (1 / \\theta)} \\left[ 1 - \\theta \\sum_{j = 1} ^ {n_i} \\log S_{ij}(t_{ij}) \\right] ^ {-1 / \\theta - D_i}; \\] further details in Gutierrez (2002). As in the univariate frailty model, assuming a log-normal distribution requires some numerical approximation to be performed, being the resulting model analytically intractable. References "],
["smre-random-effects.html", "2.3 Alternative formulation", " 2.3 Alternative formulation I mentioned briefly in Section 2.1 that the log-normal distribution for the frailty term has strong ties to random-effects models. Recall the formulation for a log-normal shared frailty model: \\[ h_{ij}(t | \\alpha_i) = \\alpha_i h(t | X_{ij}) = \\alpha_i h_0(t) \\exp(X_{ij} \\beta), \\] with \\(\\alpha_i\\) following a log-normal distribution. It is possible to formulate the same model on the log-hazard scale as \\[ h_{ij}(t | \\alpha_i) = h_0(t) \\exp(X_{ij} \\beta + \\eta_i), \\] with \\(\\eta_i = \\log \\alpha_i\\). \\(\\eta_i\\) results being normally distributed with parameters \\(\\mu\\) and \\(\\sigma ^ 2\\) related to those of the log-normal distribution by the relationship \\[ E(\\alpha_i) = \\exp(\\mu + \\sigma ^ 2 / 2) \\] and \\[ Var(\\alpha_i) = \\exp(2 \\mu + \\sigma ^ 2) (\\exp(\\sigma ^ 2) - 1) \\] By formulating the model on the log-hazard scale, the frailty term has a direct interpretation as a random intercept in the model; that is, the heterogeneity is modelled by allowing the model intercept to vary between clusters. Consequently, it is possible to further extend this model by allowing random covariates effects, potentially ranging over multiple levels of clustering. Using the usual mixed-effects models notation: \\[ h_{ij}(t | b_i) = h_0(t) \\exp(X_{ij} \\beta + Z_{i} b_i), \\] with \\(X_{ij}\\) representing the design matrix for the fixed effects \\(\\beta\\) and \\(Z_i\\) representing the design matrix for the random effects \\(b_i\\). Any distribution or functional form can be assumed for \\(h_0(t)\\) (Crowther, Look, and Riley 2014), or it is possible to leave it unspecified altogether yielding a Cox model with random effects (Ripatti and Palmgren 2000; Therneau, Grambsch, and Pankratz 2003). --> References "],
["jm.html", "3 Joint models for longitudinal and survival data", " 3 Joint models for longitudinal and survival data It is increasingly common for observational studies and trials to follow participants over time, recording abundant data on clinical features throughout the duration of the study. Moreover, routinely collected healthcare consumption data and population registries are being used more and more for research purpose, after being linked with other data sources (and each other). As a consequence, applied researchers often encounter longitudinally recorded covariates to account for when studying the clinical outcome of interest (e.g. time to event, that is what I will focus on). Researchers then face two options: (1) select only one of the multiple values per individual and analyse as such, ignoring part of the available data, or (2) take into account the potential dependency and association between the repeatedly measured covariates and the outcome interest. The latter is usually the sensible choice, as the longitudinal data can be important predictors or surrogates of the time to event outcome. A powerful tool to jointly model longitudinal and time-to-event data is joint models for longitudinal and time to event data, in which the longitudinal and survival processes are modelled jointly into a single model allowing to infer their association. The development of such models was motivated by HIV/AIDS clinical trials, in which immune response (in terms of CD4 lymphocite cells count) was recorded over the duration of the trial and the association with survival was of interest. Seminal works on the topic are the papers by Wulfsohn and Tsiatis (1997), Tsiatis and Davidian (2004), Henderson, Diggle, and Dobson (2000), Pawitan and Self (1993); a more recent tractation of the topic is in Ibrahim, Chu, and Chen (2010), Rizopoulos (2012), Gould et al. (2015). Previous attempts to tackle this problem consisted in (1) fitting a time-dependent Cox model (Cox 1972) by splitting individual rows every time a new observation from the longitudinal covariate becomes available, and (2) by using two-stages methods in which the longitudinal and survival data were modelled separately (Tsiatis, Degruttola, and Wulfsohn 1995). Nevertheless, it has been showed that joint modeling increases efficiency and reduces bias (Hogan and Laird 1998), while improving predictions at the same time (Rizopoulos et al. 2014). In this Chapter I will focus on the basic joint model for longitudinal and survival data, with a single longitudinal process. I will present its formulation in Section 3.1, and the estimation process in Section 3.2. However, several extensions of the basic joint model presented in this Chapter have been proposed during the years, as the topic has received considerable attention. A review on the state of the art in joint models with a single longitudinal process is given by Gould et al. (2015). Of course, the joint model has been extended to allow incorporating multiple longitudinal processes at one, measured intermittently and not necessarily at the same time or with the same structure of the association with the survival outcome: a review on recent developments, software, and persisting issues is given by Hickey et al. (2016). References "],
["jm-formulation.html", "3.1 Model formulation", " 3.1 Model formulation A joint model for longitudinal and survival data consists of two components: a model for the longitudinal part (I will be assuming a single longitudinal trajectory from now on for simplicity) and a model for the survival part. These two components will then share a set of parameters that will describe the association between the two processes. In literature, the dominant approach seems to be allowing the two components to share random effects; I will follow this approach. Building on the notation from Section 1.3, let \\(y_{ij} = \\{ y_{ij}(t_{ij}) \\ \\forall \\ j = 1, \\dots, n_i \\}\\) be the observed longitudinal response for the \\(i\\)th subject, with \\(y_{ij}(t_{ij})\\) the observed response at time \\(t_{ij}\\) and \\(n_i\\) the number of longitudinal observations. The longitudinal component of the joint model is modelled within the mixed-effects framework (P. J. Diggle et al. 2013), as longitudinal data is likely measured intermittently and with error. Therefore: \\[ y_i(t) = m_i(t) + \\epsilon_i(t), \\ \\epsilon_i(t) \\sim N(0, \\sigma^2) \\] and \\[ m_i(t) = X_i(t) \\beta + Z_i(t) b, \\ b \\sim N(0, \\Sigma) \\] with \\(X_i(t)\\) and \\(Z_i(t)\\) the time-dependent design matrices for the fixed and random effects, respectively. \\(y_i(t)\\) represents the observed longitudinal trajectory at time \\(t\\), which could be decomposed into the true longitudinal trajectory \\(m_i(t)\\) plus the measurement error \\(\\epsilon_i(t)\\). The survival component of the joint model is modelled using a proportional hazards time to event model, given the true unobserved longitudinal trajectory up to time \\(t\\), i.e. \\(M_i(t) = \\{m_i(s) \\ \\forall \\ 0 \\le s \\le t\\}\\): \\[ h(t | M_i(t)) = h_0(t) \\exp(W \\psi + \\alpha m_i(t)), \\] where \\(h_0(t)\\) is the baseline hazard function and \\(W\\) is a vector of time-fixed covariates with their regression parameters \\(\\psi\\). \\(\\alpha\\) is the association parameter that links the longitudinal component and the survival component of the joint model; it can be intepreted as the change in log-hazard ratio for a unit increase in the true longitudinal trajectory \\(m_i(t)\\), at time \\(t\\). This specific form of the association parameter is also known as the current value parametrisation; additional association structures are available, allowing for instance interactions, association with the slope of the trajectory or its cumulative effect, and so on. Further details in Rizopoulos (2012). The survival function follows as \\[ S(t | M_i(t)) = \\exp \\left( -\\int_0 ^ t h_0(u) \\exp(W \\psi + \\alpha m_i(u)) \\ du \\right) \\] Finally, regarding \\(h_0(t)\\): the choice of the baseline hazard function follows the usual rationale. It can be left unspecified, therefore resulting in a Cox model for the survival component of the joint model, or it can be specified using a parametric distribution (e.g. a distribution from Section 1.5.1) or some flexible alternative (Crowther, Abrams, and Lambert 2012). Nevertheless, Hsieh, Tseng, and Wang (2006) showed that choosing the Cox model for the survival component yields standard errors that are underestimated; consequently, bootstrap is required to obtain correct standard errors in that situation. References "],
["jm-estimation.html", "3.2 Estimation process", " 3.2 Estimation process Estimation of a joint model for longitudinal and survival data is a non-trivial task. The complexity of jointly modelling the longitudinal component and the survival component motivated using a two-stages procedure as mentioned in Section 3. With that approach, the longitudinal component is modelled and estimated separately; consequently, subject-specific predictions from the longitudinal model are produced and plugged into the survival model as time-varying covariates. Despite the simplicity of this approach, though, it has been showed that it produces substantial bias and poor coverage (Tsiatis and Davidian 2001; Sweeting and Thompson 2011). Therefore, an approach that models both processes jointly is required. in particular, two approaches are predominant: a full likelihood approach, and a Bayesian approach; both have appealing characteristics, but they share the feature of being computationally intensive. Focusing on the full likelihood approach, it is possible to formulate the joint likelihood (Rizopoulos 2012) for the overall parameter vector \\(\\theta = \\{\\theta_t, \\theta_y, \\theta_b\\}\\), formed by the parameters of the survival component, the parameters of the longitudinal component, and the elements of the variance-covariance matrix of the random effects, respectively. The joint distribution of the survival time \\(T_i\\), the event indicator \\(d_i\\), and the longitudinal response \\(y_i\\), conditional on the random effects \\(b_i\\), can be expressed as: \\[ P(T_i, d_i, y_i | b_i, \\theta) = P(T_i, d_i | b_i, \\theta) P (y_i | b_i, \\theta), \\] with \\[ P(y_i | b_i, \\theta) = \\prod_{j = 1} ^ {n_i} P(y_i(t_{ij}) | b_i, \\theta). \\] It follows that the contribution to the log-likelihood for the \\(i\\)th patient is \\[ \\begin{aligned} \\log L(\\theta) &amp;= \\log \\int_{-\\infty} ^ {+\\infty} P(T_i, d_i, y_i, b_i; \\theta) \\ db_i \\\\ &amp;= \\log \\int_{-\\infty} ^ {+\\infty} P(T_i, d_i | b_i, \\theta_t) \\left[ \\prod_{j = 1} ^ {n_i} P(y_i(t_{ij}) | b_i, \\theta_y) \\right] P(b_i | \\theta_b) \\ db_i \\end{aligned} \\] with \\(P(T_i, d_i | b_i, \\theta_t)\\) the likelihood relative to the survival component of the model: \\[ \\begin{aligned} P(T_i, d_i | b_i, \\theta_t) &amp;= h_i(T_i | M_i(T_i), \\theta_t) ^ {d_i} S_i(T_i | M_i(T_i), \\theta_t) \\\\ &amp;= \\left[ h_0(T_i) \\exp(W \\psi + \\alpha m_i(T_i)) \\right] ^ {d_i} \\exp \\left[ -\\int_0^{T_i} h_0(u) \\exp(W \\psi + \\alpha m_i(u)) \\ du \\right], \\end{aligned} \\] \\(P(y_i(t_{ij}) | b_i, \\theta_y)\\) the likelihood of the longitudinal process at time \\(t_{ij}\\): \\[ P(y_i(t_{ij}) | b_i, \\theta_y) = (2 \\pi \\sigma ^ 2) ^ {-1/2} \\exp \\left[ -\\frac{(y_i(t_{ij}) - m_i(t_{ij})) ^ 2}{2 \\sigma ^ 2} \\right], \\] and \\(P(b_i | \\theta_b)\\) the density of the random effects: \\[ P(b_i | \\theta_b) = (2 \\pi) ^ {-q_b / 2} | \\Sigma | ^ {-1 / 2} \\exp \\left[- \\frac{b_i^T \\Sigma ^ {-1} b_i}{2}\\right] \\] \\(q_b\\) being the dimension of the random effects. Historically the full joint likelihood has been maximised using the Expectation-Maximisation algorithm (Dempster, Laird, and Rubin 1977); alternatively, it is possible to use general purpose optimisers to maximise the full joint likelihood via algorithms such as the Newton-Raphson algorithm. Nevertheless, significant computational challenges persist; I will discuss them further in Chapter 4. --> References "],
["compch.html", "4 Computational challenges in survival models with random effects", " 4 Computational challenges in survival models with random effects The models I presented in Chapter 2 and 3 present significant computational challenges during the estimation process. I showed how frailty models with a Gamma frailty are analytically tractable, as it is possible to obtain closed-form expressions for the marginal survival function and therefore the likelihood; conversely, including a log-normal frailty (or, correspondingly, random effects) in a survival model yields a survival function - and likelihood - that does not have a closed form. Recall the \\(i\\)th-custer-specific contribution to the likelihood for a shared frailty model: \\[ L_i(\\alpha_i) = \\alpha_i ^ {D_i} \\prod_{j = 1} ^ {n_i} S_{ij}(t_{ij}) ^ {\\alpha_i} h_{ij}(t_{ij}) ^ {d_{ij}} \\] The marginal survival function has the form \\[ S_{ij}(t_{ij}) = \\int_0^{+\\infty} \\left[ S_{ij}(t_{ij}) \\right] ^ \\alpha_i f(\\alpha_i) \\ d\\alpha_i \\] with \\(f(\\alpha_i)\\) a log-normal density function. This integral has no closed form, hence it is necessary to approximate it in order to obtain (1) marginal survival and (2) the likelihood. Analogously, recall the joint likelihood in joint models for longitudinal and survival data: \\[ \\log L(\\theta) = \\log \\int_{-\\infty} ^ {+\\infty} P(T_i, d_i, y_i, b_i; \\theta) \\ db_i. \\] Evaluating this likelihood requires evaluating an analytically intractable integral over a possibly multi-dimentional integral over the infinite domain; it is therefore necessary to use some method to approximate it numerically. Methods for approximating intractable integrals form the majority of this Chapter, with more details in Section 4.1. I will conclude with additional considerations on numerical methods in Section 4.2. "],
["compch-numintgr.html", "4.1 Numerical integration", " 4.1 Numerical integration The term numerical integration implies the approximation of the integral of a function; generally, it aims to use the minimum number of function evaluations possible as it tends to be numerically expensive. There is a variety of methods being proposed in literature to perform numerical integration; throughout this Section, I will focus on quadrature rules, i.e. any method that evaluates the function to be integrated at some points over the integration domain and combines the resulting values to obtain an approximation of the integral. Quadrature rules vary in complexity and accuracy, and generally accuracy improves as rules get more complex. Additionally, integration of functions in few dimensions is generally not too problematic; the task becomes more difficult when integrating over many dimensions as obtaining an acceptable level of accuracy often requires an unfeasible number of function evaluations. 4.1.1 Unidimensional functions The simplest method to approximate the integral of a unidimensional function numerically is given by the Riemann sum. A Riemann sum is an approximation of the integral of a continuous function \\(f(x)\\) over an integration domain \\([a,b]\\) by a finite sum, defined as: \\[ \\int_a^b f(x) \\ dx \\approx \\sum_{i = 1} ^ N f(x_i^{*})\\Delta(x_i), \\] with \\(P = \\{[x_0, x_1], [x_1, x_2], \\dots, [x_{N-1}, x_N]\\}\\) a partition of \\([a,b]\\) such that \\(a = x_0 &lt; x_1 &lt; x_2 &lt; \\dots &lt; x_{N-1} &lt; x_N = b\\), \\(\\Delta(x_i) = x_i - x_{i-1}\\), and \\(x_i^{*} \\in [x_{i-1}, x_i]\\). \\(x_i^{*}\\) can be defined in many ways: it could be the left extremity of \\(\\Delta(x_i)\\), the right extremity, the midpoint, or many more. In particular, when choosing \\(x_i^{*}\\) as the midpoint of the interval, I obtain the so called midpoint rule; it approximates the integral of a continuous function \\(f(x)\\) by the area under a set of \\(N\\) step functions, with the midpoint of each matching \\(f\\): \\[ \\int_a^b f(x) \\ dx \\approx \\frac{b - a}{N} \\sum_{i = 1}^N f(a + (i - 0.5)(b - a) / N) \\] An alternative to the midpoint rule is given by the trapezoidal rule, which approximates the area under a continuous function \\(f(x)\\) as a trapezoid and then computes its area: \\[ \\int_a^b f(x) \\ dx \\approx (b - a) \\left[ \\frac{f(a) + f(b)}{2} \\right] \\] it works best when partitioning the integration area into many subinterval, applying the trapezoidal rule to all of them, and then sum the results: \\[ \\int_a^b f(x) \\ dx \\approx \\sum_{i = 1} ^ N \\frac{f(x_{k - 1}) + f(x)}{2} \\Delta(x_k), \\] with \\({x_k}\\) a partition of \\([a, b]\\) such that \\(a = x_0 &lt; x_1 &lt; x_2 &lt; \\dots &lt; X_{N-1} &lt; x_N = b\\) and \\(\\Delta(x_k) = x_k - x_{k - 1}\\) the length of the \\(k\\)th subinterval. Accuracy of the midpoint and trapezoidal rules depends on the number of steps (subintervals) \\(N\\) used to approximate the function, but so does complexity (computationally speaking). The only requirement for applying these rules is that one needs to be able to evaluate the function \\(f(x)\\) at a given point over its domain. If \\(f(x)\\) is cheap to evaluate, than the midpoint and trapezoidal rules may be just fine; otherwise, it would be better to move onto more complicated methods that yield more accurate results. A first method that is only slightly more complicated but yields better results is the Simpson’s rule. It works analogously to the midpoint and trapezoidal rule, but using a smooth quadratic interpolant which takes the same values as \\(f(x)\\) at the extremities of the integration interval \\([a, b]\\) and at the midpoint \\(m = (a + b) / 2\\): \\[ \\int_a^b f(x) \\ dx \\approx \\frac{b - a}{6} \\left[ f(a) + 4f((a + b) / 2) + f(b) \\right] \\] Analogously as the trapezoidal rule, it is possible to obtain greater accuracy by splitting the integration interval into many subintervals, applying the Simpson’s rule to each subinterval, and sum the results. Second, it is possible to show that by choosing carefully the points at which to evaluate \\(f(x)\\) and the weights assigned to each point it is possible to obtain an exact approximation of the integral of any polynomial of degree \\(2N - 1\\) or less with \\(N\\) function evaluations (proof in Monahan (2011)). Let \\(f(x)\\) be a function of order \\(2N - 1\\) or less to integrate over a domain \\([a,b]\\); let \\(w(x)\\) be a weight function. The quadrature formula is defined as: \\[ \\int_a^b f(x) w(x) \\ dx = \\sum_{i = 1} ^ N w_i f(x_i) \\] Depending on the choice of the weighting function \\(w(x)\\), different Gaussian quadrature rules can be obtained. When \\(w(x) = 1\\), the associated polynomials are Legendre polynomials, the quadrature rule is then named Gauss-Legendre quadrature rule, and it allows integrating over the interval \\([-1,1]\\). The integration points are then obtained as the the \\(N\\) roots of the Legendre polynomials: \\(x = \\{x_1, x_2, \\dots, x_N\\}\\). When choosing the weight function \\(\\exp(-x)\\) the associated polynomials are Laguerre polynomials, the quadrature rule is named Gauss-Laguerre quadrature rule, and the integration domain is \\([0, +\\infty)\\). Finally, when choosing the weight function \\(\\exp(-x^2)\\) the associated polynomials are Hermite polynomials, the quadrature rule is named Gauss-Hermite quadrature rule, and the integration domain is \\((-\\infty, +\\infty)\\). Interestingly, the Gauss-Hermite quadrature can be re-formulated using a normal density kernel with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as weighting function: \\[ \\int_{-\\infty}^{+\\infty} f(x) \\phi(x | \\mu, \\sigma^2) \\ dx = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\int_{-\\infty}^{+\\infty} f(x) \\exp\\left[ -\\frac{(x - \\mu) ^ 2}{2 \\sigma ^ 2} \\right] \\ dx \\] By applying the change of variable \\(x = \\mu + \\sigma \\sqrt{2} r\\), the integral to approximate becomes \\[ \\int_{-\\infty}^{+\\infty} f(x) \\phi(x | \\mu, \\sigma^2) \\ dx = \\frac{\\sqrt{2} \\sigma}{\\sqrt{2 \\pi} \\sigma} \\int_{-\\infty}^{+\\infty} f(\\mu + \\sigma \\sqrt{2} r) \\exp (-r^2) \\ dr, \\] which can then be approximated by the quadrature rule \\[ \\frac{\\sqrt{2} \\sigma}{\\sqrt{2 \\pi} \\sigma} \\int_{-\\infty}^{+\\infty} f(\\mu + \\sigma \\sqrt{2} r) \\exp (-r^2) \\ dr \\approx \\sum_{i = 1} ^ N f(\\mu + \\sigma \\sqrt{2} r) \\frac{w_i}{\\sqrt{\\pi}}. \\] That is, a quadrature rule based on the normal kernel as weight function with nodes \\(\\mu + \\sigma \\sqrt{2} x_i\\) and weights \\(w_i / \\sqrt{\\pi}\\) (\\(x_i\\) and \\(w_i\\) being the nodes and weights of the corresponding \\(N\\)-points Gauss-Hermite quadrature rule based on the usual weighting function). A slightly more complicated version of Gaussian quadrature is given by the Gauss–Kronrod quadrature formula. In the Gauss-Kronrod quadrature rule the evaluation points are chosen dynamically so that an accurate approximation can be computed by re-using the information produced by the computation of a less accurate approximation. In practice, integration points from previous iterations can be reused as part of the new set of points, whereas usual Gaussian quadrature would require recomputation of all abscissas at each iteration. This is particularly important when some specified degree of accuracy is needed but the number of points needed to achieve this accuracy is not known ahead of time. Despite this, the quadrature rule is the same as before, i.e. \\(\\int_{a}^{b} f(x) \\ dx \\approx \\sum_{i = 1} ^ {n} w_{i} f(x_{i})\\). Gauss-Kronrod quadrature rule is implemented in R as the integrate() function. 4.1.2 Multidimensional functions Finally, all the methods I presented so far only only apply to the integration of unidimensional functions. It is of course possible to extend quadrature rules to multidimensional settings, by recursively applying unidimensional quadrature rules. Say I want to approximate the integral of a bidimensional function \\(f(x, y)\\); the bidimensional Gaussian quadrature rule has the form: \\[ \\int_X \\int_Y f(x, y) \\ dx \\ dy \\approx \\sum_j \\sum_i w_j w_i f(x_j, y_i) \\] This can be extended to any number of dimensions \\(d\\), but it gets very computationally expensive very quickly as a \\(N\\)-points rule requires \\(N^d\\) function evaluations. A better option when the number of dimensions \\(d\\) to integrate over is high is given by Monte Carlo integration. Consider integrating a multidimensional function \\(f(x)\\) over some region \\(\\Omega\\) of volume \\(V(\\Omega)\\): \\[ I_{\\Omega} = \\int_{\\Omega} f(x) \\ dx = E[f(U)] V(\\Omega), \\] with \\(U \\sim\\) uniform over \\(\\Omega\\). Drawing \\(N\\) uniform random vectors \\(u_i\\) an estimator for \\(I_{\\Omega}\\) is \\[ \\hat{I}_{\\Omega} = \\frac{V(\\Omega)}{N} \\sum_{i = 1} ^ N f(u_i), \\] and this defines Monte Carlo integration. The variance of the estimated integral \\(\\hat{I}_{\\Omega}\\) follows, assuming the \\(u_i\\) are independenent, as \\(var(\\hat{I}_{\\Omega}) = \\frac{V(\\Omega)^2}{N^2} N var(f(u_i))\\). More details in Monahan (2011). Luckily, both Gaussian quadrature and Monte Carlo integration can be tweaked to improve accuracy and convergence rates: two appealing options are, respectively, adaptive Gaussian quadrature and importance sampling. Adaptive Gaussian quadrature works best when using the Gauss-Hermite rule with the normal density kernel as weighting function; in a multivariate setting, using an iterative algorithm, it is possible to update the mean vector \\(M\\) and variance-covariance matrix \\(\\Sigma\\) of the multivariate normal density at each step (e.g. using empirical Bayes estimates of \\(M, \\Sigma\\)) to better adapt the grid of quadrature points to the actual shape of the integral to approximate. Conversely, Monte Carlo integration works best when it is possible to draw a sample from the target distribution (i.e. the distribution of the integral to approximate); unfortunately, that is rarely the case in practice. The idea of importance sampling consists then in drawing a sample from a proposal distribution and then re-weight the estimated integral using importance weights to better adapt to the target distribution. References "],
["compch-other.html", "4.2 Other considerations", " 4.2 Other considerations 4.2.1 Cancellation error, precision, and arithmetic over- and under-flow One of the problems when doing calculations on a computer is cancellation error (or round-off error). That occurs as a side effect of performing finite-precision arithmetic, as computers can store numbers in memory using a finite number of digits. Cancellation error causes the number of significant digits in the result to be reduced unacceptably; when a sequence of calculations is performed, cancellation errors add up significantly, altering the final result. Cancellation error can be easily reproduced: a &lt;- 1e16 b &lt;- 1e16 + pi b - a ## [1] 4 pi ## [1] 3.141593 b - a should be \\(\\pi\\), instead it is 4. Analogously, arithmetic over- and under-flow is a condition that happens when the result of a calculation is, respectively, bigger or smaller than the minimum or maximum value that a given machine can store in memory. On the laptop used to produce this report, the smallest (and largest) floating-point number that the machine can represent are: .Machine$double.xmin ## [1] 2.225074e-308 .Machine$double.xmax ## [1] 1.797693e+308 Next, precision. Machines can only distinguish numbers that they can represent as different. For instance: a = 1 b = (.Machine$double.eps ^ 2) c = (.Machine$double.neg.eps ^ 2) d = a + b e = a - c # The following equalities should be FALSE a == d ## [1] TRUE a == e ## [1] TRUE # Check that b, c are not 0 b == 0 ## [1] FALSE c == 0 ## [1] FALSE In this case, .Machine$double.eps and .Machine$double.neg.eps are the smallest positive floating-point numbers \\(x\\) such that \\(1 + x \\ne 1\\) and \\(1 - x \\ne 1\\), respectively. It is necessary to keep this potential problems in mind when doing numerical calculation using finite-precision arithmetic; for instance, a common situation where we may incur in arithmetic over- or under-flow is when maximising a likelihood. That is, the product of the individual contributions to the likelihood may be a number so large (or so small) that the computer cannot distinguish it from \\(\\pm \\infty\\), or rounding error may seriously affect the results. This specific example is easy to fix by using the log-likelihood instead, as the sum of the logarithm of the individual contributions behaves much better; nevertheless, this problem may not be always evident nor as easy to solve. 4.2.2 Numerical differentiation Numerical differentiation is a series of algorithms to numerically estimate the derivative of a function. They tend to be computationally less demanding than numerical integration methods, but they are more sensitive to cancellation error. The easiest method for approximating the derivative of a function is to use finite difference approximation. Say I want to estimate the first derivative of a function \\(f(\\cdot)\\) at \\(x\\); the finite difference approximation of the derivative \\(f&#39;(x)\\) is calculated as \\[ f&#39;(x) \\approx \\frac{f(x + h) - f(x)}{h}, \\] for a small \\(h\\). This formula is affected by both truncation error (as it derives from a truncated Taylor series expansion of \\(f(x)\\)) and cancellation error (as a machine works with finite-precision arithmetic). It is necessary to choose a value \\(h\\) that gives a good balance between the two errors: it can be showed that a good choice in most cases is \\(h = \\sqrt{\\epsilon}\\), with \\(\\epsilon\\) being the machine precision. The formula I presented for finite difference approximation is also known as forward differencing; alternatively, it is possible to use methods such as central differencing (\\([f(x + h) - f(x - h)] / 2h\\), more accurate but more computationally expensive) and backward differencing (\\([f(x) - f(x - h)] / h\\)). Other methods are the complex method, which requires the function to be able to handle complex values and it is extremely powerful but with limited applicability, and the Richardson’s extrapolation method, which is more accurate but slower than finite differencing. All these methods are implemented in R in the numDeriv package, which sets the standard for numerical differentiation. 4.2.3 Numerical root finding Root-finding algorithms are algorithms for finding the values \\(x\\) such that \\(f(x) = 0\\), for a given continuous function \\(f(\\cdot)\\). Such values \\(x\\) are named roots (or zeros) of a function. Most root-finding algorithms are based on the intermediate value theorem, which states that if a continuous function has values of opposite sign at the end points of an interval then the function has at least one root in the interval. For instance, the easiest root-finding method is the bisection method: let \\(f(x)\\) be a continuous function, for which one knows an interval \\([a, b]\\) such that \\(f(a)\\) and \\(f(b)\\) have opposite sign. Let \\(c = (a + b) / 2\\) be the midpoint the bisect the interval: now, either \\(f(a)\\) and \\(f(c)\\) or \\(f(c)\\) and \\(f(b)\\) have opposite sign, and one has in fact divided by two the size of the interval. One can iterate this method until the difference between the extremities of the interval is small enough (e.g. \\(&lt;1 \\times 10^{-8}\\)). Another well established method is the secant method: it uses a succession of roots of secant lines to approximate the root of a function \\(f(x)\\). Starting with values \\(x_0\\) and \\(x_1\\), a line is constructed between \\((x_0, f(x_0))\\) and \\((x_1, f(x_1))\\): \\[ y = \\frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_1) + f(x_1) \\] The root of this line is \\[ x = x_1 - f(x_1) \\frac{x_1 - x_0}{f(x_1) - f(x_0)} \\] Now, we set \\(x_2 = x\\) and we iterate this method until the difference between the extremities of the interval is small enough (e.g. \\(&lt;1 \\times 10^{-8}\\)). The secant method is also known as a linear interpolation method; it is also possible to use higher order interpolation, specifically quadratic interpolation, to find the root of a function using the same rationale presented for the secant method. Specifically, starting with three starting values \\(x_0\\), \\(x_1\\), \\(x_2\\) and their function values \\(f(x_0)\\), \\(f(x_1)\\), \\(f(x_2)\\), applying the Lagrange interpolation formula to interpolate the inverse of \\(f(x)\\) yields the equation \\[ \\begin{aligned} f^{-1}(y) &amp;= \\frac{(y - f(x_1))(y - f(x_2))}{(f(x_0) - f(x_1))(f(x_0) - f(x_2))}x_{0} + \\frac{(y - f(x_0))(y - f(x_2))}{(f(x_1) - f(x_0))(f(x_1) - f(x_2))}x_{1} + \\\\ &amp;\\frac{(y - f(x_0))(y - f(x_1))}{(f(x_2) - f(x_0))(f(x_2) - f(x_1))}x_{2} \\end{aligned} \\] Substituting \\(y = 0\\) in the above equation yields the recursion formula, to be iterated until a desired precision is reached. Finally, a well-established and robust method is the Brent-Dekker method, implemented in R with the uniroot() function. It combines the three methods presented before, trying to use the secant or quadratic interpolation method first - as they tend to converge faster to a solution - but falling back to the bisection method if necessary, for its robustness properties. More details on the Brent-Dekker method in Brent (1973). --> References "],
["simsurv.html", "5 Simulating survival data", " 5 Simulating survival data In this Chapter I will present a flexible and efficient method to simulate survival data from a variety of parametric distributions, first introduced by Bender, Augustin, and Blettner (2005). Then, I will present an extension that allows simulating from a variety of complex distributions proposed by Crowther and Lambert (2013). Let \\(h(t) = h_0(t) \\exp(X \\beta)\\) be the hazard function of a proportional hazards model, with \\(h_0(t)\\) baseline hazard function and \\(X\\) a matrix of covariates with regression coefficients \\(\\beta\\). Let \\(H_0(t) = H_0(t) \\exp(X \\beta)\\) be the corresponding cumulative hazard function, with \\(H_0(t) = \\int_0^t h_0(u) \\ du\\). The survival function \\(S(t)\\) and cumulative distribution function \\(F(t)\\) follow naturally: \\(S(t) = \\exp(-H(t))\\) and \\(F(t) = 1 - S(t) = 1 - \\exp(X \\beta)\\). Bender, Augustin, and Blettner (2005) showed that by letting \\[ F(\\tau) = u, \\ u \\sim U(0, 1) \\] and denoting the simulated survival time with \\(\\tau\\), it is possible to derive \\(\\tau\\) analytically by inverting \\(H_0(t)\\) if \\(h_0(\\tau) &gt; 0\\): \\[ \\tau = H_0^{-1}(-\\log(u) \\exp(X \\beta)) \\] The only requirement is for \\(H_0(t)\\) to be directly invertible, which happens to be the case when simulating from an exponential, Weibull, or Gompertz distribution for the baseline hazard \\(h_0(t)\\). The algorithm for simulating \\(m\\) survival times is as follows: draw a vector \\(u\\) of \\(m\\) observations from a \\(U(0, 1)\\) distribution; simulate \\(X\\) (e.g. a binary treatment from a Bernoulli distribution) and fix \\(\\beta\\); the survival times can be obtained directly by applying the formula \\(H_0^{-1}(-\\log(u) \\exp(X \\beta))\\). Bender, Augustin, and Blettner (2005) derived the closed-form version of \\(H_0^{-1}(t)\\) for the exponential, Weibull, and Gompertz distributions, presented in Table 5.1. Table 5.1: Closed-form formulas for simulating survival data from an exponential, Weibull, or Gompertz distribution. Exponential Weibull Gompertz Hazard function \\(h_0(t)\\) \\(\\lambda\\) \\(\\lambda p t ^ {p - 1}\\) \\(\\exp(\\gamma t)\\) Cumulative hazard function \\(H_0(t)\\) \\(\\lambda t\\) \\(\\lambda t ^ p\\) \\((\\lambda / \\gamma) (\\exp(\\gamma t) - 1)\\) Inverse cumulative hazard function \\(H_0^{-1}(t)\\) \\(\\lambda ^ {-1} t\\) \\((\\lambda ^ {-1} t) ^ {1 / p}\\) \\((1 / \\gamma) \\log((\\gamma / \\lambda) t + 1)\\) Survival time \\(\\tau\\) \\(-\\frac{\\log(u)}{\\lambda \\exp(X \\beta)}\\) \\(\\left[-\\frac{\\log(u)}{\\lambda \\exp(X \\beta)}\\right]^{1/p}\\) \\((1 / \\gamma) \\log\\left[1 - \\frac{\\gamma \\log(u)}{\\lambda\\exp(X \\beta)}\\right]\\) The requirement requirement for \\(H_0(t)\\) to be directly invertible impedes the use of distributions other than the exponential, Weibull, or Gompertz - which could be appropriate in some setting but too restricting in others. Crowther and Lambert (2013) generalised this method in order to accommodate more complex distributions, even with turning points, under a proportional hazards model. In brief, when the cumulative baseline hazard function is not invertible it is not possible to solve the equation \\(F(\\tau) = u\\) for \\(\\tau\\); assuming it is possible to write \\(H_0(t)\\) analytically - a broader assumption compared to assuming \\(H_0(t)\\) is invertible - it is possible to use root-finding methods to solve for \\(\\tau\\) numerically (Section 4.2.3). Formally, the survival time \\(\\tau\\) can be simulated as the root of the equation \\(S(\\tau) - u = 0\\). Crowther and Lambert (2013) present an example of their method by simulating from a two-components mixture distribution (McLachlan and McGiffin 1994), defined by additive components on the survival scale: \\[ S_0(t) = \\pi S_1(t) + (1 - \\pi) S_2(t), \\] with \\(\\pi \\in [0, 1]\\) mixing parameter. \\(S_i(t)\\) can be any standard parametric distribution. Choosing two Weibull components for the mixture distribution, it can be showed that a proportional hazards model has the form \\[ h(t) = \\frac{\\lambda_1 p_1 t ^ {p_1-1} \\pi \\exp(-\\lambda_1 t ^ {p_1}) + \\lambda_2 p_2 t ^ {p_2-1} (1 - \\pi) \\exp(-\\lambda_2 t ^ {p_2})} {\\pi \\exp(-\\lambda_1 t ^ {p_1}) + (1 - \\pi) \\exp(-\\lambda_2 t ^ {p_2})} \\exp(X \\beta); \\] the survival function can be obtained directly from \\(h(t)\\) in closed-form, plugged into the equation \\(S(\\tau) - u = 0\\), and numerically solved for \\(\\tau\\). References "],
["simst1.html", "6 Simulation study: accuracy of Gaussian quadrature", " 6 Simulation study: accuracy of Gaussian quadrature In this Chapter I will present the first simulation study I run during my first year, on accuracy on Gaussian quadrature methods. The Chapter is partitioned into seven Sections, and in each Section I will present an aspect of the simulation study: the aim of the study (Section 6.1), the data-generating mechanisms (Section 6.2), the methods that I will compare (Section 6.3) and the estimands of interest (Section 6.4), the performance measures I will use to compare the methods (Section 6.5), the results of the study (Section 6.6), and finally a brief discussion in Section 6.7. "],
["simst1-aim.html", "6.1 Aim", " 6.1 Aim The aim of this simulation study is two-fold. First, I want to assess the accuracy of numerical integration methods - Gaussian quadrature, specifically - in settings where it is possible to obtain analytical formulae; analytical formulae will be used as a control method. Second, I aim to assess the accuracy of Gaussian quadrature when analytical formulae are not available and therefore quadrature is indeed required. In order to fulfill these two aims I will simulate clustered survival data with a frailty component shared between individuals belonging to the same cluster; the distribution I will assign to the frailty will determine whether analytical formulae are available or not. I will choose a Gamma distribution for answering the first aim, as I showed in Section 2.2 that in that setting the likelihood has a closed form; then, I will choose a log-normal distribution for the frailty using the random intercept parametrisation as explained in Section 2.3 for answering the second aim. The likelihood of this model doesn’t have a closed form, hence I will need numerical integration to estimate the model. "],
["simst1-dgms.html", "6.2 Data-generating mechanisms", " 6.2 Data-generating mechanisms I generated survival data from a Weibull distribution with shape parameter \\(\\lambda = 0.5\\) and scale parameter \\(p = 0.6\\) (Figure 6.1) using the method of Bender, Augustin, and Blettner (2005) as explained in Section 5, and applying administrative censoring at time \\(t = 5\\); I used the following parametrisation for the Weibull distribution: \\[ h(t) = \\lambda p t ^ {p - 1} \\] Figure 6.1: Weibull baseline hazard function used in Simulation 1 (\\(\\lambda = 0.5, p = 0.6\\)) I included a binary covariate (e.g. a treatment) simulated by drawing from a Bernoulli random variable with parameter \\(\\pi = 0.5\\), and a frailty term shared between individuals in a cluster by drawing first from a Gamma distribution with shape parameter \\(1 / \\theta\\) and scale parameter \\(\\theta\\) (for identifiability purposes) and then by drawing from a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = \\sqrt{\\theta}\\). I varied \\(\\theta\\): \\(\\theta = \\{0.25, 0.75, 1.25\\}\\). I also varied the regression coefficient (e.g. the log-treatment effect) \\(\\beta\\) associated with the binary covariate: \\(\\beta = \\{-0.50, 0.00, 0.50\\}\\). I simulated data for six different sample sizes: 15 clusters of 30, 100, or 500 individuals each, 50 clusters of 30 or 100 individuals, and 1000 clusters of 2 individuals. As a result of this, sample size varied between 450 and 7500 individuals. Finally, I used a fully factorial design combining different frailty variances, frailty distributions, treatment effects, and sample sizes; it resulted in \\(3 \\times 2 \\times 3 \\times 6 = 108\\) different data-generating mechanisms, and for each of them I generated 1000 datasets. I will present the results separately by frailty variance, as I will use the 54 scenarios with a Gamma frailty to answer the first aim and the remaining 54 scenarios with a log-normal frailty to answer the second aim. References "],
["simst1-methods.html", "6.3 Methods", " 6.3 Methods I fitted a set of models for each simulated dataset under each data-generating mechanism. Specifically, for the data generated assuming a Gamma frailty, I compared the following models: a shared Gamma frailty model with a baseline Weibull hazard using the analytical formulation of the likelihood (method AN); a shared Gamma frailty model with a baseline Weibull hazard using the likelihood approximated numerically via Gaussian quadrature (specifically, a Gauss-Laguerre quadrature rule) with 15, 35, 75, and 105 nodes (methods GQ15, GQ35, GQ75, GQ105); a shared Gamma frailty model with a baseline Weibull hazard using the likelihood approximated numerically via Gauss-Kronrod quadrature (as implemented in R’s integrate() function; method IN). Then, for data generated assuming a log-normal frailty, I fitted a Weibull model with a random intercept using the likelihood approximated via Gauss-Hermite quadrature using 15, 35, 75, and 105 nodes. "],
["simst1-est.html", "6.4 Estimands", " 6.4 Estimands For each model fitted under each scenario, I compared: the estimated parameters of the Weibull distribution, i.e. \\(\\hat{\\lambda}\\) and \\(\\hat{p}\\); the estimated log-treatment effect, i.e. \\(\\hat{\\beta}\\); the estimated variance of the frailty term, i.e. \\(\\hat{\\theta}\\). "],
["simst1-pm.html", "6.5 Performance measures", " 6.5 Performance measures First, I am interested in the performance of the maximum likelihood estimation procedure; that is, how precise is the maximum likelihood estimator. I will assess this by computing bias for each estimand, defined as \\(b = E(\\hat{\\beta}) - \\beta\\). Additionally, I will compute relative bias (defined as \\(100 \\times [E(\\hat{\\beta}) - \\beta] / \\beta\\)) for presentation purposes, as it will be useful to compare bias for estimands with different magnitude (and therefore bias may be greater in absolute value but smaller in relative value). Next, I am interested in coverage, i.e. the proportion of times the \\(100 \\times (1 - \\alpha)\\%\\) confidence interval \\(\\hat{\\beta} \\pm Z_{1 - \\alpha / 2} \\times SE(\\hat{\\beta})\\) includes the true value \\(\\beta\\). This allow to assess whether the empirical coverage rate approaches the nominal coverage rate (\\(100 \\times (1 - \\alpha)\\%\\)), to properly control the type I error rate for testing a null hypotesis of no effect. Finally, I am interested in overall accuracy and therefore I will compute the mean squared error, defined as the sum of bias and variability: \\((\\bar{\\hat{\\beta}} - \\beta) ^ 2 + (SE(\\hat{\\beta})) ^ 2\\). Summary measures for \\(\\lambda\\), \\(p\\), and \\(\\theta\\) are computed on the log-scale. For bias and coverage I will further include Monte Carlo standard errors to quantify the uncertainty in estimating the performance measures (see White (2010) for further details). References "],
["simst1-res.html", "6.6 Results", " 6.6 Results I selected three scenarios for each aim of this simulation study (out of 54) to present, for conciseness. Specifically, I will present in this report: small frailty variance (0.25) and negative regression coefficient (-0.50); large frailty variance (1.25) and null regression coefficient (0.00); 1000 clusters of 2 individuals each and positive regression coefficient (0.50). The full results can be explored online interactively at LINK TO ADD. 6.6.1 Aim 1: comparison of Gaussian quadrature with analytical formulae In this Section I will present results for each aforementioned scenario and the simulation comparing quadrature methods with analytical formulae, that is, fitting a shared Gamma frailty Weibull regression model. First, bias, coverage, mean squared error, and convergence rates for scenario 1 are presented in Tables A.1, A.2, A.3, A.4 and Figures B.1, B.2, B.3, B.4. Convergence rates were generally good (&gt; 90%) for most sample sizes; the method that showed the worst convergence rates was Gauss-Kronrod quadrature with 1000 clusters of 2 individuals each, where approximately 75% of replications converged. Bias, coverage, and overall accuracy were optimal for all methods and across all sample sizes for the scale parameter of the Weibull distribution \\(p\\) and the regression coefficient \\(\\beta\\); conversely, the methods performed quite differently for the shape parameter \\(\\lambda\\) and the frailty variance \\(\\theta\\). The shape parameter estimated using analytical formulae or Gauss-Kronrod quadrature was generally unbiased, with good coverage and accuracy; vice versa, using Gauss-Laguerre quadrature produced underestimated coefficients when using a small number of nodes and required at least 75 nodes to yield unbiased results. As the number of nodes increased, coverage and mean squared error improved considerably. Also, sample sizes with a higher number of clusters generally yielded better estimates for the shape parameter in terms of bias, coverage, and mean squared error. The frailty variance \\(\\theta\\) was the parameter estimated with greatest variability in the results. Analytical formulae required a high number of clusters to produce unbiased results (50 or 1000), yielding underestimated coefficients otherwise. Gauss-Kronrod performed similarly to analytical formulae, as did Gauss-Laguerre quadrature with a sufficiently high number of nodes. Coverage was generally good, above 90% (except Gauss-Laguerre with 15 nodes, where coverage fell to 60-70% in some settings), symptom of overestimated standard errors for the frailty variance; this inflation of the standard errors was reflected in the mean squared error, which was generally greater that the other estimated parameters for all methods under all sample sizes explored in this scenario. Next, bias, coverage, mean squared error, and convergence rates for scenario 2 are presented in Tables A.5, A.6, A.7, A.8 and Figures B.5, B.6, B.7, B.8. Analogously as in scenario 1, convergence rate was generally good with Gauss-Kronrod quadrature performing the worst and estimates of the scale parameter \\(p\\) and the regression coefficient \\(\\beta\\) were unbiased with optimal coverage and low mean squared error. Bias for the shape parameter was substantial when using Gauss-Laguerre quadrature with 15 nodes, up to -0.5 on the log-scale; bias was reduced greatly by increasing the number of quadrature nodes, except when assuming 15 clusters of 500 individuals each where it remained substantial (approximately -0.35 on the log-scale) even when using 105 quadrature nodes. Coverage and mean squared error followed the same pattern; however, an exception was found in the case of 1000 clusters of 2 individuals: in that setting, \\(\\lambda\\) was estimated properly with small bias and good coverage and mean squared error. The estimated frailty variance also followed again the pattern depicted in scenario 1: analytical formulae required 50 or 1000 clusters to yield unbiased results, Gauss-Kronrod quadrature performed similarly to analytical formulae, and Gauss-Laguerre quadrature generally yielded better results with a greater number of quadrature nodes. Coverage was also generally good, symptom once again of overestimated standard errors - except the setting of 1000 clusters of 2 individuals each in which Gauss-Laguerre quadrature with 15 or 35 nodes did not cover the true value at all; the situation improved with a greater number of nodes, up to a coverage of 68.3% with 105 nodes. Inflated standard errors for the estimated frailty variance yielded increased mean squared error in this scenario as well, with all the methods performing similarly except Gauss-Laguerre quadrature with 15 knots which performed the worst once again. Finally, bias, coverage, percentage bias, mean squared error, and convergence rates for scenario 3 are presented in Tables A.9, A.10, A.11, A.12, A.13 and Figures B.9, B.10, B.11, B.12, B.13. Convergence rates were good as in the previous scenarios, with Gauss-Kronrod quadrature performing the worst again. The scale parameter \\(p\\) and the regression coefficient were estimated optimally once again, and further to that, in scenario 3 the shape parameter \\(\\lambda\\) was also well estimated (in terms of bias, coverage, and overall accuracy) with the exception of methods using Gauss-Laguerre quadrature when the true frailty variance was large; in that setting, the shape parameter was slighly overestimated (3% to 8%), coverage was suboptimal (60-809%) and mean squared error was greatest. Nevertheless, performance improved when increasing the number of quadrature nodes to approach results obtained using analytical formulae and Gauss-Kronrod quadrature. Finally, performance measures for the estimated frailty variance were generally good with a true frailty variance small or medium; Gauss-Kronrod undercovered the true value (coverage of approximately 90%). Conversely, when the true frailty variance was large, only analytical formulae and Gauss-Kronrod quadrature performed well. Gauss-Laguerre quadrature yielded severely underestimated results, up to -140% with 15 nodes, with null or poor coverage and large mean squared error. 6.6.2 Aim 2: accuracy when analytical formulae are not available In this section I will present results for the simulation comparing Gauss-Hermite quadrature with varying number of knots when fitting a model for which it is not possible to derive analytical formulae, specifically a Weibull regression model with a random intercept. First, bias, coverage, mean squared error, and convergence rates for scenario 1 are presented in Tables A.14, A.15, A.16, A.17 and Figures B.14, B.15, B.16, B.17. Convergence rates are good for all sample sizes (&gt; 97%) except when assuming 15 clusters of 500 individuals, where convergence rates drop to approximately 50% for all methods included in this comparison. Bias is generally negligible for the parameters of the Weibull distribution \\(\\lambda\\) and \\(p\\) and the regression coefficient \\(\\beta\\): between 0.0059 and 0.0193 for \\(\\lambda\\), between -0.0424 and -0.0332 for \\(p\\), between 0.0040 and 0.0867 for \\(\\beta\\). Conversely, estimates for \\(\\sigma\\) were negatively biased for a sample size of 15 clusters - 100 individuals, 1000 clusters - 2 individuals, 15 clusters - 30 individuals (between -0.3057 and -0.0854) and positively biased for a sample size of 15 clusters - 500 individuals (between and 0.2427 and 0.4020). Bias was negligible for a sample size of 50 clusters - 30 individuals and 50 clusters - 100 individuals (between -0.0536 and -0.0095). Coverage of all estimated coefficients was poor (&lt; 75%) for a sample size of 15 clusters - 500 individuals. For the regression coefficient \\(\\beta\\) and the frailty variance \\(\\sigma\\) coverage was good or superoptimal for the remaining sample sizes, with the exception of \\(\\sigma\\) estimated using Gauss-Hermite quadrature with 15 nodes that resulted in slight undercoverage for sample sizes of 15 clusters - 100 individuals and 50 clusters - 100 individuals. The parameters of the Weibull distribution were generally undercovered (&lt; 80%) across sample sizes, except \\(\\lambda\\) with a sample size of 1000 clusters - 2 individuals and \\(p\\) with a sample size of 15 clusters - 30 individuals for which coverage was in the range 90-95%. Finally, mean squared error was low for \\(\\lambda\\), \\(p\\), and \\(\\beta\\), comparable for \\(\\sigma\\) with a sample size of 50 clusters - 30 individuals and 50 clusters - 100 individuals, much higher for \\(\\sigma\\) with all the remaining sample sizes (i.e. overall accuracy was lower in these settings). Next, bias, coverage, mean squared error, and convergence rates for scenario 2 are presented in Tables A.18, A.19, A.20, A.21 and Figures B.18, B.19, B.20, B.21. Analogously as before, coverage was geenrally good except for the sample size of 15 clusters - 100 individuals, where only 15% of replications succesfully converged to a solution. Bias decreased for the regression coefficient \\(\\beta\\) with increasing number of quadrature nodes; conversely, bias for the scale parameter \\(p\\) was not affected by the number of quadrature nodes and remained more or less constant across sample sizes at approximately -0.15. Bias for the shape parameter \\(\\lambda\\) was also not affected by the quadrature nodes, with negative bias for the sample size of 15 clusters - 500 individuals (approximately -0.05) and positive bias elsewhere (0.0241 to 0.0622). \\(\\sigma\\) was consistently underestimated, with bias ranging between -0.3291 and -0.0821. The pattern for coverage was similar: coverage for \\(\\beta\\) was good assuming at least 75 quadrature nodes were used. With a sample size of 1000 clusters - 2 individuals, 15 clusters - 30 individuals, 50 clusters - 30 individuals coverage for the regression coefficient was good irrespectively of the number of quadrature nodes, and for a sample size of 15 clusters - 500 individuals coverage was mediocre even when using 105 quadrature nodes. Coverage of \\(\\lambda\\) and \\(p\\) was generally poor, between 0% and 64%. Coverage for \\(\\sigma\\) was only slightly suboptimal with a sample size of 15 clusters - 30 individuals and 50 clusters - 30 individuals (&gt; 75%), poor to suboptimal otherwise (7% to 75%, with peaks above 80% when using 75 or 105 quadrature nodes). Bias and poor coverage were reflected in the mean squared error; interestingly, it seemed to generally decrease with increasing number of quadrature nodes (especially for the regression coefficient and the variance of the random intercept). Finally, bias, coverage, mean squared error, and convergence rates for scenario 3 are presented in Tables A.22, A.23, A.25, A.26 and Figures B.22, B.23, B.25, B.26. Convergence rates were good, above 99% for each method included in the comparison and variance magnitude. Negative bias was present for \\(\\lambda\\), and \\(\\beta\\), between -15% and -2%; positive bias for \\(p\\) increased with greater variance magnitude, from 7% for a true \\(\\sigma\\) = 0.25 to 35% for a true \\(\\sigma\\) = 1.25. Estimates of \\(\\sigma\\) with a small-medium frailty variance were positively biased (30 to 180%), negatively biased otherwise (-300%). Coverage was null to poor for parameters of the Weibull distribution and a medium-large frailty variance, less than 80%; with a small frailty variance, coverage was good for \\(\\lambda\\) and poor for \\(p\\). Coverage for the regression parameter \\(\\beta\\) was good to suboptimal, decreasing to 75-80% with a large frailty variance. Coverage for \\(\\sigma\\) was superoptimal for a small frailty variance, null to poor for a medium-large frailty variance. Mean squared error was greatest for \\(\\sigma\\), irrespectively of the magnitude of the frailty variance; conversely, mean squared error tended to increase as the frailty variance increased. "],
["simst1-conclusions.html", "6.7 Conclusions", " 6.7 Conclusions I showed in the previous Section how Gaussian quadrature performs (1) compared to analytical formulae and (2) when it is not possible to obtain analytical formulae. Overall, Gaussian quadrature performs well with a sufficient number of quadrature nodes but the variability is great. The regression coefficient \\(\\beta\\) is the most robust estimand across different scenarios, it is mostly unbiased (or with little bias) and with good coverage and accuracy (in terms of mean squared error). The frailty variance is the least robust estimand, with precision and accuracy greatly depending on many factors: among others, important ones seems to be the number of quadrature nodes and the number of clusters. The latter makes sense on a theoretical level: with more clusters it should be easier to estimate a properly the variance of the frailty. Accuracy and precision of the parameters of the Weibull baseline hazard also varies greatly. In conclusion, using a shared frailty model to do inference on a regression coefficient seems to be robust to the accuracy of numerical integration methods; nevertheless, if the principal research interest lays in relative risk estimates, using a parametric model may not be the best choice after all. A semiparametric Cox model - even with frailty terms if necessary - could be utilised instead. If the research objectives include absolute risk estimations, though, a parametric model is immediately more appealing. However, checking the convergence, precision, and accuracy of numerical integration by evauating and comparing an increasing number of quadrature knots appears to be fundamental. -->"],
["simst2.html", "7 Simulation study: impact of misspecification in survival models with shared frailty terms", " 7 Simulation study: impact of misspecification in survival models with shared frailty terms "],
["simst2-aim.html", "7.1 Aim", " 7.1 Aim "],
["simst2-dgms.html", "7.2 Data-generating mechanisms", " 7.2 Data-generating mechanisms "],
["simst2-methods.html", "7.3 Methods", " 7.3 Methods "],
["simst2-est.html", "7.4 Estimands", " 7.4 Estimands "],
["simst2-pm.html", "7.5 Performance measures", " 7.5 Performance measures "],
["simst2-res.html", "7.6 Results", " 7.6 Results "],
["simst2-conclusions.html", "7.7 Conclusions", " 7.7 Conclusions -->"],
["sirex.html", "8 Exploring results from simulation studies interactively", " 8 Exploring results from simulation studies interactively "],
["infvp.html", "9 Informative visiting process", " 9 Informative visiting process "],
["future.html", "10 Future research developments", " 10 Future research developments "],
["pdevelop.html", "11 Personal development", " 11 Personal development In this chapter I will introduce and briefly discuss the personal development activities I carried out during the first year of my PhD. In particular, I will present the supervisory meetings, training courses, and conferences I attended. "],
["supervisory-meetings.html", "11.1 Supervisory meetings", " 11.1 Supervisory meetings I have been having frequent meetings with my supervisors, formally and informally. Formal supervisory meetings, recorded on PROSE (https://prose.le.ac.uk), have been held on average every other week, with summaries produced and shared between us. A comprehensive list is available on PROSE. Additionally, we held informal meetings to discuss developments and more urgent matters more often, whenever it was needed and without scheduling it. "],
["training-and-courses.html", "11.2 Training and courses", " 11.2 Training and courses I have attended a wide variety of courses during my first year, both externally and internally to the University of Leicester. The external courses I attended are: Efficient R Programming, on November 8th 2016, organised by the Royal Statistical Society in London. The instructor was Dr. Colin Gillespie, from the University of Newcastle, United Kingdom, and Jumping Rivers. The course covered how to program efficiently with R; in particular, it covered common pitfalls when writing R code, code profiling, RCpp, and parallel programming. General hints and tips were provided. Introduction to causal inference, on April 25th and 26th 2017, organised by the Biostatistics Research Group at the University of Leicester and delivered by Dr. Arvid Sjölander from Karolinska Institutet, Stockholm, Sweden. The course provided foundational concepts of causal inference such as the difference between association and causation, the counterfactual framework, exchangeability, directed acyclic graphs, methods for estimating a causal effect, etc. Additionally, it provided an introduction to more advanced methods such as intrumental variables and Mendelian randomisation. Using simulation studies to evaluate statistical methods, on May 22nd 2017, organised by University College London. The course was delivered by Dr. Tim Morris, Prof. Ian White and Dr. Michael Crowther, and it covered the rationale for using simulation studies, important concepts to keep in mind when planning a simulation study, computational tools, estimates of uncertainty, and tools for improving reporting and dissemination. Workshop on Joint modelling of longitudinal and time-to-event data with R, on July 5th, 2017, organised by the Department of Biostatistics of the University of Liverpool. The course was delivered by Dr. Graeme Hickey, and provided an introduction to joint models of longitudinal and survival data, including extensions to incorporate competing risks and multiple longitudinal processes and a practical session using R. I have attended a few courses within the University and not offered on PROSE; specifically, I attended a course on Time series analysis with R (November 10th, 2016), a course on Data visualisation (November 15th, 2016), and a course on High performance computing at Leicester (February 8th, 2017). The latter was particularly important, as it allowed me to make better use of the high-performance computing facilities offered by the University. I also attended the Preparing to teach in higher education workshop, strand A (July 24th and 27th 2017). Additionally, I have attended the following PROSE training sessions to develop personal and communication skills in research settings. These are listed below: Planning your literature search, October 21st 2016; Conducting your literature search, October 25th 2016 ; Assertiveness, November 14th 2016; Introduction to critical thinking, December 15th 2016; Presentations A: Fundamentals of an effective presentation, January 30th 2017; Communication in research and other work settings, January 31st 2017; Enhancing your digital profile, February 2nd 2017; Saying it with your abstract, February 10th 2017; Designing a poster, February 27th 2017; Leadership in research and other work environments, February 28th 2017; Preparing for the probation review (Physical natural and medical sciences), May 30th 2017. "],
["conferences.html", "11.3 Conferences", " 11.3 Conferences I have attended a number of conferences during this year, in which I delivered the following oral presentations: Survival Analysis for Junior Researchers conference, held in Leicester, UK, on April 5th and 6th 2017. I delivered a talk titled Direct likelihood maximisation using numerical quadrature to approximate intractable terms; Statistical Analysis of Multi-Outcome Data (SAM) conference, held in Liverpool, UK, on July 3rd and 4th 2017. I delivered a talk titled Impact of model misspecification in survival models with frailties; Annual Conference of the International Society for Clinical Biostatistics conference, held in Vigo, Spain, on July 9th to July 13th 2017. I delivered two talks: a titled Impact of model misspecification in survival models with frailties during the main conference, and a talk titled Exploring results from simulation studies interactively during the Students’ Day organised on July 13th. Additionally, I delivered an oral presentation on previous work external to my PhD project during the 54th ERA-EDTA Congress held in Madrid, Spain, between June 3rd and June 6th. The ERA-EDTA Congress is the main conference in the field of Nephrology in Europe, with approximately 10,000 participants in 2017. I delivered my presentation, titled Inappropriate prescription of nephrotoxic drugs to individuals with chronic kidney disease, to an audience of clinicians, epidemiologists, clinical researchers, and other stakeholders. "],
["ax-tables.html", "A Tables ", " A Tables "],
["ax-tables-simst1.html", "A.1 Simulation study 1", " A.1 Simulation study 1 Table A.1: Bias, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 0.0019 (0.0016) 0.0006 (0.0009) -0.0030 (0.0020) -0.0173 (0.0082) 1000c. of 2i. IN 0.0001 (0.0016) 0.0004 (0.0009) -0.0029 (0.0020) -0.0319 (0.0079) 1000c. of 2i. GQ15 0.0111 (0.0015) 0.0053 (0.0008) -0.0070 (0.0020) 0.0858 (0.0029) 1000c. of 2i. GQ35 0.0022 (0.0016) 0.0007 (0.0009) -0.0032 (0.0020) -0.0105 (0.0075) 1000c. of 2i. GQ75 0.0019 (0.0016) 0.0006 (0.0009) -0.0030 (0.0020) -0.0175 (0.0082) 1000c. of 2i. GQ105 0.0019 (0.0016) 0.0006 (0.0009) -0.0030 (0.0020) -0.0174 (0.0082) 15c. of 100i. AF -0.0084 (0.0044) 0.0015 (0.0009) -0.0012 (0.0021) -0.1462 (0.0129) 15c. of 100i. IN -0.0072 (0.0044) 0.0016 (0.0009) -0.0009 (0.0021) -0.1462 (0.0129) 15c. of 100i. GQ15 -0.1648 (0.0077) 0.0026 (0.0009) -0.0017 (0.0022) 0.2228 (0.0127) 15c. of 100i. GQ35 -0.0860 (0.0060) 0.0015 (0.0009) -0.0014 (0.0021) -0.0284 (0.0127) 15c. of 100i. GQ75 -0.0403 (0.0051) 0.0014 (0.0009) -0.0010 (0.0021) -0.1142 (0.0130) 15c. of 100i. GQ105 -0.0075 (0.0049) 0.0015 (0.0009) -0.0012 (0.0021) -0.1304 (0.0131) 15c. of 30i. AF -0.0101 (0.0051) 0.0017 (0.0017) -0.0071 (0.0040) -0.2152 (0.0170) 15c. of 30i. IN -0.0083 (0.0051) 0.0019 (0.0017) -0.0085 (0.0040) -0.2151 (0.0170) 15c. of 30i. GQ15 -0.0531 (0.0070) 0.0026 (0.0017) -0.0074 (0.0040) -0.0252 (0.0143) 15c. of 30i. GQ35 -0.0027 (0.0053) 0.0019 (0.0017) -0.0081 (0.0040) -0.1938 (0.0210) 15c. of 30i. GQ75 -0.0100 (0.0051) 0.0017 (0.0017) -0.0071 (0.0040) -0.2158 (0.0170) 15c. of 30i. GQ105 -0.0102 (0.0051) 0.0017 (0.0017) -0.0071 (0.0040) -0.2162 (0.0172) 15c. of 500i. AF -0.0108 (0.0041) 0.0004 (0.0004) 0.0009 (0.0009) -0.1415 (0.0122) 15c. of 500i. IN -0.0489 (0.0051) 0.0005 (0.0004) 0.0010 (0.0009) -0.0672 (0.0134) 15c. of 500i. GQ15 -0.2246 (0.0090) 0.0008 (0.0004) 0.0007 (0.0010) 0.3167 (0.0154) 15c. of 500i. GQ35 -0.1448 (0.0062) 0.0006 (0.0004) 0.0010 (0.0009) 0.0726 (0.0126) 15c. of 500i. GQ75 -0.1094 (0.0052) 0.0004 (0.0004) 0.0009 (0.0009) -0.0327 (0.0121) 15c. of 500i. GQ105 -0.1013 (0.0049) 0.0004 (0.0004) 0.0009 (0.0009) -0.0574 (0.0120) 50c. of 100i. AF 0.0004 (0.0023) 0.0003 (0.0005) -0.0022 (0.0012) -0.0442 (0.0068) 50c. of 100i. IN 0.0012 (0.0023) 0.0003 (0.0005) -0.0020 (0.0012) -0.0445 (0.0068) 50c. of 100i. GQ15 -0.2070 (0.0062) 0.0006 (0.0005) -0.0027 (0.0012) 0.3155 (0.0091) 50c. of 100i. GQ35 -0.0875 (0.0041) 0.0002 (0.0005) -0.0020 (0.0012) 0.0623 (0.0072) 50c. of 100i. GQ75 -0.0198 (0.0030) 0.0002 (0.0005) -0.0021 (0.0012) -0.0246 (0.0069) 50c. of 100i. GQ105 -0.0018 (0.0028) 0.0003 (0.0005) -0.0022 (0.0012) -0.0356 (0.0069) 50c. of 30i. AF -0.0019 (0.0027) 0.0000 (0.0009) -0.0024 (0.0022) -0.0464 (0.0078) 50c. of 30i. IN -0.0021 (0.0027) 0.0001 (0.0009) -0.0038 (0.0022) -0.0433 (0.0078) 50c. of 30i. GQ15 -0.0451 (0.0040) 0.0006 (0.0009) -0.0026 (0.0022) 0.0822 (0.0070) 50c. of 30i. GQ35 -0.0023 (0.0029) 0.0001 (0.0009) -0.0024 (0.0022) -0.0382 (0.0080) 50c. of 30i. GQ75 -0.0018 (0.0027) 0.0000 (0.0009) -0.0024 (0.0022) -0.0464 (0.0078) 50c. of 30i. GQ105 -0.0019 (0.0027) 0.0000 (0.0009) -0.0024 (0.0022) -0.0464 (0.0078) Table A.2: Coverage, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 94.2000 (0.7392) 95.5000 (0.6556) 95.0000 (0.6892) 96.5000 (0.5812) 1000c. of 2i. IN 93.4228 (0.7839) 95.7047 (0.6412) 93.0201 (0.8058) 77.4497 (1.3216) 1000c. of 2i. GQ15 93.6000 (0.7740) 95.4000 (0.6624) 95.1000 (0.6826) 98.7000 (0.3582) 1000c. of 2i. GQ35 94.2943 (0.7335) 95.0951 (0.6830) 95.2953 (0.6696) 95.8959 (0.6273) 1000c. of 2i. GQ75 94.2000 (0.7392) 95.5000 (0.6556) 95.0000 (0.6892) 96.5000 (0.5812) 1000c. of 2i. GQ105 94.2000 (0.7392) 95.5000 (0.6556) 95.0000 (0.6892) 96.5000 (0.5812) 15c. of 100i. AF 92.4000 (0.8380) 95.5000 (0.6556) 94.7000 (0.7085) 92.6000 (0.8278) 15c. of 100i. IN 90.6475 (0.9208) 95.4779 (0.6571) 94.4502 (0.7240) 92.3947 (0.8383) 15c. of 100i. GQ15 27.8000 (1.4167) 94.6000 (0.7147) 94.9000 (0.6957) 86.7000 (1.0738) 15c. of 100i. GQ35 44.0440 (1.5699) 94.7948 (0.7024) 94.8949 (0.6960) 93.9940 (0.7514) 15c. of 100i. GQ75 66.0000 (1.4980) 95.1000 (0.6826) 95.1000 (0.6826) 93.0000 (0.8068) 15c. of 100i. GQ105 76.2000 (1.3467) 95.5000 (0.6556) 94.8000 (0.7021) 92.5000 (0.8329) 15c. of 30i. AF 92.7000 (0.8226) 95.0000 (0.6892) 94.7000 (0.7085) 94.5000 (0.7209) 15c. of 30i. IN 92.2118 (0.8474) 94.9117 (0.6949) 94.0810 (0.7462) 93.1464 (0.7990) 15c. of 30i. GQ15 68.1000 (1.4739) 94.7000 (0.7085) 94.7000 (0.7085) 93.1000 (0.8015) 15c. of 30i. GQ35 88.9119 (0.9929) 94.6114 (0.7140) 94.1969 (0.7393) 95.2332 (0.6738) 15c. of 30i. GQ75 92.5000 (0.8329) 95.0000 (0.6892) 94.7000 (0.7085) 94.1000 (0.7451) 15c. of 30i. GQ105 92.7000 (0.8226) 95.0000 (0.6892) 94.6000 (0.7147) 94.6000 (0.7147) 15c. of 500i. AF 92.5000 (0.8329) 95.4000 (0.6624) 95.7000 (0.6415) 91.6000 (0.8772) 15c. of 500i. IN 31.7396 (1.4719) 95.0153 (0.6882) 95.6256 (0.6468) 91.2513 (0.8935) 15c. of 500i. GQ15 11.0000 (0.9894) 92.5000 (0.8329) 94.9000 (0.6957) 80.4000 (1.2553) 15c. of 500i. GQ35 14.2000 (1.1038) 93.8000 (0.7626) 95.7000 (0.6415) 91.5000 (0.8819) 15c. of 500i. GQ75 18.4000 (1.2253) 94.8000 (0.7021) 95.9000 (0.6270) 93.4000 (0.7851) 15c. of 500i. GQ105 22.0000 (1.3100) 95.2000 (0.6760) 95.4000 (0.6624) 93.2000 (0.7961) 50c. of 100i. AF 95.4000 (0.6624) 94.9000 (0.6957) 94.3000 (0.7332) 93.3000 (0.7906) 50c. of 100i. IN 93.0704 (0.8031) 94.5629 (0.7170) 93.8166 (0.7616) 91.6844 (0.8732) 50c. of 100i. GQ15 17.2000 (1.1934) 94.3000 (0.7332) 94.7000 (0.7085) 60.6000 (1.5452) 50c. of 100i. GQ35 37.8000 (1.5333) 94.6000 (0.7147) 94.3000 (0.7332) 91.5000 (0.8819) 50c. of 100i. GQ75 69.8000 (1.4519) 94.8000 (0.7021) 94.5000 (0.7209) 93.7000 (0.7683) 50c. of 100i. GQ105 82.9000 (1.1906) 94.9000 (0.6957) 94.2000 (0.7392) 93.6000 (0.7740) 50c. of 30i. AF 93.8000 (0.7626) 95.3000 (0.6693) 95.3000 (0.6693) 95.4000 (0.6624) 50c. of 30i. IN 92.6454 (0.8254) 95.2799 (0.6706) 94.7311 (0.7065) 93.3041 (0.7904) 50c. of 30i. GQ15 71.9000 (1.4214) 95.0000 (0.6892) 95.1000 (0.6826) 92.6000 (0.8278) 50c. of 30i. GQ35 92.0000 (0.8579) 95.1000 (0.6826) 95.1000 (0.6826) 93.9000 (0.7568) 50c. of 30i. GQ75 93.8000 (0.7626) 95.4000 (0.6624) 95.2000 (0.6760) 95.4000 (0.6624) 50c. of 30i. GQ105 93.7000 (0.7683) 95.3000 (0.6693) 95.3000 (0.6693) 95.4000 (0.6624) Table A.3: Mean squared error, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 0.0026 0.0008 0.0042 0.0670 1000c. of 2i. IN 0.0024 0.0008 0.0041 0.0635 1000c. of 2i. GQ15 0.0024 0.0007 0.0042 0.0159 1000c. of 2i. GQ35 0.0025 0.0008 0.0042 0.0571 1000c. of 2i. GQ75 0.0026 0.0008 0.0042 0.0668 1000c. of 2i. GQ105 0.0026 0.0008 0.0042 0.0669 15c. of 100i. AF 0.0190 0.0009 0.0045 0.1886 15c. of 100i. IN 0.0193 0.0009 0.0046 0.1880 15c. of 100i. GQ15 0.0867 0.0009 0.0046 0.2116 15c. of 100i. GQ35 0.0432 0.0009 0.0046 0.1621 15c. of 100i. GQ75 0.0274 0.0009 0.0045 0.1816 15c. of 100i. GQ105 0.0242 0.0009 0.0045 0.1877 15c. of 30i. AF 0.0262 0.0029 0.0157 0.3341 15c. of 30i. IN 0.0259 0.0029 0.0158 0.3353 15c. of 30i. GQ15 0.0517 0.0029 0.0161 0.2038 15c. of 30i. GQ35 0.0283 0.0029 0.0159 0.4801 15c. of 30i. GQ75 0.0263 0.0029 0.0157 0.3365 15c. of 30i. GQ105 0.0262 0.0029 0.0157 0.3414 15c. of 500i. AF 0.0169 0.0002 0.0009 0.1691 15c. of 500i. IN 0.0284 0.0002 0.0009 0.1835 15c. of 500i. GQ15 0.1315 0.0002 0.0009 0.3364 15c. of 500i. GQ35 0.0599 0.0002 0.0009 0.1638 15c. of 500i. GQ75 0.0394 0.0002 0.0009 0.1486 15c. of 500i. GQ105 0.0345 0.0002 0.0009 0.1477 50c. of 100i. AF 0.0053 0.0003 0.0014 0.0487 50c. of 100i. IN 0.0054 0.0003 0.0014 0.0488 50c. of 100i. GQ15 0.0816 0.0003 0.0014 0.1827 50c. of 100i. GQ35 0.0242 0.0003 0.0014 0.0555 50c. of 100i. GQ75 0.0095 0.0003 0.0014 0.0480 50c. of 100i. GQ105 0.0076 0.0003 0.0014 0.0491 50c. of 30i. AF 0.0073 0.0009 0.0047 0.0628 50c. of 30i. IN 0.0074 0.0009 0.0047 0.0629 50c. of 30i. GQ15 0.0180 0.0009 0.0048 0.0560 50c. of 30i. GQ35 0.0082 0.0009 0.0047 0.0647 50c. of 30i. GQ75 0.0073 0.0009 0.0047 0.0627 50c. of 30i. GQ105 0.0073 0.0009 0.0047 0.0628 Table A.4: Convergence rates, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size AF IN GQ15 GQ35 GQ75 GQ105 1000c. of 2i. 100.0% 74.5% 100.0% 99.9% 100.0% 100.0% 15c. of 100i. 100.0% 97.3% 100.0% 99.9% 100.0% 100.0% 15c. of 30i. 100.0% 96.3% 100.0% 96.5% 100.0% 100.0% 15c. of 500i. 100.0% 98.3% 100.0% 100.0% 100.0% 100.0% 50c. of 100i. 100.0% 93.8% 100.0% 100.0% 100.0% 100.0% 50c. of 30i. 100.0% 91.1% 100.0% 100.0% 100.0% 100.0% Table A.5: Bias, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF -0.0013 (0.0021) 0.0025 (0.0009) -0.0021 (0.0024) -0.0008 (0.0032) 1000c. of 2i. IN 0.0003 (0.0021) 0.0020 (0.0009) -0.0018 (0.0024) -0.0044 (0.0032) 1000c. of 2i. GQ15 -0.0637 (0.0019) -0.0376 (0.0008) -0.0018 (0.0023) -0.3201 (0.0010) 1000c. of 2i. GQ35 -0.0493 (0.0019) -0.0264 (0.0008) -0.0018 (0.0023) -0.2203 (0.0014) 1000c. of 2i. GQ75 -0.0368 (0.0019) -0.0183 (0.0009) -0.0019 (0.0023) -0.1550 (0.0017) 1000c. of 2i. GQ105 -0.0319 (0.0019) -0.0153 (0.0009) -0.0019 (0.0023) -0.1319 (0.0018) 15c. of 100i. AF -0.0586 (0.0088) 0.0014 (0.0010) -0.0001 (0.0023) -0.1229 (0.0113) 15c. of 100i. IN -0.0672 (0.0092) 0.0010 (0.0010) 0.0008 (0.0023) -0.1375 (0.0114) 15c. of 100i. GQ15 -0.4546 (0.0095) -0.0027 (0.0010) 0.0001 (0.0022) -0.0652 (0.0109) 15c. of 100i. GQ35 -0.3520 (0.0089) -0.0007 (0.0010) -0.0000 (0.0023) -0.0923 (0.0104) 15c. of 100i. GQ75 -0.2557 (0.0082) -0.0000 (0.0010) 0.0000 (0.0023) -0.1170 (0.0106) 15c. of 100i. GQ105 -0.1829 (0.0078) 0.0004 (0.0010) -0.0001 (0.0023) -0.1322 (0.0106) 15c. of 30i. AF -0.0407 (0.0099) 0.0020 (0.0018) -0.0055 (0.0042) -0.1121 (0.0121) 15c. of 30i. IN -0.0435 (0.0099) 0.0016 (0.0018) -0.0059 (0.0042) -0.1149 (0.0120) 15c. of 30i. GQ15 -0.2776 (0.0097) -0.0049 (0.0018) -0.0058 (0.0042) -0.1773 (0.0100) 15c. of 30i. GQ35 -0.0915 (0.0098) -0.0003 (0.0018) -0.0056 (0.0042) -0.1758 (0.0104) 15c. of 30i. GQ75 -0.0465 (0.0101) 0.0013 (0.0018) -0.0055 (0.0042) -0.1550 (0.0108) 15c. of 30i. GQ105 -0.0476 (0.0099) 0.0014 (0.0018) -0.0055 (0.0042) -0.1484 (0.0109) 15c. of 500i. AF -0.0369 (0.0093) 0.0007 (0.0004) 0.0010 (0.0011) -0.1249 (0.0112) 15c. of 500i. IN -0.2891 (0.0081) 0.0002 (0.0004) 0.0010 (0.0011) -0.1394 (0.0102) 15c. of 500i. GQ15 -0.4658 (0.0101) -0.0042 (0.0007) 0.0008 (0.0011) -0.0547 (0.0104) 15c. of 500i. GQ35 -0.4024 (0.0091) -0.0011 (0.0005) 0.0008 (0.0010) -0.0676 (0.0105) 15c. of 500i. GQ75 -0.3619 (0.0084) -0.0007 (0.0006) 0.0009 (0.0010) -0.0733 (0.0108) 15c. of 500i. GQ105 -0.3442 (0.0082) -0.0002 (0.0006) 0.0010 (0.0011) -0.0748 (0.0107) 50c. of 100i. AF -0.0146 (0.0050) -0.0000 (0.0005) -0.0016 (0.0012) -0.0363 (0.0058) 50c. of 100i. IN -0.0376 (0.0050) -0.0005 (0.0005) -0.0006 (0.0012) -0.0671 (0.0058) 50c. of 100i. GQ15 -0.4998 (0.0074) -0.0042 (0.0005) -0.0016 (0.0013) 0.0609 (0.0072) 50c. of 100i. GQ35 -0.3359 (0.0060) -0.0024 (0.0005) -0.0016 (0.0012) -0.0120 (0.0056) 50c. of 100i. GQ75 -0.1945 (0.0053) -0.0014 (0.0005) -0.0016 (0.0012) -0.0455 (0.0053) 50c. of 100i. GQ105 -0.1000 (0.0052) -0.0008 (0.0005) -0.0016 (0.0012) -0.0560 (0.0053) 50c. of 30i. AF -0.0170 (0.0053) 0.0003 (0.0010) -0.0000 (0.0022) -0.0219 (0.0061) 50c. of 30i. IN -0.0126 (0.0053) 0.0007 (0.0009) 0.0007 (0.0022) -0.0384 (0.0060) 50c. of 30i. GQ15 -0.2482 (0.0064) -0.0069 (0.0010) 0.0006 (0.0022) -0.1152 (0.0050) 50c. of 30i. GQ35 -0.0618 (0.0056) -0.0019 (0.0010) -0.0001 (0.0022) -0.0958 (0.0050) 50c. of 30i. GQ75 -0.0238 (0.0052) -0.0004 (0.0010) 0.0000 (0.0022) -0.0656 (0.0053) 50c. of 30i. GQ105 -0.0224 (0.0052) -0.0003 (0.0010) 0.0000 (0.0022) -0.0570 (0.0054) Table A.6: Coverage, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 96.4000 (0.5891) 94.6000 (0.7147) 95.2000 (0.6760) 94.1000 (0.7451) 1000c. of 2i. IN 92.6254 (0.8265) 93.9528 (0.7538) 94.2478 (0.7363) 87.3156 (1.0524) 1000c. of 2i. GQ15 82.2000 (1.2096) 73.5000 (1.3956) 96.1000 (0.6122) 0.0000 (0.0000) 1000c. of 2i. GQ35 88.3000 (1.0164) 84.6000 (1.1414) 96.0000 (0.6197) 1.7000 (0.4088) 1000c. of 2i. GQ75 91.6000 (0.8772) 90.7000 (0.9184) 95.9000 (0.6270) 47.0000 (1.5783) 1000c. of 2i. GQ105 92.7000 (0.8226) 92.5000 (0.8329) 95.9000 (0.6270) 68.3000 (1.4714) 15c. of 100i. AF 94.2000 (0.7392) 95.9000 (0.6270) 95.0000 (0.6892) 92.6000 (0.8278) 15c. of 100i. IN 74.1729 (1.3841) 95.4109 (0.6617) 93.8100 (0.7620) 90.7150 (0.9178) 15c. of 100i. GQ15 11.1000 (0.9934) 93.7000 (0.7683) 95.2000 (0.6760) 94.0000 (0.7510) 15c. of 100i. GQ35 17.0000 (1.1879) 95.6000 (0.6486) 94.9000 (0.6957) 94.9000 (0.6957) 15c. of 100i. GQ75 36.9000 (1.5259) 95.6000 (0.6486) 95.1000 (0.6826) 93.9000 (0.7568) 15c. of 100i. GQ105 51.5000 (1.5804) 95.9000 (0.6270) 94.9000 (0.6957) 93.5000 (0.7796) 15c. of 30i. AF 92.8000 (0.8174) 96.1000 (0.6122) 96.0000 (0.6197) 93.0000 (0.8068) 15c. of 30i. IN 91.8665 (0.8644) 96.0375 (0.6169) 95.6204 (0.6471) 91.8665 (0.8644) 15c. of 30i. GQ15 46.7000 (1.5777) 96.0000 (0.6197) 95.6000 (0.6486) 94.4000 (0.7271) 15c. of 30i. GQ35 78.0000 (1.3100) 96.1000 (0.6122) 96.0000 (0.6197) 94.0000 (0.7510) 15c. of 30i. GQ75 88.8000 (0.9973) 95.9000 (0.6270) 96.0000 (0.6197) 93.7000 (0.7683) 15c. of 30i. GQ105 92.0000 (0.8579) 96.1000 (0.6122) 96.0000 (0.6197) 93.8000 (0.7626) 15c. of 500i. AF 92.3000 (0.8430) 94.4000 (0.7271) 94.1000 (0.7451) 92.9000 (0.8122) 15c. of 500i. IN 12.9291 (1.0610) 95.0801 (0.6839) 92.9062 (0.8118) 92.1053 (0.8527) 15c. of 500i. GQ15 3.8066 (0.6051) 86.1111 (1.0936) 94.1358 (0.7430) 93.9300 (0.7551) 15c. of 500i. GQ35 5.1177 (0.6968) 90.9928 (0.9053) 94.7799 (0.7034) 94.6776 (0.7099) 15c. of 500i. GQ75 7.3245 (0.8239) 92.6755 (0.8239) 94.6083 (0.7142) 93.8962 (0.7570) 15c. of 500i. GQ105 8.9340 (0.9020) 93.2995 (0.7907) 94.2132 (0.7384) 94.2132 (0.7384) 50c. of 100i. AF 95.2000 (0.6760) 95.7000 (0.6415) 94.4000 (0.7271) 93.8000 (0.7626) 50c. of 100i. IN 71.3376 (1.4299) 94.2675 (0.7351) 87.5159 (1.0453) 84.4586 (1.1457) 50c. of 100i. GQ15 2.7000 (0.5126) 94.0000 (0.7510) 94.0000 (0.7510) 85.4000 (1.1166) 50c. of 100i. GQ35 9.5000 (0.9272) 94.4000 (0.7271) 94.9000 (0.6957) 94.9000 (0.6957) 50c. of 100i. GQ75 31.3000 (1.4664) 95.1000 (0.6826) 94.8000 (0.7021) 95.7000 (0.6415) 50c. of 100i. GQ105 54.3000 (1.5753) 95.6000 (0.6486) 94.5000 (0.7209) 95.0000 (0.6892) 50c. of 30i. AF 94.6000 (0.7147) 96.0000 (0.6197) 95.6000 (0.6486) 95.1000 (0.6826) 50c. of 30i. IN 89.7114 (0.9607) 95.6085 (0.6480) 91.8444 (0.8655) 91.3425 (0.8893) 50c. of 30i. GQ15 33.6000 (1.4937) 94.6000 (0.7147) 96.4000 (0.5891) 95.1000 (0.6826) 50c. of 30i. GQ35 81.4000 (1.2305) 95.8000 (0.6343) 95.9000 (0.6270) 95.8000 (0.6343) 50c. of 30i. GQ75 93.7000 (0.7683) 95.8000 (0.6343) 95.7000 (0.6415) 96.2000 (0.6046) 50c. of 30i. GQ105 94.2000 (0.7392) 95.8000 (0.6343) 95.7000 (0.6415) 96.2000 (0.6046) Table A.7: Mean squared error, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size Method Lambda P Beta Theta 1000c. of 2i. AF 0.0042 0.0009 0.0057 0.0100 1000c. of 2i. IN 0.0043 0.0009 0.0056 0.0104 1000c. of 2i. GQ15 0.0075 0.0021 0.0051 0.1035 1000c. of 2i. GQ35 0.0060 0.0014 0.0052 0.0505 1000c. of 2i. GQ75 0.0050 0.0011 0.0053 0.0270 1000c. of 2i. GQ105 0.0047 0.0010 0.0054 0.0208 15c. of 100i. AF 0.0806 0.0009 0.0052 0.1437 15c. of 100i. IN 0.0888 0.0009 0.0051 0.1482 15c. of 100i. GQ15 0.2960 0.0010 0.0051 0.1221 15c. of 100i. GQ35 0.2031 0.0009 0.0051 0.1173 15c. of 100i. GQ75 0.1321 0.0009 0.0051 0.1255 15c. of 100i. GQ105 0.0948 0.0009 0.0051 0.1291 15c. of 30i. AF 0.0994 0.0032 0.0180 0.1583 15c. of 30i. IN 0.0995 0.0032 0.0180 0.1581 15c. of 30i. GQ15 0.1716 0.0032 0.0178 0.1316 15c. of 30i. GQ35 0.1052 0.0032 0.0179 0.1385 15c. of 30i. GQ75 0.1038 0.0032 0.0179 0.1404 15c. of 30i. GQ105 0.1000 0.0032 0.0180 0.1417 15c. of 500i. AF 0.0882 0.0002 0.0011 0.1402 15c. of 500i. IN 0.1485 0.0002 0.0011 0.1234 15c. of 500i. GQ15 0.3182 0.0005 0.0011 0.1106 15c. of 500i. GQ35 0.2453 0.0002 0.0011 0.1142 15c. of 500i. GQ75 0.2012 0.0004 0.0011 0.1231 15c. of 500i. GQ105 0.1859 0.0003 0.0011 0.1204 50c. of 100i. AF 0.0249 0.0003 0.0015 0.0352 50c. of 100i. IN 0.0263 0.0003 0.0015 0.0378 50c. of 100i. GQ15 0.3048 0.0003 0.0016 0.0551 50c. of 100i. GQ35 0.1491 0.0003 0.0015 0.0315 50c. of 100i. GQ75 0.0661 0.0003 0.0015 0.0301 50c. of 100i. GQ105 0.0375 0.0003 0.0015 0.0312 50c. of 30i. AF 0.0279 0.0009 0.0049 0.0375 50c. of 30i. IN 0.0279 0.0009 0.0048 0.0380 50c. of 30i. GQ15 0.1023 0.0010 0.0048 0.0383 50c. of 30i. GQ35 0.0351 0.0009 0.0049 0.0340 50c. of 30i. GQ75 0.0281 0.0009 0.0049 0.0319 50c. of 30i. GQ105 0.0279 0.0009 0.0049 0.0320 Table A.8: Convergence rates, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size AF IN GQ15 GQ35 GQ75 GQ105 1000c. of 2i. 100.0% 67.8% 100.0% 100.0% 100.0% 100.0% 15c. of 100i. 100.0% 93.7% 100.0% 100.0% 100.0% 100.0% 15c. of 30i. 100.0% 95.9% 100.0% 100.0% 100.0% 100.0% 15c. of 500i. 100.0% 87.4% 97.2% 97.7% 98.3% 98.5% 50c. of 100i. 100.0% 78.5% 100.0% 100.0% 100.0% 100.0% 50c. of 30i. 100.0% 79.7% 100.0% 100.0% 100.0% 100.0% Table A.9: Bias, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. fv Method Lambda P Beta Theta 0.25 AF -0.0016 (0.0015) 0.0002 (0.0008) -0.0018 (0.0018) -0.0342 (0.0070) 0.25 IN 0.0003 (0.0015) 0.0002 (0.0008) -0.0027 (0.0018) -0.0271 (0.0067) 0.25 GQ15 0.0069 (0.0014) 0.0073 (0.0007) 0.0035 (0.0018) 0.0723 (0.0016) 0.25 GQ35 -0.0015 (0.0015) 0.0004 (0.0008) -0.0016 (0.0018) -0.0296 (0.0067) 0.25 GQ75 -0.0016 (0.0015) 0.0002 (0.0008) -0.0018 (0.0018) -0.0342 (0.0070) 0.25 GQ105 -0.0016 (0.0015) 0.0002 (0.0008) -0.0018 (0.0018) -0.0342 (0.0070) 0.75 AF 0.0003 (0.0018) 0.0004 (0.0009) 0.0005 (0.0022) -0.0032 (0.0035) 0.75 IN -0.0008 (0.0018) 0.0005 (0.0009) -0.0000 (0.0022) -0.0039 (0.0035) 0.75 GQ15 0.0065 (0.0018) 0.0024 (0.0008) 0.0024 (0.0022) -0.0001 (0.0017) 0.75 GQ35 0.0040 (0.0018) 0.0025 (0.0008) 0.0022 (0.0022) 0.0110 (0.0026) 0.75 GQ75 0.0023 (0.0018) 0.0017 (0.0008) 0.0015 (0.0022) 0.0065 (0.0031) 0.75 GQ105 0.0017 (0.0018) 0.0014 (0.0009) 0.0013 (0.0022) 0.0043 (0.0032) 1.25 AF -0.0006 (0.0022) 0.0007 (0.0009) -0.0004 (0.0024) -0.0011 (0.0030) 1.25 IN -0.0009 (0.0021) 0.0005 (0.0009) 0.0006 (0.0024) 0.0015 (0.0029) 1.25 GQ15 -0.0575 (0.0020) -0.0426 (0.0008) -0.0312 (0.0023) -0.3082 (0.0010) 1.25 GQ35 -0.0437 (0.0020) -0.0299 (0.0008) -0.0226 (0.0023) -0.2074 (0.0014) 1.25 GQ75 -0.0320 (0.0021) -0.0209 (0.0008) -0.0162 (0.0023) -0.1436 (0.0017) 1.25 GQ105 -0.0276 (0.0021) -0.0177 (0.0008) -0.0139 (0.0024) -0.1215 (0.0018) Table A.10: Coverage, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance Method Lambda P Beta Theta 0.25 AF 94.9000 (0.6957) 93.5000 (0.7796) 96.0000 (0.6197) 95.7000 (0.6415) 0.25 IN 94.4730 (0.7226) 93.4447 (0.7827) 95.1157 (0.6816) 86.5039 (1.0805) 0.25 GQ15 95.5000 (0.6556) 93.6000 (0.7740) 96.2000 (0.6046) 99.8000 (0.1413) 0.25 GQ35 94.8898 (0.6964) 93.7876 (0.7633) 95.8918 (0.6276) 94.3888 (0.7278) 0.25 GQ75 94.9000 (0.6957) 93.5000 (0.7796) 96.0000 (0.6197) 95.7000 (0.6415) 0.25 GQ105 94.9000 (0.6957) 93.5000 (0.7796) 96.0000 (0.6197) 95.7000 (0.6415) 0.75 AF 94.6000 (0.7147) 94.5000 (0.7209) 94.7000 (0.7085) 94.7000 (0.7085) 0.75 IN 93.6198 (0.7729) 94.0104 (0.7504) 94.1406 (0.7427) 89.1927 (0.9818) 0.75 GQ15 95.0000 (0.6892) 95.4000 (0.6624) 94.6000 (0.7147) 99.4000 (0.2442) 0.75 GQ35 94.7000 (0.7085) 95.0000 (0.6892) 94.5000 (0.7209) 97.3000 (0.5126) 0.75 GQ75 94.5000 (0.7209) 94.9000 (0.6957) 94.6000 (0.7147) 95.7000 (0.6415) 0.75 GQ105 94.6000 (0.7147) 94.7000 (0.7085) 94.5000 (0.7209) 95.1000 (0.6826) 1.25 AF 94.4000 (0.7271) 95.0000 (0.6892) 95.1000 (0.6826) 94.3000 (0.7332) 1.25 IN 91.6064 (0.8769) 93.9219 (0.7556) 93.1983 (0.7962) 84.6599 (1.1396) 1.25 GQ15 81.7000 (1.2227) 61.2000 (1.5410) 92.5000 (0.8329) 0.0000 (0.0000) 1.25 GQ35 87.2000 (1.0565) 80.9000 (1.2431) 94.4000 (0.7271) 2.6000 (0.5032) 1.25 GQ75 91.0000 (0.9050) 90.1000 (0.9445) 95.2000 (0.6760) 49.6000 (1.5811) 1.25 GQ105 91.4000 (0.8866) 91.5000 (0.8819) 95.2000 (0.6760) 71.2000 (1.4320) Table A.11: Percentage bias (%), comparison with analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance Method Lambda P Beta Theta 0.25 AF 0.2294 -0.0426 -0.3674 2.4659 0.25 IN -0.0399 -0.0327 -0.5387 1.9523 0.25 GQ15 -0.9919 -1.4226 0.7090 -5.2137 0.25 GQ35 0.2175 -0.0801 -0.3289 2.1336 0.25 GQ75 0.2298 -0.0421 -0.3678 2.4656 0.25 GQ105 0.2297 -0.0423 -0.3677 2.4668 0.75 AF -0.0404 -0.0798 0.1065 1.0954 0.75 IN 0.1171 -0.0916 -0.0028 1.3386 0.75 GQ15 -0.9319 -0.4739 0.4804 0.0287 0.75 GQ35 -0.5823 -0.4846 0.4319 -3.8150 0.75 GQ75 -0.3289 -0.3279 0.3001 -2.2747 0.75 GQ105 -0.2506 -0.2665 0.2514 -1.4891 1.25 AF 0.0931 -0.1407 -0.0851 -0.4862 1.25 IN 0.1267 -0.0883 0.1262 0.6778 1.25 GQ15 8.2934 8.3381 -6.2492 -138.1108 1.25 GQ35 6.2978 5.8510 -4.5265 -92.9554 1.25 GQ75 4.6158 4.0956 -3.2494 -64.3743 1.25 GQ105 3.9749 3.4630 -2.7833 -54.4579 Table A.12: Mean squared error, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance Method Lambda P Beta Theta 0.25 AF 0.0022 0.0007 0.0033 0.0503 0.25 IN 0.0022 0.0007 0.0033 0.0451 0.25 GQ15 0.0021 0.0006 0.0033 0.0077 0.25 GQ35 0.0022 0.0007 0.0033 0.0452 0.25 GQ75 0.0022 0.0007 0.0033 0.0502 0.25 GQ105 0.0022 0.0007 0.0033 0.0503 0.75 AF 0.0032 0.0007 0.0048 0.0122 0.75 IN 0.0032 0.0007 0.0049 0.0121 0.75 GQ15 0.0031 0.0006 0.0048 0.0027 0.75 GQ35 0.0032 0.0007 0.0048 0.0067 0.75 GQ75 0.0032 0.0007 0.0048 0.0097 0.75 GQ105 0.0032 0.0007 0.0048 0.0105 1.25 AF 0.0048 0.0008 0.0060 0.0088 1.25 IN 0.0046 0.0008 0.0059 0.0085 1.25 GQ15 0.0072 0.0024 0.0061 0.0961 1.25 GQ35 0.0060 0.0015 0.0059 0.0450 1.25 GQ75 0.0052 0.0011 0.0058 0.0235 1.25 GQ105 0.0050 0.0010 0.0058 0.0180 Table A.13: Convergence rates, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance AF IN GQ15 GQ35 GQ75 GQ105 0.25 100.0% 77.8% 100.0% 99.8% 100.0% 100.0% 0.75 100.0% 76.8% 100.0% 100.0% 100.0% 100.0% 1.25 100.0% 69.1% 100.0% 100.0% 100.0% 100.0% Table A.14: Bias, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 0.0187 (0.0012) -0.0415 (0.0008) 0.0149 (0.0019) -0.3010 (0.0250) 1000c. of 2i. GQ35 0.0188 (0.0012) -0.0415 (0.0008) 0.0144 (0.0019) -0.2842 (0.0232) 1000c. of 2i. GQ75 0.0188 (0.0012) -0.0415 (0.0008) 0.0146 (0.0019) -0.3057 (0.0254) 1000c. of 2i. GQ105 0.0187 (0.0012) -0.0415 (0.0008) 0.0147 (0.0019) -0.2946 (0.0243) 15c. of 100i. GQ15 0.0102 (0.0040) -0.0332 (0.0010) 0.0552 (0.0040) 0.0282 (0.0113) 15c. of 100i. GQ35 0.0102 (0.0040) -0.0333 (0.0010) 0.0173 (0.0024) -0.0854 (0.0085) 15c. of 100i. GQ75 0.0102 (0.0040) -0.0333 (0.0010) 0.0088 (0.0022) -0.1015 (0.0075) 15c. of 100i. GQ105 0.0102 (0.0040) -0.0333 (0.0010) 0.0088 (0.0022) -0.1015 (0.0075) 15c. of 30i. GQ15 0.0193 (0.0045) -0.0348 (0.0017) 0.0047 (0.0039) -0.2510 (0.0272) 15c. of 30i. GQ35 0.0192 (0.0045) -0.0348 (0.0017) 0.0041 (0.0039) -0.2455 (0.0256) 15c. of 30i. GQ75 0.0191 (0.0045) -0.0348 (0.0017) 0.0041 (0.0039) -0.2593 (0.0275) 15c. of 30i. GQ105 0.0195 (0.0045) -0.0347 (0.0017) 0.0040 (0.0039) -0.2522 (0.0265) 15c. of 500i. GQ15 0.0113 (0.0037) -0.0411 (0.0006) 0.0867 (0.0050) 0.2427 (0.0104) 15c. of 500i. GQ35 0.0157 (0.0038) -0.0423 (0.0006) 0.0860 (0.0040) 0.4011 (0.0091) 15c. of 500i. GQ75 0.0078 (0.0036) -0.0405 (0.0005) 0.0707 (0.0033) 0.4020 (0.0112) 15c. of 500i. GQ105 0.0059 (0.0036) -0.0410 (0.0006) 0.0600 (0.0028) 0.3119 (0.0125) 50c. of 100i. GQ15 0.0135 (0.0022) -0.0365 (0.0005) 0.0244 (0.0020) -0.0095 (0.0049) 50c. of 100i. GQ35 0.0135 (0.0022) -0.0365 (0.0005) 0.0102 (0.0012) -0.0350 (0.0038) 50c. of 100i. GQ75 0.0135 (0.0022) -0.0365 (0.0005) 0.0105 (0.0012) -0.0350 (0.0037) 50c. of 100i. GQ105 0.0135 (0.0022) -0.0365 (0.0005) 0.0105 (0.0012) -0.0350 (0.0037) 50c. of 30i. GQ15 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0535 (0.0052) 50c. of 30i. GQ35 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0536 (0.0052) 50c. of 30i. GQ75 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0536 (0.0052) 50c. of 30i. GQ105 0.0154 (0.0025) -0.0367 (0.0009) 0.0079 (0.0021) -0.0536 (0.0052) Table A.15: Coverage, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 92.5662 (0.8295) 64.8676 (1.5096) 94.5010 (0.7209) 99.0835 (0.3013) 1000c. of 2i. GQ35 92.4413 (0.8359) 64.7600 (1.5107) 94.6885 (0.7092) 99.0807 (0.3018) 1000c. of 2i. GQ75 92.4720 (0.8343) 64.6999 (1.5113) 94.6083 (0.7142) 99.0844 (0.3012) 1000c. of 2i. GQ105 92.5586 (0.8299) 64.8318 (1.5100) 94.5973 (0.7149) 99.0826 (0.3015) 15c. of 100i. GQ15 52.1000 (1.5797) 76.4000 (1.3428) 86.0000 (1.0973) 70.2000 (1.4464) 15c. of 100i. GQ35 52.1000 (1.5797) 76.5000 (1.3408) 98.8000 (0.3443) 88.9000 (0.9934) 15c. of 100i. GQ75 52.1000 (1.5797) 76.4000 (1.3428) 99.9000 (0.0999) 94.0000 (0.7510) 15c. of 100i. GQ105 52.1000 (1.5797) 76.4000 (1.3428) 99.9000 (0.0999) 94.1000 (0.7451) 15c. of 30i. GQ15 75.4016 (1.3619) 90.0602 (0.9461) 99.0964 (0.2992) 98.3936 (0.3976) 15c. of 30i. GQ35 75.3769 (1.3624) 90.2513 (0.9380) 99.0955 (0.2994) 99.1960 (0.2824) 15c. of 30i. GQ75 75.4263 (1.3614) 90.0702 (0.9457) 99.0973 (0.2991) 99.2979 (0.2640) 15c. of 30i. GQ105 75.5020 (1.3600) 90.0602 (0.9461) 99.0964 (0.2992) 99.2972 (0.2642) 15c. of 500i. GQ15 30.0813 (1.4503) 21.3415 (1.2956) 27.6423 (1.4143) 23.1707 (1.3342) 15c. of 500i. GQ35 27.8826 (1.4180) 17.8197 (1.2101) 37.1069 (1.5277) 15.7233 (1.1511) 15c. of 500i. GQ75 30.1053 (1.4506) 18.9474 (1.2392) 55.5789 (1.5713) 16.6316 (1.1775) 15c. of 500i. GQ105 29.1028 (1.4364) 18.3807 (1.2248) 67.8337 (1.4771) 32.1663 (1.4771) 50c. of 100i. GQ15 51.9000 (1.5800) 37.8000 (1.5333) 94.8000 (0.7021) 85.6000 (1.1102) 50c. of 100i. GQ35 51.9000 (1.5800) 37.5000 (1.5309) 100.0000 (0.0000) 95.0000 (0.6892) 50c. of 100i. GQ75 51.9000 (1.5800) 37.5000 (1.5309) 100.0000 (0.0000) 94.9000 (0.6957) 50c. of 100i. GQ105 51.9000 (1.5800) 37.5000 (1.5309) 100.0000 (0.0000) 94.9000 (0.6957) 50c. of 30i. GQ15 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) 50c. of 30i. GQ35 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) 50c. of 30i. GQ75 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) 50c. of 30i. GQ105 73.9000 (1.3888) 74.1000 (1.3853) 99.4000 (0.2442) 97.2000 (0.5217) Table A.16: Mean squared error, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 0.0019 0.0024 0.0040 0.7180 1000c. of 2i. GQ35 0.0019 0.0024 0.0039 0.6198 1000c. of 2i. GQ75 0.0019 0.0024 0.0039 0.7394 1000c. of 2i. GQ105 0.0019 0.0024 0.0039 0.6752 15c. of 100i. GQ15 0.0164 0.0021 0.0189 0.1285 15c. of 100i. GQ35 0.0164 0.0021 0.0063 0.0790 15c. of 100i. GQ75 0.0164 0.0022 0.0048 0.0665 15c. of 100i. GQ105 0.0164 0.0022 0.0048 0.0665 15c. of 30i. GQ15 0.0208 0.0041 0.0152 0.8029 15c. of 30i. GQ35 0.0208 0.0040 0.0149 0.7164 15c. of 30i. GQ75 0.0208 0.0041 0.0149 0.8228 15c. of 30i. GQ105 0.0207 0.0041 0.0149 0.7636 15c. of 500i. GQ15 0.0141 0.0020 0.0326 0.1666 15c. of 500i. GQ35 0.0146 0.0021 0.0234 0.2438 15c. of 500i. GQ75 0.0131 0.0019 0.0160 0.2881 15c. of 500i. GQ105 0.0129 0.0020 0.0113 0.2542 50c. of 100i. GQ15 0.0048 0.0016 0.0046 0.0237 50c. of 100i. GQ35 0.0048 0.0016 0.0015 0.0153 50c. of 100i. GQ75 0.0048 0.0016 0.0015 0.0153 50c. of 100i. GQ105 0.0048 0.0016 0.0015 0.0153 50c. of 30i. GQ15 0.0066 0.0022 0.0043 0.0301 50c. of 30i. GQ35 0.0066 0.0022 0.0043 0.0300 50c. of 30i. GQ75 0.0066 0.0022 0.0043 0.0300 50c. of 30i. GQ105 0.0066 0.0022 0.0043 0.0300 Table A.17: Convergence rates, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Sample size GQ15 GQ35 GQ75 GQ105 1000c. of 2i. 98.2% 97.9% 98.3% 98.1% 15c. of 100i. 100.0% 100.0% 100.0% 100.0% 15c. of 30i. 99.6% 99.5% 99.7% 99.6% 15c. of 500i. 49.2% 47.7% 47.5% 45.7% 50c. of 100i. 100.0% 100.0% 100.0% 100.0% 50c. of 30i. 100.0% 100.0% 100.0% 100.0% Table A.18: Bias, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 0.0622 (0.0015) -0.1785 (0.0008) 0.0043 (0.0019) -0.3291 (0.0032) 1000c. of 2i. GQ35 0.0622 (0.0015) -0.1785 (0.0008) 0.0043 (0.0019) -0.3291 (0.0032) 1000c. of 2i. GQ75 0.0622 (0.0015) -0.1785 (0.0008) 0.0043 (0.0019) -0.3291 (0.0032) 1000c. of 2i. GQ105 0.0622 (0.0015) -0.1785 (0.0008) 0.0043 (0.0019) -0.3291 (0.0032) 15c. of 100i. GQ15 0.0438 (0.0071) -0.1252 (0.0016) 0.0506 (0.0064) -0.1666 (0.0046) 15c. of 100i. GQ35 0.0427 (0.0071) -0.1230 (0.0016) -0.0051 (0.0051) -0.1106 (0.0043) 15c. of 100i. GQ75 0.0431 (0.0071) -0.1235 (0.0016) -0.0299 (0.0041) -0.1262 (0.0058) 15c. of 100i. GQ105 0.0431 (0.0071) -0.1238 (0.0016) -0.0231 (0.0037) -0.1356 (0.0060) 15c. of 30i. GQ15 0.0241 (0.0077) -0.1266 (0.0021) -0.0382 (0.0072) -0.1045 (0.0069) 15c. of 30i. GQ35 0.0249 (0.0077) -0.1282 (0.0021) -0.0173 (0.0051) -0.1319 (0.0072) 15c. of 30i. GQ75 0.0252 (0.0077) -0.1288 (0.0021) 0.0049 (0.0044) -0.1442 (0.0068) 15c. of 30i. GQ105 0.0252 (0.0077) -0.1289 (0.0021) 0.0053 (0.0044) -0.1457 (0.0067) 15c. of 500i. GQ15 -0.0588 (0.0051) -0.1209 (0.0012) 0.0878 (0.0049) -0.2444 (0.0047) 15c. of 500i. GQ35 -0.0544 (0.0051) -0.1212 (0.0011) 0.0453 (0.0042) -0.1481 (0.0036) 15c. of 500i. GQ75 -0.0609 (0.0052) -0.1179 (0.0011) 0.0199 (0.0032) -0.1001 (0.0026) 15c. of 500i. GQ105 -0.0438 (0.0051) -0.1206 (0.0011) 0.0139 (0.0028) -0.0982 (0.0023) 50c. of 100i. GQ15 0.0436 (0.0038) -0.1336 (0.0009) 0.0519 (0.0050) -0.2172 (0.0032) 50c. of 100i. GQ35 0.0423 (0.0038) -0.1310 (0.0009) 0.0118 (0.0040) -0.1119 (0.0027) 50c. of 100i. GQ75 0.0422 (0.0038) -0.1308 (0.0009) -0.0157 (0.0028) -0.0821 (0.0031) 50c. of 100i. GQ105 0.0424 (0.0038) -0.1310 (0.0009) -0.0060 (0.0022) -0.0830 (0.0033) 50c. of 30i. GQ15 0.0349 (0.0041) -0.1342 (0.0011) 0.0016 (0.0044) -0.1006 (0.0036) 50c. of 30i. GQ35 0.0349 (0.0041) -0.1341 (0.0011) 0.0078 (0.0027) -0.0891 (0.0037) 50c. of 30i. GQ75 0.0350 (0.0041) -0.1343 (0.0011) 0.0107 (0.0024) -0.0939 (0.0035) 50c. of 30i. GQ105 0.0350 (0.0041) -0.1343 (0.0011) 0.0107 (0.0024) -0.0940 (0.0035) Table A.19: Coverage, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 64.4000 (1.5141) 0.0000 (0.0000) 95.8000 (0.6343) 6.9000 (0.8015) 1000c. of 2i. GQ35 64.4000 (1.5141) 0.0000 (0.0000) 95.8000 (0.6343) 7.0000 (0.8068) 1000c. of 2i. GQ75 64.4000 (1.5141) 0.0000 (0.0000) 95.8000 (0.6343) 7.0000 (0.8068) 1000c. of 2i. GQ105 64.4000 (1.5141) 0.0000 (0.0000) 95.8000 (0.6343) 7.0000 (0.8068) 15c. of 100i. GQ15 31.2000 (1.4651) 7.9000 (0.8530) 53.6000 (1.5770) 43.4000 (1.5673) 15c. of 100i. GQ35 31.2000 (1.4651) 8.8000 (0.8959) 80.2000 (1.2601) 75.6000 (1.3582) 15c. of 100i. GQ75 31.3313 (1.4668) 8.5085 (0.8823) 96.3964 (0.5894) 83.1832 (1.1827) 15c. of 100i. GQ105 31.3000 (1.4664) 8.4000 (0.8772) 98.6000 (0.3715) 86.0000 (1.0973) 15c. of 30i. GQ15 52.4000 (1.5793) 35.0000 (1.5083) 91.2000 (0.8959) 87.6000 (1.0422) 15c. of 30i. GQ35 52.4000 (1.5793) 33.9000 (1.4969) 99.5000 (0.2230) 91.1000 (0.9004) 15c. of 30i. GQ75 52.3000 (1.5795) 33.5000 (1.4926) 100.0000 (0.0000) 92.3000 (0.8430) 15c. of 30i. GQ105 52.3000 (1.5795) 33.5000 (1.4926) 100.0000 (0.0000) 92.8000 (0.8174) 15c. of 500i. GQ15 18.7919 (1.2353) 0.0000 (0.0000) 37.5839 (1.5316) 9.3960 (0.9227) 15c. of 500i. GQ35 19.3548 (1.2493) 0.0000 (0.0000) 36.1290 (1.5191) 19.3548 (1.2493) 15c. of 500i. GQ75 17.5325 (1.2024) 0.0000 (0.0000) 56.4935 (1.5677) 33.1169 (1.4883) 15c. of 500i. GQ105 20.2454 (1.2707) 0.0000 (0.0000) 60.7362 (1.5443) 37.4233 (1.5303) 50c. of 100i. GQ15 31.2312 (1.4655) 0.0000 (0.0000) 49.2492 (1.5810) 10.1101 (0.9533) 50c. of 100i. GQ35 31.4000 (1.4677) 0.0000 (0.0000) 70.2000 (1.4464) 46.6000 (1.5775) 50c. of 100i. GQ75 31.5000 (1.4689) 0.0000 (0.0000) 97.0000 (0.5394) 79.2000 (1.2835) 50c. of 100i. GQ105 31.4314 (1.4681) 0.0000 (0.0000) 99.5996 (0.1997) 83.4835 (1.1742) 50c. of 30i. GQ15 49.2000 (1.5809) 1.4000 (0.3715) 90.7000 (0.9184) 74.9000 (1.3711) 50c. of 30i. GQ35 49.2000 (1.5809) 1.5000 (0.3844) 100.0000 (0.0000) 88.3000 (1.0164) 50c. of 30i. GQ75 49.0000 (1.5808) 1.4000 (0.3715) 100.0000 (0.0000) 89.6000 (0.9653) 50c. of 30i. GQ105 49.1000 (1.5809) 1.4000 (0.3715) 100.0000 (0.0000) 89.6000 (0.9653) Table A.20: Mean squared error, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size Method Lambda P Beta Sigma 1000c. of 2i. GQ15 0.0061 0.0325 0.0037 0.1187 1000c. of 2i. GQ35 0.0061 0.0325 0.0037 0.1187 1000c. of 2i. GQ75 0.0061 0.0325 0.0037 0.1187 1000c. of 2i. GQ105 0.0061 0.0325 0.0037 0.1187 15c. of 100i. GQ15 0.0524 0.0182 0.0434 0.0490 15c. of 100i. GQ35 0.0524 0.0177 0.0259 0.0311 15c. of 100i. GQ75 0.0524 0.0178 0.0179 0.0491 15c. of 100i. GQ105 0.0523 0.0179 0.0139 0.0549 15c. of 30i. GQ15 0.0595 0.0206 0.0527 0.0585 15c. of 30i. GQ35 0.0594 0.0210 0.0265 0.0695 15c. of 30i. GQ75 0.0594 0.0212 0.0194 0.0675 15c. of 30i. GQ105 0.0594 0.0212 0.0194 0.0668 15c. of 500i. GQ15 0.0299 0.0160 0.0319 0.0815 15c. of 500i. GQ35 0.0291 0.0160 0.0193 0.0351 15c. of 500i. GQ75 0.0310 0.0151 0.0104 0.0169 15c. of 500i. GQ105 0.0282 0.0158 0.0080 0.0151 50c. of 100i. GQ15 0.0163 0.0187 0.0273 0.0575 50c. of 100i. GQ35 0.0162 0.0180 0.0162 0.0198 50c. of 100i. GQ75 0.0162 0.0179 0.0083 0.0161 50c. of 100i. GQ105 0.0162 0.0180 0.0048 0.0175 50c. of 30i. GQ15 0.0178 0.0193 0.0193 0.0232 50c. of 30i. GQ35 0.0178 0.0193 0.0075 0.0217 50c. of 30i. GQ75 0.0178 0.0194 0.0060 0.0214 50c. of 30i. GQ105 0.0178 0.0194 0.0060 0.0214 Table A.21: Convergence rates, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Sample size GQ15 GQ35 GQ75 GQ105 1000c. of 2i. 100.0% 100.0% 100.0% 100.0% 15c. of 100i. 100.0% 100.0% 99.9% 100.0% 15c. of 30i. 100.0% 100.0% 100.0% 100.0% 15c. of 500i. 14.9% 15.5% 15.4% 16.3% 50c. of 100i. 99.9% 100.0% 100.0% 99.9% 50c. of 30i. 100.0% 100.0% 100.0% 100.0% Table A.22: Bias, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. fv Method Lambda P Beta Sigma 0.25 GQ15 0.0156 (0.0013) -0.0378 (0.0008) -0.0128 (0.0017) -0.2150 (0.0093) 0.25 GQ35 0.0156 (0.0013) -0.0378 (0.0008) -0.0128 (0.0017) -0.2150 (0.0093) 0.25 GQ75 0.0156 (0.0013) -0.0378 (0.0008) -0.0128 (0.0017) -0.2150 (0.0093) 0.25 GQ105 0.0156 (0.0013) -0.0378 (0.0008) -0.0127 (0.0017) -0.2112 (0.0085) 0.75 GQ15 0.0429 (0.0014) -0.1108 (0.0008) -0.0431 (0.0019) -0.2560 (0.0036) 0.75 GQ35 0.0429 (0.0014) -0.1108 (0.0008) -0.0431 (0.0019) -0.2560 (0.0036) 0.75 GQ75 0.0429 (0.0014) -0.1108 (0.0008) -0.0431 (0.0019) -0.2560 (0.0036) 0.75 GQ105 0.0429 (0.0014) -0.1108 (0.0008) -0.0431 (0.0019) -0.2560 (0.0036) 1.25 GQ15 0.0652 (0.0015) -0.1758 (0.0008) -0.0749 (0.0019) -0.3289 (0.0029) 1.25 GQ35 0.0652 (0.0015) -0.1758 (0.0008) -0.0749 (0.0019) -0.3289 (0.0029) 1.25 GQ75 0.0652 (0.0015) -0.1758 (0.0008) -0.0749 (0.0019) -0.3289 (0.0029) 1.25 GQ105 0.0652 (0.0015) -0.1758 (0.0008) -0.0749 (0.0019) -0.3289 (0.0029) Table A.23: Coverage, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance Method Lambda P Beta Sigma 0.25 GQ15 92.8929 (0.8125) 66.5666 (1.4918) 95.8959 (0.6273) 99.4995 (0.2232) 0.25 GQ35 92.8929 (0.8125) 66.5666 (1.4918) 95.8959 (0.6273) 99.4995 (0.2232) 0.25 GQ75 92.8929 (0.8125) 66.5666 (1.4918) 95.8959 (0.6273) 99.4995 (0.2232) 0.25 GQ105 92.8858 (0.8129) 66.6333 (1.4911) 95.8918 (0.6276) 99.4990 (0.2233) 0.75 GQ15 77.8000 (1.3142) 0.7000 (0.2636) 89.0000 (0.9894) 39.7000 (1.5472) 0.75 GQ35 77.8000 (1.3142) 0.7000 (0.2636) 89.0000 (0.9894) 39.7000 (1.5472) 0.75 GQ75 77.8000 (1.3142) 0.7000 (0.2636) 89.0000 (0.9894) 39.7000 (1.5472) 0.75 GQ105 77.8000 (1.3142) 0.7000 (0.2636) 89.0000 (0.9894) 39.7000 (1.5472) 1.25 GQ15 60.9000 (1.5431) 0.0000 (0.0000) 78.7000 (1.2947) 3.3000 (0.5649) 1.25 GQ35 60.9000 (1.5431) 0.0000 (0.0000) 78.7000 (1.2947) 3.3000 (0.5649) 1.25 GQ75 60.9000 (1.5431) 0.0000 (0.0000) 78.7000 (1.2947) 3.3000 (0.5649) 1.25 GQ105 60.9000 (1.5431) 0.0000 (0.0000) 78.7000 (1.2947) 3.3000 (0.5649) Table A.24: Percentage bias (%), comparison without analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance Method Lambda P Beta Sigma 0.25 GQ15 -2.2571 7.4087 -2.5529 31.0210 0.25 GQ35 -2.2571 7.4087 -2.5529 31.0219 0.25 GQ75 -2.2571 7.4087 -2.5529 31.0214 0.25 GQ105 -2.2498 7.4012 -2.5382 30.4701 0.75 GQ15 -6.1917 21.6854 -8.6277 177.9883 0.75 GQ35 -6.1917 21.6855 -8.6277 177.9884 0.75 GQ75 -6.1917 21.6855 -8.6277 177.9884 0.75 GQ105 -6.1917 21.6855 -8.6277 177.9884 1.25 GQ15 -9.4056 34.4137 -14.9841 -294.7880 1.25 GQ35 -9.4055 34.4134 -14.9838 -294.7815 1.25 GQ75 -9.4055 34.4134 -14.9838 -294.7815 1.25 GQ105 -9.4055 34.4134 -14.9838 -294.7815 Table A.25: Mean squared error, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance Method Lambda P Beta Sigma 0.25 GQ15 0.0019 0.0021 0.0029 0.1334 0.25 GQ35 0.0019 0.0021 0.0029 0.1335 0.25 GQ75 0.0019 0.0021 0.0029 0.1335 0.25 GQ105 0.0019 0.0021 0.0029 0.1173 0.75 GQ15 0.0039 0.0129 0.0054 0.0786 0.75 GQ35 0.0039 0.0129 0.0054 0.0786 0.75 GQ75 0.0039 0.0129 0.0054 0.0786 0.75 GQ105 0.0039 0.0129 0.0054 0.0786 1.25 GQ15 0.0065 0.0316 0.0094 0.1165 1.25 GQ35 0.0065 0.0316 0.0094 0.1165 1.25 GQ75 0.0065 0.0316 0.0094 0.1165 1.25 GQ105 0.0065 0.0316 0.0094 0.1165 Table A.26: Convergence rates, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals and a positive regression coefficient. Frailty variance GQ15 GQ35 GQ75 GQ105 0.25 99.9% 99.9% 99.9% 99.8% 0.75 100.0% 100.0% 100.0% 100.0% 1.25 100.0% 100.0% 100.0% 100.0% "],
["ax-tables-simst2.html", "A.2 Simulation study 2", " A.2 Simulation study 2 -->"],
["ax-plots.html", "B Plots ", " B Plots "],
["ax-plots-simst1.html", "B.1 Simulation study 1", " B.1 Simulation study 1 Figure B.1: Bias, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.2: Coverage, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.3: Mean squared error, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.4: Convergence rates, comparison with analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.5: Bias, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.6: Coverage, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.7: Mean squared error, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.8: Convergence rates, comparison with analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.9: Bias, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.10: Coverage, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.11: Percentage bias, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.12: Mean squared error, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.13: Convergence rates, comparison with analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.14: Bias, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.15: Coverage, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.16: Mean squared error, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.17: Convergence rates, comparison without analytical formulae, scenario with a small frailty variance and a negative regression coefficient. Figure B.18: Bias, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.19: Coverage, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.20: Mean squared error, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.21: Convergence rates, comparison without analytical formulae, scenario with a large frailty variance and a null regression coefficient. Figure B.22: Bias, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.23: Coverage, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.24: Percentage bias, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.25: Mean squared error, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. Figure B.26: Convergence rates, comparison without analytical formulae, scenario with 1000 clusters of 2 individuals each with a positive regression coefficient. "],
["ax-plots-simst2.html", "B.2 Simulation study 2", " B.2 Simulation study 2 -->"],
["ax-slides.html", "C Slides", " C Slides "],
["ax-manuscript.html", "D Manuscript", " D Manuscript "],
["ax-r-session.html", "E R Session", " E R Session sessionInfo() ## R version 3.4.1 (2017-06-30) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United Kingdom.1252 ## [2] LC_CTYPE=English_United Kingdom.1252 ## [3] LC_MONETARY=English_United Kingdom.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United Kingdom.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bindrcpp_0.2 kableExtra_0.4.0 tidyr_0.6.3 scales_0.4.1 ## [5] marqLevAlg_1.1 readr_1.1.1 ggfortify_0.4.1 dplyr_0.7.2 ## [9] cowplot_0.8.0 survival_2.41-3 ggplot2_2.2.1 pacman_0.4.6 ## [13] knitr_1.17 bookdown_0.4 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.12 highr_0.6 compiler_3.4.1 plyr_1.8.4 ## [5] bindr_0.1 tools_3.4.1 digest_0.6.12 evaluate_0.10.1 ## [9] tibble_1.3.3 gtable_0.2.0 lattice_0.20-35 pkgconfig_2.0.1 ## [13] rlang_0.1.2 Matrix_1.2-10 rstudioapi_0.6 yaml_2.1.14 ## [17] gridExtra_2.2.1 xml2_1.1.1 httr_1.2.1 stringr_1.2.0 ## [21] hms_0.3 rprojroot_1.2 grid_3.4.1 glue_1.1.1 ## [25] R6_2.2.2 rmarkdown_1.6 reshape2_1.4.2 magrittr_1.5 ## [29] backports_1.1.0 htmltools_0.3.6 splines_3.4.1 rvest_0.3.2 ## [33] assertthat_0.2.0 colorspace_1.3-2 labeling_0.3 stringi_1.1.5 ## [37] lazyeval_0.2.0 munsell_0.4.3 "],
["references.html", "References", " References "]
]
