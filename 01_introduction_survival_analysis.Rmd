# Introduction to survival analysis {#sa}

Survival analysis is a branch of statistics in which the main outcome consists in the time until the occurrence of a given event. Time could be years, months, weeks, or any amount of calendar time or even age time; event could be death, disease occurrence or relapse, or any other experience of interest. Survival analysis is also known as reliability theory in engineering, duration analysis in economics, and event history analysis in sociology. A broad overview of survival analysis is given in @kalbfleisch_2011 and in @kleinbaum_2012.

Some examples of time to event data are:

- disease remission in leukemia patients. In this study, leukemia patients are followed over several weeks to study how long they stay in remission status;

- heart disease occurrence. In this study, healthy subjects are followed over several years until occurrence of heart disease, or end of the study;

- renal failure. In this study, individuals with kidney disease are followed until renal failure, or end of the study;

- reliability of complex technical installations. For instance, studies assessing failure rates of components such as bulbs and valves.

In this Chapter I will define survival data and its peculiarities in Section \@ref(sa-survdata) and \@ref(sa-censoring). Terminology and notation used throughout this report will be introduced in Section \@ref(sa-terminology-notation). I will introduce common non-parametric and parametric methods in survival analysis in Sections \@ref(sa-estsurv) and \@ref(sa-parsa). I will introduce the widely used semi-parametric Cox model in Section \@ref(sa-cox). Finally, I will provide a brief overview on advances in survival analysis in Section \@ref(sa-advances). 

## Survival data {#sa-survdata}

Survival data generally consists - as previously mentioned - in an event of interest and time until its occurrence. In the leukemia remission example, time to event would be how many weeks it takes before a given patient experiences disease relapse and the event would be whether the individual relapsed or not before the end of the study. Nevertheless, in certain situations we may have some information about the survival time but the actual survival time may be unknown. This problem is know as censoring and it is presented in Section \@ref(sa-censoring).

## Censoring {#sa-censoring}

Censoring is a mechanisms that causes survival times to be unobserved. There are many reasons why censoring may occur; among others:

1. a person does not experience the event before the end of the study;

2. a person drops out of the study before the occurrence of the event of interest;

3. a person experience a competing event that impedes the occurrence of the event of interest (e.g.: death, when death is not the study outcome).

```{r censoring, fig.width = 2 * opts_chunk$get()$fig.width, fig.cap = "Simulated right censored survival data, plotted by their calendar time in panel A and by their study time in panel B."}
set.seed(1)
n = 10
u = runif(n)
lambda = 0.1
p = 1.5
entry = runif(n)
s = (-log(u) / (lambda)) ^ (1 / p) + entry
c = 5
t = pmin(s, c)
t0 = t - entry
d = as.numeric(s <= c)
df = factor(d, levels = 0:1, labels = c("Censored", "Event"))

data = data.frame(
  id = LETTERS[1:n],
  entry = entry,
  s = s,
  c = c,
  t = t,
  t0 = t0,
  d = d,
  df = df)

g1 = ggplot(data, aes(x = t, y = id, shape = df)) + 
  geom_vline(xintercept = 0, lty = "dashed") +
  geom_vline(xintercept = 1, lty = "dotted") +
  geom_segment(aes(x = entry, xend = t, y = id, yend = id), color = "grey50") +
  geom_point(size = 2) +
  scale_shape_manual(values = c(1, 4)) +
  theme_bw() +
  theme(legend.position = "bottom") + 
  labs(y = "Individual", x = "Calendar time", shape = "")

g2 = ggplot(data, aes(x = t0, y = id, shape = df)) + 
  geom_vline(xintercept = 0, lty = "dashed") +
  geom_segment(aes(x = 0, xend = t0, y = id, yend = id), color = "grey50") +
  geom_point(size = 2) +
  scale_shape_manual(values = c(1, 4)) +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(y = "Individual", x = "Study time", shape = "")

plot_grid(g1, g2, align = "hv", labels = LETTERS)

rm(n, u, entry, s, c, t, d, g1, g2)
```

I simulated survival data for illustration purposes: I assumed a clinical trial with 10 individuals enrolled during a recruitment window of 1 year, and followed for up to 5 years. Not all individuals experience the event of interest during the study period, and are therefore censored after five years from the start of the study. The observation time for each individual is depicted in Figure \@ref(fig:censoring) with a solid dark grey line, a cross represents the occurrence of the study event, and a circle represents censoring. Individuals `A`, `E`, and `J`  all have censored survival time: I know that they were still event-free at the end of follow-up, i.e. their real survival time is greater than the observed one, but the former is unknown. The simulated data is presented in Figure \@ref(fig:censoring): in panel A, survival data is plotted against the calendar time; conversely, in panel B, survival data is plotted against the study time, e.g. each individual is assigned a _time zero_ corresponding to their enrollment in the study, and survival time is counted from there.

This example represents a particular form of censoring: _right censoring_. The defining characteristic of right-censored data is that it is censored (or incomplete) at the right side of the follow-up time, hence the true survival time is greater than the observed time. This example represents _administrative censoring_ as well, as individuals are censored at the end of the study to artificially restrict follow-up time (e.g. for financial reasons). 

It is also possible to encounter data that is _left censored_ or _interval censored_. In the former case, the true survival time is shorter that the observed one, e.g. I know that the event occurred before the observation time, but I do not know when - imagine onset of a viral infection, which can be detected only at a visit time. In the latter, I know that the event occurred within a certain interval of time but I do not know when; using the same example of infection onset, if infection was detected at a visit date but the individual was known to be infection-free at the previous visit, the true infection onset time is unknown and the event time is said to be interval censored.

Finally, another important concept related to right censoring is that of _left truncation_ (or _delayed entry_). Left truncation occurs when an individual enrolls in the study some time after the inclusion criteria are satisfied; individuals that die (or emigrate, ...) before the start of observation time will never enter the study, and inclusion time may differ between individuals. Data arising from such phenomenon is therefore said to be left truncated.

## Terminology and notation {#sa-terminology-notation}

I denote the random variable for an individual's survival time with \(S\); since it denotes time, \(S\) can assume any non-negative value. The lower-case \(s\) represent a specific value of interest drawn from \(S\) for a given individual. In the case of right censoring, I denote with \(C\) the random variable representing censoring time, and \(c\) its realisation. The observed time is denoted with \(T = \min(S, C)\), and its realisation is \(t\). Finally, I denote with \(D = I(S \le C)\) the random variable indicating either occurrence of the event of interest or censorship; analogously as before, its realisation is lower-case \(d\).

Next, I defined two quantities of interest in survival analysis, the _survival function_ and the _hazard function_. They are both functions of the observed time \(t\) and are denoted by \(S(t)\) and \(h(t)\), respectively. 

The survival function is the complement of the cumulative distribution function of the observed time \(T\) and represent the probability that a given individual survives^[I use the term _survives_ loosely speaking, for conciseness - formally, I refer to _not experiencing the event of interest_.] longer than a specified time \(t\): 
\[
S(t) = 1 - F_T(t) = 1 - P(T \le t) = P(T > t)
\]
\(t\) ranges (theoretically) between 0 and infinity, hence the survival function can be plotted as a smooth, continuous function that tends to 0 as \(t\) goes to infinity. In practice, though, the survival function appears as a step function as (1) individuals can be observed at discrete times only and (2) not all individuals may experience the event before the end of the study. Figure \@ref(fig:survival) depicts this difference: in panel A I plotted a theoretical survival function, restricted to 15 years of follow-up for comparison purposes, while in panel B I plotted the survival function relative to the survival data simulated in Section \@ref(sa-censoring). The former is a smooth function of time, and should we extend the x-axis to infinity the function would eventually reach zero. Conversely, the latter is a step function with steps at each event time, and should we extend the x-axis to infinity the function would remain flat after the last observed event.

```{r survival, fig.width = 2 * opts_chunk$get()$fig.width, fig.cap = "Theoretical survival function (A) and observed survival function for simulated data (B)."}
# theoretical survival function (weibull with lambda = 0.1, p = 1.5)
sfun = function(t) exp(-lambda * t ^ p)
g1 = ggplot(data.frame(t = 0:5), aes(t)) + 
  stat_function(fun = sfun) +
  theme_bw() + 
  scale_y_continuous(labels = scales::percent) + 
  coord_cartesian(ylim = c(0, 1)) + 
  labs(x = "Time", y = "S(t)")

# actual survival function from simulated data
sf = survfit(Surv(time = t0, event = d) ~ 1, data = data)
g2 = autoplot(sf, conf.int = F, censor = F) + 
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(x = "Time", y = "S(t)")
plot_grid(g1, g2, align = "hv", labels = LETTERS)
rm(sfun, sf, g1, g2)
```

The hazard function \(h(t)\) is the limit of the probability of the survival time \(T\) laying within an interval \([t, t + \Delta(t))\) given that an individual survived up to time \(t\) divided by the length of the interval \(\Delta(t)\), for \(\Delta(t)\) approaching zero:
\[
h(t) = \lim_{\Delta(t) \to 0} \frac{P(t \le T < t + \Delta(t) | T \ge t)}{\Delta(t)}
\]
It represent the instantaneous potential (e.g. risk) for the event to occur within the interval \([t, t + \Delta(t))\) (with \(\Delta(t) \to 0\)), given that the individual survived up to time \(t\). The hazard function is always non-negative, it can assume different shapes over time, and it has no upper bound. In Figure \@ref(fig:hazard) I present a simple hazard function; it increases over time, which means that the instantaneous risk of event increases over time.

```{r hazard, fig.cap = "Example of hazard function."}
# theoretical hazard function (weibull with lambda = 0.01, p = 1.7)
hfun = function(t) lambda * p * t ^ (p - 1)
ggplot(data.frame(t = 0:5), aes(t)) + 
  stat_function(fun = hfun) +
  theme_bw() + 
  labs(x = "Time", y = "h(t)")

rm(hfun)
```

The survival function from Figure \@ref(fig:survival), panel A, and the hazard function from Figure \@ref(fig:hazard) are strictly related. In fact, there is a clearly defined mathematical relationship between the survival and the hazard function: it is possible to derive the form of \(S(t)\) when knowing the form of \(h(t)\), and vice versa. Formally:
\[
S(t) = \exp \left[ -\int_0^t h(u) \ du \right]
\]
\[
h(t) =  -\left[ \frac{d S(t) / dt}{S(t)} \right]
\]

Finally, a third quantity of interest in survival analysis that is strictly related to the survival and hazard functions is the cumulative hazard function \(H(t)\). The cumulative hazard function represents the accumulation of hazard (e.g. \(h(t)\)) over time, and can be defined as
\[
H(t) = \int_0^t h(u) \ du;
\]
it can conveniently be expressed in terms of survival function via the relationship \(H(t) = - \log S(t)\), or alternatively with \(S(t) = \exp(-H(t))\).

## Estimation of the survival function {#sa-estsurv}

The survival function presented in Figure \@ref(fig:survival), panel B, is a non-parametric estimate of the true survival function based on the data only. The estimator employed is this case is the Kaplan-Meier estimator of the survival function [@kaplan_1958], with which the estimated survival probabilities are obtained using a product limit formula. The general form for the Kaplan-Meier estimator at time \(t_{(i)}\) is
\[
\hat{S}(t_{(i)}) = \hat{S}(t_{(i - 1)}) \times \hat{P}(T > t_{(i)} | T \ge t_{(i)}),
\]
with \(t_{(i)}\) being the \(i\)^th^ ordered failure time. The interpretation is straightforward: it is the product of the probability of surviving past the previous event-time (\(\hat{S}(t_{(i - 1)})\)) times the conditional probability of surviving past the current time \(t_{(i)}\) given survival to at least the current time (\(\hat{P}(T > t_{(i)} | T \ge t_{(i)})\)). The product limit formula is:
\[
\hat{S}(t_{(i)}) = \prod_{j = 1}^i \hat{P}(T > t_{(j)} | T \ge t_{(j)})
\]
The conditional probability in the product limit formula can be estimated from the observed data as:
\[
\hat{P}(T > t_{(i)} | T \ge t_{(i)}) = \frac{r_{(i)} - e_{(i)}}{r_{(i)}},
\]
where \(r_{(i)}\) and \(e_{(i)}\) are the number of individuals at risk and the number of events at time \(t_{(i)}\), respectively.

The Kaplan-Meier estimator can be computed using R and the function `survfit` from the `survival` package. An example using the simulated data from Section \@ref(sa-censoring) (stored in a data frame named `data`):

```{r km-example-fit, echo = TRUE}
# library(survival)
fit = survfit(Surv(time = t0, event = d) ~ 1, data = data)
summary(fit)
```

By doing so, I obtain an estimate of the survival function (column `survival`) at each distinct failure time (column `time`). For instance, the survival probability at \(t\) = `r cute(summary(fit)$time[3])` is `r cute(summary(fit)$surv[3])`, with 95% confidence interval (`r cute(summary(fit)$lower[3])` - `r cute(summary(fit)$upper[3])`).

Finally, plotting the estimated survival curve I obtain Figure \@ref(fig:km-example-plot), which is exactly the same survival curve presented in panel B of Figure \@ref(fig:survival).

```{r km-example-plot, echo = TRUE, fig.cap = "Estimated survival function using the Kaplan-Meier estimator on the simulated data."}
# library(ggfortify)
autoplot(fit, conf.int = FALSE, censor = FALSE) + 
  theme_bw() +
  coord_cartesian(ylim = c(0, 1)) +
  labs(x = "Time", y = expression(hat(S)[KM](t)))
```

An alternative way of estimating the survival function is to use the _Nelson-Aalen_ estimator for the cumulative hazard
\[
\hat{H}(t) = \sum_{t_i < t} \frac{e_i}{r_i} = \sum_{t_i < t} \hat{h}_i,
\]
and then use the relationship presented in Section \@ref(sa-terminology-notation) to obtain the survival function.

## Parametric survival analysis {#sa-parsa}

In applied settings it is often of interest to assess the association between observed covariates and the survival time of interest. For instance, it may be of interest to study whether a treatment is effective in slowing disease relapse (e.g. relapse of leukemia), whether there are difference between genders or age categories. A common way of assessing the effect of covariates on a time to event outcome, while adjusting for potentially confounding factors at the same time, consists in using a regression model. 

In the context of survival data, two models are commonly used: the _accelerated failure time_ model (AFT), and the _proportional hazards_ (PH) model. In the former, the natural logarithm of the observed survival time \(\log t\) is expressed as a linear function of the covariates \(X\):
\[
\log t = X \beta + \epsilon,
\]
with \(\beta\) a vector of regression coefficients and \(\epsilon\) a vector of residual error terms. Assuming a parametric distribution for \(\epsilon\) determines the regression model: log-normal, log-logistic, Weibull, etc. In the AFT model, a positive association of the covariates with survival time implies an increased expected time to event. In the PH model, the covariates have a multiplicative effect on the hazard function:
\[
h(t; X) = h_0(t) g(X),
\]
for some \(h_0(t)\) and \(g(X)\), with \(g(\cdot)\)) a non-negative function of the covariates. A popular choice for the latter is \(g(X) = \exp(X \beta)\); conversely, is is possible to either left the former unspecified, or assume a parametric distribution. The focus of this Section is on specifying a parametric distribution for \(h_0(t)\), yielding the so-called _parametric survival regression models_; I will present commonly assumed parametric distributions in Section \@ref(sa-parsa-distributions), the estimation procedure in Section \@ref(sa-parsa-estimation), and an example using data from the International Stroke Trial (IST) [@ist_1997; @istdb_2011] in Section \@ref(sa-parsa-example). Leaving \(h_0(t)\) unspecified yields the semi-parametric Cox model, that I will present in Section \@ref(sa-cox). 

From now on I will focus on the proportional hazards formulation of the survival model.

### Failure time distribution {#sa-parsa-distributions}

I mentioned in Section \@ref(sa) that the random variable representing the survival time is non-negative; hence, we can choose any non-negative distribution to assign to \(h_0(t)\). Commonly used distribution are the Exponential, Weibull, log-Normal, and Gompertz distributions; other possible distributions are the inverse Weibull, the log-skew-Normal [@azzalini_1985], the log-logistic and complex mixture distributions (such as the two components mixture Weibull distribution, @mclachlan_1994). Each distribution yields a different Survival and hazard function:

- Exponential distribution:
    * \(h_0(t) = \lambda\)
    * \(S(t) = \exp(-\lambda t)\)
    * \(\lambda > 0\)
- Weibull distribution:
    * \(h_0(t) = \lambda p t ^ {p - 1}\)
    * \(S(t) = \exp(-\lambda t ^ p)\)
    * \(\lambda, p > 0\)
- log-Normal distribution:
    * \(h_0(t) = \frac{\phi(\frac{\log t - \mu}{\sigma})}{\sigma t \left(1 - \Phi \left[ \frac{\log t - mu}{\sigma} \right] \right)}\)
    * \(S(t) = 1 - \Phi\left( \frac{\log t - \mu}{\sigma} \right)\)
    * \(\mu \in R; \sigma > 0\)
- Gompertz distribution:
    * \(h_0(t) = \lambda \exp(\gamma t)\)
    * \(S(t) = \exp \left[ -\frac{\lambda}{\gamma} (\exp(\gamma t) - 1) \right]\)
    * \(\lambda, \gamma > 0\)
- inverse Weibull distribution:
    * \(h_0(t) = \frac{\lambda p t ^ {-(p + 1)}}{\exp(\lambda t ^ {-p}) - 1}\)
    * \(S(t) = 1 - \exp(-\lambda t ^ {-p})\)
    * \(\lambda, p > 0\)
- log-logistic distribution:
    * \(h_0(t) = \frac{\exp(\alpha) \kappa t ^ {\kappa - 1}}{1 + \exp(\alpha) t ^ \kappa}\)
    * \(S(t) = \frac{1}{1 + \exp(\alpha) t ^ \kappa}\)
    * \(\alpha \in R; \kappa > 0\)
- log-skew-Normal distribution^[\(SN(\cdot)\) is the cumulative distribution function of a skew-Normal random variable with parameters \(\xi\), \(\omega\), and \(\alpha\) [@azzalini_1985]]:
    * \(h_0(t) = \frac{1 \phi \left( \frac{\log t - \xi}{\omega} \right) \Phi \left( \alpha \frac{\log t - \xi}{\omega}\right)}{t \omega \left[ 1 - SN \left( \log t; \xi, \omega, \alpha \right) \right]}\)
    * \(S(t) = 1 - SN\left( \log t; \xi, \omega, \alpha \right)\)
    * \(\xi, \alpha \in R; \omega > 0 \)
- Two components mixture Weibull distribution:
    * \(h_0(t) = \frac{\lambda_1 p_1 t ^ {p_1 - 1} \pi \exp(-\lambda_1 t ^ {p_1}) + \lambda_2 p_2 t ^ {p_2 - 1} (1 - \pi) \exp(-\lambda_2 t ^ {p_2})}{\pi \exp(-\lambda_1 t ^ {p_1}) + (1 - \pi) \exp(-\lambda_2 t ^ {p_2})}\)
    * \(S(t) = \pi \exp(-\lambda_1 t ^ {p_1}) + (1 - \pi) \exp(-\lambda_2 t ^ {p_2})\)
    * \(\lambda_1, \lambda_2, p_1, p_2 > 0; \pi \in [0, 1]\)

### Estimation procedure {#sa-parsa-estimation}

Assume \(n\) observations with the bivariate response \((t_i, d_i)\), with \(i = 1, \dots, n\). For a given survival function \(S(t)\) the density function is given by 
\[
f(t) = - \frac{d S(t)}{dt},
\]
and the hazard function by 
\[
h(t) = \frac{f(t)}{S(t)}.
\]

The parameters of the parametric proportional hazards survival model presented in Section \@ref(sa-parsa) can be estimated via the maximum likelihood method. A subject that experiences the event of interest at time \(t_i\) contributes to the likelihood the density at time \(t_i\), i.e. \(f(t_i)\); conversely, a censored observation know to survive until tile \(t_i\) contributes \(S(t_i)\) to the likelihood. The individual contribution to the likelihood \(L_i\) can therefore be written as
\[
L_i = h(t_i) ^ d_i S(t_i),
\]
where \(d_i\) is the event indicator variable. The overall likelihood is the product of the individual contributions:
\[
L = \prod_{i = 1} ^ n L_i.
\]
Taking the natural logarithm of the likelihood for ease of computation:
\[
\begin{aligned}
\log L &= \sum_{i = 1} ^ n \left[ d_i \log f_i(t_i) + (1 - d_i) \log S_i(t_i) \right] = \\
       &= \sum_{i = 1} ^ n \left[ d_i \log h_i(t_i) + \log S_i(t_i) \right]
\end{aligned}
\]
Implicit in the above log-likelihood are the regression parameters \(\beta\) and the parameters of the parametric distribution of choice for \(h_0(t)\). 

The log-likelihood function \(\log L\) has a closed-form; maximum likelihood estimates for \(\beta\) and the distribution parameters can hence be obtained by maximising \(\log L\), e.g. using one of the many general purpose optimisers available in R (`optim`, `nlm`, ...).  

### Data analysis example {#sa-parsa-example}

The International Stroke Trial (IST) was a large, prospective, randomised controlled trial conducted between 1991 and 1996. The aim of the trial was to assess whether early administration of aspirin, heparin, both or neither influenced clinical outcomes in patients with acute ischaemic stroke [@ist_1997; @istdb_2011]. 

```{r get-process-ist-data, eval = FALSE}
# this chunk does not run always
# I run it once to download and process the data, that will be saved in the data/ folder as "ist.csv"

# read data
ist = read_csv("http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_corrected.csv?sequence=5&isAllowed=y", col_types = paste(c("___ci", rep("_", 20), "cc", rep("_", 60), "ii", rep("_", 23)), collapse = ""))

# recode heparin treatment
ist$RXHEP = ifelse(ist$RXHEP %in% c("H", "M"), "H", ist$RXHEP)

# drop rows with missing values
ist = na.omit(ist)

# drop rows with survival time of zero
ist = ist[ist$TD > 0,]

# export
write_csv(ist, "data/ist.csv")
```

As illustration, I will evaluate the association between tretment with aspirin and/or heparin and survival after acute ischaemic stroke. I will start by reading the data, stored in the `ist.csv` file. This file is a subset of the full IST dataset containing information on age, gender, treatment, and survival; further, individuals with missing values and individuals with a survival time of zero were dropped.

```{r read-ist-data, echo = TRUE}
# library(readr)
ist = read_csv("data/ist.csv", 
  col_names = c("gender", "age", "rxasp", "rxhep", "d", "t"), 
  col_types = "ciccii", skip = 1)
attr(ist, "spec") = NULL # removing "spec" attribute

# turn treatments into factors
ist$rxasp = factor(ist$rxasp, levels = c("N", "Y"))
ist$rxhep = factor(ist$rxhep, levels = c("N", "L", "H"))
```

I fit first a parametric survival model assuming a Weibull distribution for \(h_0(t)\). The hazard function, including covariates and the imposing proportional hazards, has the form
\[
h(t; X) = \lambda p t ^ {p - 1} \exp(X \beta),
\]
while the survival function has the form
\[
S(t; X) = \exp(-\lambda t ^ p \exp(X \beta)).
\]
\(X\) is the model design matrix, and \(\beta\) is the vector of regression coefficients. The log-likelihood has the form
\[
\log L = \sum_{i = 1} ^ n \left[ d_i \log h_i(t_i) + \log S_i(t_i) \right]
\]

First, I code a function with the model log-likelihood. The function depends on (1) the model parameters \(\beta\), \(\lambda\), and \(p\) (`pars` argument), (2) the model design matrix \(X\) (`X` argument), and (3) survival time \(t\) and event indicator \(d\) (`t` and `d` arguments):

```{r likelihood, echo = TRUE}
ll = function(pars, X, t, d) {
  lambda = exp(pars[1])
  p = exp(pars[2])
  beta = pars[-(1:2)]
  log_hi = log(lambda) + log(p) + (p - 1) * log(t) + c(X %*% beta)
  log_Si = -lambda * t ^ p * exp(c(X %*% beta))
  ll = sum(d * log_hi + log_Si + log(t))
  # + sum(log(t)) is the same adjustment that Stata
  # does to remove the time units from log L
  return(-ll)
}
```

The function `ll()` returns the negative log-likelihood as most optimisers minimise a target function (and so does `optim`); however, minimising the negative log-likelihood function is equivalent to maximising the log-likelihood.

I define the model matrix \(X\) for a model with aspirin treatment, heparin treatment, and their interactions. The first column is removed to avoid collinearity:

```{r model-matrix, echo = TRUE}
X = with(ist, model.matrix(t ~ rxasp * rxhep - 1))[,-1]
```

Next, I define the starting values for the optmisation routine. I choose the value 1 for the parameters of the Weibull distribution and the value 0 for the regression coefficients:

```{r starting-values, echo = TRUE}
start = c(1, 1, rep(0, ncol(X)))
names(start) = c("lambda", "p", colnames(X))
```

The value of the log-likelihood function at the starting values is `r cute(-ll(start, X, ist$t, ist$d))`. Finally, I use the robust-variance modification of the Marquard algorithm, which is more efficient than Gauss-Newton-like algorithms when starting from points very far from the optimum [@marquardt_1963; @commenges_2006]:

```{r optimisation, echo = TRUE}
# library(marqLevAlg)
fit = marqLevAlg(b = start, 
  fn = function(x) ll(x, X = X, t = ist$t, d = ist$d))
```

Assess convergency:

```{r convergency, echo = TRUE}
fit$istop
```

The convergence status indicator is equal to 1, hence the convergence criteria were satisfied. The log-likelihood at the maximum likelihood estimates is `r cute(-ll(fit$b, X, ist$t, ist$d))`. The optimising routine returns the upper triangle matrix of variance-covariance estimates at the stopping point, which can be used to obtain standard errors of the estimated coefficients:

```{r se, echo = TRUE}
fit$vcov = matrix(0, 
  nrow = length(fit$b), 
  ncol = length(fit$b))
fit$vcov[upper.tri(fit$vcov, diag = TRUE)] = fit$v
fit$vcov[lower.tri(fit$vcov)] = t(fit$vcov)[lower.tri(fit$vcov)]
```

Finally, I build a table of results:

```{r results, echo = TRUE}
res = data.frame(
  coef = fit$b,
  hr = exp(fit$b),
  se = sqrt(diag(fit$vcov)))
res$z = res$coef / res$se
res$p = 2 * pmin(pnorm(-abs(res$z)), 1 - pnorm(abs(res$z)))
kable(res, 
  digits = 3, 
  align = "rrrrr", 
  booktabs = TRUE,
  col.names = c("Beta", "Hazard ratio", "SE (Beta)", "Z", "P > |Z|"),
  linesep = "",
  caption = "Results from a parametric Weibull model.")
```

I test the interaction term using the Wald test to assess whether combining aspirin and heparin alter their association with time to event. I use the Wald \(\chi^2\) test statistic as the sample
size is big enough for it to be equivalent to its \(F\) counterpart:

```{r wald-interaction, echo = TRUE}
# identify the interaction terms
idx = grepl(":", names(fit$b))

# compute the W statistic
W = t(fit$b[idx]) %*% solve(fit$vcov[idx, idx]) %*% fit$b[idx]

# produce the test
c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx)))
```

The interaction terms seem to be not statistically significant. We can conclude the association of aspirin and heparin treatments with survival are not dependent on one another. 

I can then re-fit the model excluding the interaction terms:

```{r re-optimisation, echo = TRUE}
X = with(ist, model.matrix(t ~ rxasp + rxhep - 1))[,-1]
start = c(1, 1, rep(0, ncol(X)))
names(start) = c("lambda", "p", colnames(X))
re_fit = marqLevAlg(b = start, 
  fn = function(x) ll(x, X = X, t = ist$t, d = ist$d))
re_fit$istop
```

The routine converged. I produce the variance-covariance matrix:

```{r re-se, echo = TRUE}
re_fit$vcov = matrix(0, 
  nrow = length(re_fit$b), 
  ncol = length(re_fit$b))
re_fit$vcov[upper.tri(re_fit$vcov, diag = TRUE)] = re_fit$v
re_fit$vcov[lower.tri(re_fit$vcov)] = t(re_fit$vcov)[lower.tri(re_fit$vcov)]
```

Finally, I build a new table of results:

```{r re-results, echo = TRUE}
re_res = data.frame(
  coef = re_fit$b,
  hr = exp(re_fit$b),
  se = sqrt(diag(re_fit$vcov)))
re_res$z = re_res$coef / re_res$se
re_res$p = 2 * pmin(pnorm(-abs(re_res$z)), 1 - pnorm(abs(re_res$z)))
kable(re_res, 
  digits = 3, 
  align = "rrrrr",
  col.names = c("Beta", "Hazard ratio", "SE (Beta)", "Z", "P > |Z|"),
  booktabs = TRUE,
  linesep = "",
  caption = "Results from a parametric Weibull model with no interactions.")
```

I now test the significance of the two coefficients related to heparin treatment jointly:

```{r re-wald-rxhep, echo = TRUE}
idx = grepl("^rxhep", names(re_fit$b))
W = t(re_fit$b[idx]) %*% solve(re_fit$vcov[idx, idx]) %*% re_fit$b[idx]
c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx)))
```

Heparin treatment seems to be not statistically significantly associated with time to death in acute ischaemic stroke patients; the effect size is small, with a 4% and 7% increased risk for the `L` and `H` heparin treatment modalities versus no heparin treatment, respectively. 

Finally, the treatment with aspirin is also not statistically significantly associated with the outcome; effect size is small as well, approximately a 5% risk reduction for aspirin treatment compared to no treatment with aspirin (Table \@ref(tab:re-results)).

This is a simple application of parametric survival models; a fully developed analysis should take further aspects into account, such as considering different hazard distributions. It is possible to estimate various models and compare their fit to a specific distribution using information criteria such as the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). 

## The Cox proportional hazards model {#sa-cox}

The parametric survival models of Section \@ref(sa-parsa) could have both the accelerated failure time form and the proportional hazards form. Recall that the latter is formulated in terms of the hazard function:
\[
h(t; X) = h_0(t) \exp(X \beta)
\]
As I mentioned before, this model requires specifying the baseline hazard function \(h_0(t)\) (e.g. using one of the parametric distributions of Section \@ref(sa-parsa-distributions)) and by leaving it unspecified I obtain the Cox proportional hazards model. Such model is also called _semi-parametric_ as it is formed by a non-parametric component (the baseline hazard left unspecified) and a parametric component (the modelling assumption for the functional form of \(g(\cdot)\), the usual \(\exp(X \beta)\) in this case). The survival function for a Cox model can be written as:
\[
S(t; X) = \exp \left[ -\int_0^t h_0(u) \exp(X \beta) \ du \right]
\]

The main problems when fitting a Cox model are related to estimation of the regression coefficients \(\beta\) and of the survival function \(S(t)\). 

The main method for estimating the regression coefficients is the method of partial likelihood, proposed and discussed in detail in @cox_1972 and @cox_1975. In brief, the observed data are assumed to have density function \(f(t; \theta, \beta)\) in which \(\beta\) is the vector of regression coefficients of interest and \(\theta\) can be considered a vector of nuisance parameters. In particular, \(\theta\) represents the unspecified function \(h_0(t)\). It can be showed that is is possible to factorise the density into two terms, one of which only depends on \(\beta\): this term is called _partial likelihood_. Ignoring the term that depends on \(\theta\), and even if the partial likelihood is not directly interpretable as a likelihood in the ordinary sense, it can be used like an ordinary likelihood for estimation purposes as the usual
asymptotic properties formulas and properties associated with the likelihood function
and likelihood estimation apply.
The partial likelihood applies directly to the relative risk model \(h(t; X)\), assuming independent right censoring. The individual contribution to the likelihood has the form
\[
L_i(\beta) = \frac{h(t_i; x_i) \Delta t_i}{\sum_{l \in R(t_i)} h(t_i; x_l) \Delta t_i},
\]
and provides information on failures occurrence in the interval \([t_i, t_i + \Delta t_i)\); \(R(t_i)\) is the risk set of individuals at risk of failing at time \(t_i^{-}\), right before \(t_i\). Under the relative risk model, the baseline hazard in \(h(t; X)\) cancels out in the numerator and denominator; the product over \(i\) gives the partial likelihood for \(\beta\):
\[
L(\beta) = \prod_{i = 1}^n \frac{\exp(x_i \beta)}{\sum_{l \in R(t_i)} \exp(x_l \beta)}.
\]
The values of \(\beta\) that maximise the partial likelihood \(\hat{\beta}\) can be obtained by using a Newton-Raphson-like algorithm; asymptotics are fully analogous to a parametric likelihood.
A caveat of the partial likelihood method is that it assumes continuous failure times: in practice, that is unrealistic and there will be tied failure times (e.g. due to rounding). In that case, several methods have been proposed to adjust the partial likelihood in order to handle ties; see for instance @peto_1972, @breslow_1974, and @efron_1977

Next, consider deriving an estimator for the survival function from a Cox model. The form of \(h_0(t)\) is unspecified, hence it is not possible to directly estimate the parameters of the distribution as in fully parametric survival models. Under a Cox model, the survival function has the form
\[
S(t; X) = S_0(t) ^ {\exp(X \beta)}
\]
The coefficients \(\beta\) are estimated using the penalised likelihood procedure, and the baseline survival function \(S_0(t)\) is estimated by assuming that the baseline hazard function is constant between each pair of consecutive observed failure times. The resulting estimator, known as the Breslow estimator, estimates the cumulative baseline hazard function as
\[
\hat{H}_0(t) = \sum_{t(i) \le t} \frac{e_{(i)}}{\sum_{l \in R(t_{(i)}) \exp(x_l \hat{\beta})}},
\]
with \(e_{(i)}\) the number of events at time \(t_{(i)}\). The baseline survival function follows as
\[
\hat{S}_0(t) = \exp \left[ -\hat{H}_0(t) \right],
\]
and the survival function as
\[
\hat{S}(t; X) = \hat{S}_0(t) ^ {\exp(X \hat{\beta})}.
\]
An alternative estimator based on approximating the baseline survival function as a step function and consequently solving \(k\) simultaneous equations has been proposed by @kalbfleisch_2011, and is omitted here.

Finally, the Cox model relies on two main assumptions. First, the assumption of non-informative censoring, e.g. the censoring process must be independent of any covariate, observed and not. Second, the proportional hazards assumption requires hazards to be proportional across time, e.g. the hazard must be constant. There are several ways of testing the proportional hazards assumption, both analytical and graphical; see Chapter 4 of @kleinbaum_2012 for further details.

### Data analysis example {#sa-cox-example}

In this section I re-analyse the IST data of Section \@ref(sa-parsa-example) using a semi-parametric Cox model. I first read the dataset:

```{r cox-read-ist-data, echo = TRUE}
# library(readr)
ist = read_csv("data/ist.csv", 
  col_names = c("gender", "age", "rxasp", "rxhep", "d", "t"), 
  col_types = "ciccii", skip = 1)
attr(ist, "spec") = NULL # removing "spec" attribute

# turn treatments into factors
ist$rxasp = factor(ist$rxasp, levels = c("N", "Y"))
ist$rxhep = factor(ist$rxhep, levels = c("N", "L", "H"))
```

I fit the Cox model using the `coxph()` function from the `survival` package:

```{r cox-fit, echo = TRUE}
# library(survival)
fit = coxph(Surv(t, d) ~ rxasp * rxhep, data = ist)
summary(fit)
```

I test again the joint significancy of the interaction terms using the Wald test:

```{r cox-wald-interaction, echo = TRUE}
idx = grepl(":", names(coef(fit)))
W = t(coef(fit)[idx]) %*% solve(vcov(fit)[idx, idx]) %*% coef(fit)[idx]
c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx)))
```

Analogously as before, the interaction is not significantly different than zero. I re-fit the model without the interaction term:

```{r re-cox-fit, echo = TRUE}
# library(survival)
re_fit = coxph(Surv(t, d) ~ rxasp + rxhep, data = ist)
summary(re_fit)
```

Testing the significancy of the heparin treatment using the Wald test:

```{r re-cox-wald-rxhep, echo = TRUE}
idx = grepl("^rxhep", names(coef(fit)))
W = t(coef(fit)[idx]) %*% solve(vcov(fit)[idx, idx]) %*% coef(fit)[idx]
c(W = W, df = sum(idx), `p-value` = 1 - pchisq(W, sum(idx)))
```

Treatment with heparin is not statistically significantly associated with risk of death; besides that, the effect size of heparin treatment is small: approximately 4% and 6% risk increase for heparin treatment modalities `L` and `H` compared to no heparin treatment, respectively. 

The treatment with aspirin is also barely significantly different than zero, assuming a significancy level \(\alpha = 0.10\), with a p-value of 0.0954. The effect size is comparable to the estimated effect size obtained with the Weibull model, approximately 5% risk reduction for treatment with aspirin compared to no aspirin treatment.

## Advances in survival analysis {#sa-advances}

There are several extensions of the statistical methods presented in this Chapter: I will briefly introduce some of them in this Section, without going into great detail as that would be beyond the scope of this report.

The proportional hazards models have been extended to include time-dependent covariates and time-dependent covariate effects; additionally, the Cox model has been extended to allow stratification by a given factor (details in @kalbfleisch_2011 and @kleinbaum_2012). Parametric survival models have been generalised to allow combinations of linear predictors and penalised smoothers for the effect of time and covariates, both in the proportional hazards and proportional odds framework [@liu_2016]. More generally, models have been developed to account for competing events and multi-state diseases, even with intermediate states, and for modelling a wide range of multivariate survival data [@geskus_2015; @crowder_2016]. Finally, the main advances that I will discuss further are models with random effects and joint models for longitudinal and survival data, in Chapters \@ref(smre) and \@ref(jm) respectively.
