# Simulation study: accuracy of Gaussian quadrature {#simst1}
\chaptermark{Accuracy of Gaussian quadrature}

In this Chapter I will present the first simulation study I run during my first year, on accuracy on Gaussian quadrature methods. I presented part of this work as an oral presentation at the 2017 Survival Analysis for Junior Researchers conference (more details in Chapter \@ref(pdevelop) and slides available in Appendix \@ref(ax-slides-safjr)). The Chapter is partitioned into seven Sections, and in each Section I will present an aspect of the simulation study: the aim of the study (Section \@ref(simst1-aim)), the data-generating mechanisms (Section \@ref(simst1-dgms)), the methods that I will compare (Section \@ref(simst1-methods)) and the estimands of interest (Section \@ref(simst1-est)), the performance measures I will use to compare the methods (Section \@ref(simst1-pm)), the results of the study (Section \@ref(simst1-res)), and finally a brief discussion in Section \@ref(simst1-conclusions).

## Aim {#simst1-aim}

The aim of this simulation study is two-fold. First, I want to assess the accuracy of numerical integration methods - Gaussian quadrature, specifically - in settings where it is possible to obtain analytical formulae; analytical formulae will be used as a control method. Second, I aim to assess the accuracy of Gaussian quadrature when analytical formulae are not available and therefore quadrature is indeed required.
In order to fulfill these two aims I will simulate clustered survival data with a frailty component shared between individuals belonging to the same cluster; the distribution I will assign to the frailty will determine whether analytical formulae are available or not. I will choose a Gamma distribution for answering the first aim, as I showed in Section \@ref(smre-shared-frailty) that in that setting the likelihood has a closed form; then, I will choose a log-normal distribution for the frailty using the random intercept parametrisation as explained in Section \@ref(smre-random-effects) for answering the second aim. The likelihood of this model doesn't have a closed form, hence I will need numerical integration to estimate the model.

## Data-generating mechanisms {#simst1-dgms}

I generated survival data from a Weibull distribution with shape parameter \(\lambda = 0.5\) and scale parameter \(p = 0.6\) (Figure \@ref(fig:sim1-wh)) using the method of @bender_2005 as explained in Section \@ref(simsurv), and applying administrative censoring at time \(t = 5\); I used the following parametrisation for the Weibull distribution:
\[
h(t) = \lambda p t ^ {p - 1}
\]

```{r sim1-wh, fig.cap = "Weibull baseline hazard function used in Simulation 1 (\\(\\lambda = 0.5, p = 0.6\\)) "}
wh <- function(t, lambda = 0.5, p = 0.6) lambda * p * t ^ (p - 1)
data_frame(
  t = seq(0.001, 5, length.out = 1000)) %>%
  mutate(h = wh(t)) %>%
  ggplot(aes(x = t, y = h)) +
  geom_line() +
  theme_bw() +
  labs(x = "Time", y = "h(t)")
```

I included a binary covariate (e.g. a treatment) simulated by drawing from a Bernoulli random variable with parameter \(\pi = 0.5\), and a frailty term shared between individuals in a cluster by drawing first from a Gamma distribution with shape parameter \(1 / \theta\) and scale parameter \(\theta\) (for identifiability purposes) and then by drawing from a normal distribution with mean \(\mu = 0\) and standard deviation \(\sigma = \sqrt{\theta}\). I varied \(\theta\): \(\theta = \{0.25, 0.75, 1.25\}\). I also varied the regression coefficient (e.g. the log-treatment effect) \(\beta\) associated with the binary covariate: \(\beta = \{-0.50, 0.00, 0.50\}\). I simulated data for six different sample sizes: 15 clusters of 30, 100, or 500 individuals each, 50 clusters of 30 or 100 individuals, and 1000 clusters of 2 individuals. As a result of this, sample size varied between 450 and 7500 individuals. Finally, I used a fully factorial design combining different frailty variances, frailty distributions, treatment effects, and sample sizes; it resulted in \(3 \times 2 \times 3 \times 6 = 108\) different data-generating mechanisms, and for each of them I generated 1000 datasets.

I will present the results separately by frailty variance, as I will use the 54 scenarios with a Gamma frailty to answer the first aim and the remaining 54 scenarios with a log-normal frailty to answer the second aim.

## Methods {#simst1-methods}

I fitted a set of models for each simulated dataset under each data-generating mechanism. Specifically, for the data generated assuming a Gamma frailty, I compared the following models:

* a shared Gamma frailty model with a baseline Weibull hazard using the analytical formulation of the likelihood (method _AN_);

* a shared Gamma frailty model with a baseline Weibull hazard using the likelihood approximated numerically via Gaussian quadrature (specifically, a Gauss-Laguerre quadrature rule) with 15, 35, 75, and 105 nodes (methods _GQ15_, _GQ35_, _GQ75_, _GQ105_);

* a shared Gamma frailty model with a baseline Weibull hazard using the likelihood approximated numerically via Gauss-Kronrod quadrature (as implemented in R's `integrate()` function; method _IN_).

Then, for data generated assuming a log-normal frailty, I fitted a Weibull model with a random intercept using the likelihood approximated via Gauss-Hermite quadrature using 15, 35, 75, and 105 nodes.

## Estimands {#simst1-est}

For each model fitted under each scenario, I compared:

1. the estimated parameters of the Weibull distribution, i.e. \(\hat{\lambda}\) and \(\hat{p}\);

2. the estimated log-treatment effect, i.e. \(\hat{\beta}\);

3. the estimated variance of the frailty term, i.e. \(\hat{\theta}\).

## Performance measures {#simst1-pm}

First, I am interested in the performance of the maximum likelihood estimation procedure; that is, how precise is the maximum likelihood estimator. I will assess this by computing bias for each estimand, defined as \(b = E(\hat{\beta}) - \beta\). Additionally, I will compute relative bias (defined as \(100 \times [E(\hat{\beta}) - \beta] / \beta\)) for presentation purposes, as it will be useful to compare bias for estimands with different magnitude (and therefore bias may be greater in absolute value but smaller in relative value).

Next, I am interested in coverage, i.e. the proportion of times the \(100 \times (1 - \alpha)\%\) confidence interval \(\hat{\beta} \pm Z_{1 - \alpha / 2} \times SE(\hat{\beta})\) includes the true value \(\beta\). This allow to assess whether the empirical coverage rate approaches the nominal coverage rate (\(100 \times (1 - \alpha)\%\)), to properly control the type I error rate for testing a null hypotesis of no effect.

Finally, I am interested in overall accuracy and therefore I will compute the mean squared error, defined as the sum of bias and variability: \((\bar{\hat{\beta}} - \beta) ^ 2 + (SE(\hat{\beta})) ^ 2\).

Summary measures for \(\lambda\), \(p\), and \(\theta\) are computed on the log-scale. For bias and coverage I will further include Monte Carlo standard errors to quantify the uncertainty in estimating the performance measures (see @white_2010 for further details). I will also discuss convergence rates of the different methods included in the comparison.

## Results {#simst1-res}

```{r load-sim1-results}
summary_an_vs_gq = read_csv("data/summary_an_vs_qg.csv.gz") %>%
  mutate(method = factor(method, levels = c("AF", "IN", "GQ15", "GQ35", "GQ75", "GQ105")),
         par = factor(par, levels = c("lambda", "p", "trt", "theta"), labels = c("Lambda", "P", "Beta", "Theta")))
summary_normal_gq = read_csv("data/summary_normal_qg.csv.gz") %>%
  mutate(method = factor(method, levels = c("GQ15", "GQ35", "GQ75", "GQ105")),
         par = factor(par, levels = c("lambda", "p", "trt", "sigma"), labels = c("Lambda", "P", "Beta", "Sigma")))
```

I selected three scenarios for each aim of this simulation study (out of 54) to present, for conciseness. Specifically, I will present in this report:

1. small frailty variance (0.25) and negative regression coefficient (-0.50);

2. large frailty variance (1.25) and null regression coefficient (0.00);

3. 1000 clusters of 2 individuals each and positive regression coefficient (0.50).

The full results can be explored online interactively at [LINK TO ADD](https://www.google.com).

### Aim 1: comparison of Gaussian quadrature with analytical formulae

In this Section I will present results for each aforementioned scenario and the simulation comparing quadrature methods with analytical formulae, that is, fitting a shared Gamma frailty Weibull regression model.

First, bias, coverage, mean squared error, and convergence rates for scenario 1 are presented in Tables \@ref(tab:sim1-an-vs-gq-scenario1-bias-t), \@ref(tab:sim1-an-vs-gq-scenario1-cov-t), \@ref(tab:sim1-an-vs-gq-scenario1-mse-t), \@ref(tab:sim1-an-vs-gq-scenario1-convergence-t) and Figures \@ref(fig:sim1-an-vs-gq-scenario1-bias), \@ref(fig:sim1-an-vs-gq-scenario1-cov), \@ref(fig:sim1-an-vs-gq-scenario1-mse), \@ref(fig:sim1-an-vs-gq-scenario1-convergence). Convergence rates were generally good (> 90\%) for most sample sizes; the method that showed the worst convergence rates was Gauss-Kronrod quadrature with 1000 clusters of 2 individuals each, where approximately 75\% of replications converged. Bias, coverage, and overall accuracy were optimal for all methods and across all sample sizes for the scale parameter of the Weibull distribution \(p\) and the regression coefficient \(\beta\); conversely, the methods performed quite differently for the shape parameter \(\lambda\) and the frailty variance \(\theta\). The shape parameter estimated using analytical formulae or Gauss-Kronrod quadrature was generally unbiased, with good coverage and accuracy; vice versa, using Gauss-Laguerre quadrature produced underestimated coefficients when using a small number of nodes and required at least 75 nodes to yield unbiased results. As the number of nodes increased, coverage and mean squared error improved considerably. Also, sample sizes with a higher number of clusters generally yielded better estimates for the shape parameter in terms of bias, coverage, and mean squared error. The frailty variance \(\theta\) was the parameter estimated with greatest variability in the results. Analytical formulae required a high number of clusters to produce unbiased results (50 or 1000), yielding underestimated coefficients otherwise. Gauss-Kronrod performed similarly to analytical formulae, as did Gauss-Laguerre quadrature with a sufficiently high number of nodes. Coverage was generally good, above 90\% (except Gauss-Laguerre with 15 nodes, where coverage fell to 60-70\% in some settings), symptom of overestimated standard errors for the frailty variance; this inflation of the standard errors was reflected in the mean squared error, which was generally greater that the other estimated parameters for all methods under all sample sizes explored in this scenario.

Next, bias, coverage, mean squared error, and convergence rates for scenario 2 are presented in Tables \@ref(tab:sim1-an-vs-gq-scenario2-bias-t), \@ref(tab:sim1-an-vs-gq-scenario2-cov-t), \@ref(tab:sim1-an-vs-gq-scenario2-mse-t), \@ref(tab:sim1-an-vs-gq-scenario2-convergence-t) and Figures \@ref(fig:sim1-an-vs-gq-scenario2-bias), \@ref(fig:sim1-an-vs-gq-scenario2-cov), \@ref(fig:sim1-an-vs-gq-scenario2-mse), \@ref(fig:sim1-an-vs-gq-scenario2-convergence). Analogously as in scenario 1, convergence rate was generally good with Gauss-Kronrod quadrature performing the worst and estimates of the scale parameter \(p\) and the regression coefficient \(\beta\) were unbiased with optimal coverage and low mean squared error. Bias for the shape parameter was substantial when using Gauss-Laguerre quadrature with 15 nodes, up to -0.5 on the log-scale; bias was reduced greatly by increasing the number of quadrature nodes, except when assuming 15 clusters of 500 individuals each where it remained substantial (approximately -0.35 on the log-scale) even when using 105 quadrature nodes. Coverage and mean squared error followed the same pattern; however, an exception was found in the case of 1000 clusters of 2 individuals: in that setting, \(\lambda\) was estimated properly with small bias and good coverage and mean squared error. The estimated frailty variance also followed again the pattern depicted in scenario 1: analytical formulae required 50 or 1000 clusters to yield unbiased results, Gauss-Kronrod quadrature performed similarly to analytical formulae, and Gauss-Laguerre quadrature generally yielded better results with a greater number of quadrature nodes. Coverage was also generally good, symptom once again of overestimated standard errors - except the setting of 1000 clusters of 2 individuals each in which Gauss-Laguerre quadrature with 15 or 35 nodes did not cover the true value at all; the situation improved with a greater number of nodes, up to a coverage of 68.3\% with 105 nodes. Inflated standard errors for the estimated frailty variance yielded increased mean squared error in this scenario as well, with all the methods performing similarly except Gauss-Laguerre quadrature with 15 knots which performed the worst once again.

Finally, bias, coverage, percentage bias, mean squared error, and convergence rates for scenario 3 are presented in Tables \@ref(tab:sim1-an-vs-gq-scenario3-bias-t), \@ref(tab:sim1-an-vs-gq-scenario3-cov-t), \@ref(tab:sim1-an-vs-gq-scenario3-pbias-t), \@ref(tab:sim1-an-vs-gq-scenario3-mse-t), \@ref(tab:sim1-an-vs-gq-scenario3-convergence-t) and Figures \@ref(fig:sim1-an-vs-gq-scenario3-bias), \@ref(fig:sim1-an-vs-gq-scenario3-cov), \@ref(fig:sim1-an-vs-gq-scenario3-pbias), \@ref(fig:sim1-an-vs-gq-scenario3-mse), \@ref(fig:sim1-an-vs-gq-scenario3-convergence). Convergence rates were good as in the previous scenarios, with Gauss-Kronrod quadrature performing the worst again. The scale parameter \(p\) and the regression coefficient were estimated optimally once again, and further to that, in scenario 3 the shape parameter \(\lambda\) was also well estimated (in terms of bias, coverage, and overall accuracy) with the exception of methods using Gauss-Laguerre quadrature when the true frailty variance was large; in that setting, the shape parameter was slighly overestimated (3\% to 8\%), coverage was suboptimal (60-809%) and mean squared error was greatest. Nevertheless, performance improved when increasing the number of quadrature nodes to approach results obtained using analytical formulae and Gauss-Kronrod quadrature. Finally, performance measures for the estimated frailty variance were generally good with a true frailty variance small or medium; Gauss-Kronrod undercovered the true value (coverage of approximately 90\%). Conversely, when the true frailty variance was large, only analytical formulae and Gauss-Kronrod quadrature performed well. Gauss-Laguerre quadrature yielded severely underestimated results, up to -140\% with 15 nodes, with null or poor coverage and large mean squared error.

### Aim 2: accuracy when analytical formulae are not available

In this section I will present results for the simulation comparing Gauss-Hermite quadrature with varying number of knots when fitting a model for which it is not possible to derive analytical formulae, specifically a Weibull regression model with a random intercept.

First, bias, coverage, mean squared error, and convergence rates for scenario 1 are presented in Tables \@ref(tab:sim1-normal-gq-scenario1-bias-t), \@ref(tab:sim1-normal-gq-scenario1-cov-t), \@ref(tab:sim1-normal-gq-scenario1-mse-t), \@ref(tab:sim1-normal-gq-scenario1-convergence-t) and Figures \@ref(fig:sim1-normal-gq-scenario1-bias), \@ref(fig:sim1-normal-gq-scenario1-cov), \@ref(fig:sim1-normal-gq-scenario1-mse), \@ref(fig:sim1-normal-gq-scenario1-convergence). Convergence rates are good for all sample sizes (> 97\%) except when assuming 15 clusters of 500 individuals, where convergence rates drop to approximately 50\% for all methods included in this comparison. Bias is generally negligible for the parameters of the Weibull distribution \(\lambda\) and \(p\) and the regression coefficient \(\beta\): between 0.0059 and 0.0193 for \(\lambda\), between -0.0424 and -0.0332 for \(p\), between 0.0040 and 0.0867 for \(\beta\). Conversely, estimates for \(\sigma\) were negatively biased for a sample size of 15 clusters - 100 individuals, 1000 clusters - 2 individuals, 15 clusters - 30 individuals (between -0.3057 and -0.0854) and positively biased for a sample size of 15 clusters - 500 individuals (between and 0.2427 and 0.4020). Bias was negligible for a sample size of 50 clusters - 30 individuals and 50 clusters - 100 individuals (between -0.0536 and -0.0095). Coverage of all estimated coefficients was poor (< 75\%) for a sample size of 15 clusters - 500 individuals. For the regression coefficient \(\beta\) and the frailty variance \(\sigma\) coverage was good or superoptimal for the remaining sample sizes, with the exception of \(\sigma\) estimated using Gauss-Hermite quadrature with 15 nodes that resulted in slight undercoverage for sample sizes of 15 clusters - 100 individuals and 50 clusters - 100 individuals. The parameters of the Weibull distribution were generally undercovered (< 80\%) across sample sizes, except \(\lambda\) with a sample size of 1000 clusters - 2 individuals and \(p\) with a sample size of 15 clusters - 30 individuals for which coverage was in the range 90-95\%. Finally, mean squared error was low for \(\lambda\), \(p\), and \(\beta\), comparable for \(\sigma\) with a sample size of 50 clusters - 30 individuals and 50 clusters - 100 individuals, much higher for \(\sigma\) with all the remaining sample sizes (i.e. overall accuracy was lower in these settings).

Next, bias, coverage, mean squared error, and convergence rates for scenario 2 are presented in Tables \@ref(tab:sim1-normal-gq-scenario2-bias-t), \@ref(tab:sim1-normal-gq-scenario2-cov-t), \@ref(tab:sim1-normal-gq-scenario2-mse-t), \@ref(tab:sim1-normal-gq-scenario2-convergence-t) and Figures \@ref(fig:sim1-normal-gq-scenario2-bias), \@ref(fig:sim1-normal-gq-scenario2-cov), \@ref(fig:sim1-normal-gq-scenario2-mse), \@ref(fig:sim1-normal-gq-scenario2-convergence). Analogously as before, coverage was geenrally good except for the sample size of 15 clusters - 100 individuals, where only 15\% of replications succesfully converged to a solution. Bias decreased for the regression coefficient \(\beta\) with increasing number of quadrature nodes; conversely, bias for the scale parameter \(p\) was not affected by the number of quadrature nodes and remained more or less constant across sample sizes at approximately -0.15. Bias for the shape parameter \(\lambda\) was also not affected by the quadrature nodes, with negative bias for the sample size of 15 clusters - 500 individuals (approximately -0.05) and positive bias elsewhere (0.0241 to 0.0622). \(\sigma\) was consistently underestimated, with bias ranging between -0.3291 and -0.0821. The pattern for coverage was similar: coverage for \(\beta\) was good assuming at least 75 quadrature nodes were used. With a sample size of 1000 clusters - 2 individuals, 15 clusters - 30 individuals, 50 clusters - 30 individuals coverage for the regression coefficient was good irrespectively of the number of quadrature nodes, and for a sample size of 15 clusters - 500 individuals coverage was mediocre even when using 105 quadrature nodes. Coverage of \(\lambda\) and \(p\) was generally poor, between 0\% and 64\%. Coverage for \(\sigma\) was only slightly suboptimal with a sample size of 15 clusters - 30 individuals and 50 clusters - 30 individuals (> 75\%), poor to suboptimal otherwise (7\% to 75\%, with peaks above 80\% when using 75 or 105 quadrature nodes). Bias and poor coverage were reflected in the mean squared error; interestingly, it seemed to generally decrease with increasing number of quadrature nodes (especially for the regression coefficient and the variance of the random intercept).

Finally, bias, coverage, mean squared error, and convergence rates for scenario 3 are presented in Tables \@ref(tab:sim1-normal-gq-scenario3-bias-t), \@ref(tab:sim1-normal-gq-scenario3-cov-t), \@ref(tab:sim1-normal-gq-scenario3-mse-t), \@ref(tab:sim1-normal-gq-scenario3-convergence-t) and Figures \@ref(fig:sim1-normal-gq-scenario3-bias), \@ref(fig:sim1-normal-gq-scenario3-cov), \@ref(fig:sim1-normal-gq-scenario3-mse), \@ref(fig:sim1-normal-gq-scenario3-convergence). Convergence rates were good, above 99\% for each method included in the comparison and variance magnitude. Negative bias was present for \(\lambda\), and \(\beta\), between -15\% and -2\%; positive bias for \(p\) increased with greater variance magnitude, from 7\% for a true \(\sigma\) = 0.25 to 35\% for a true \(\sigma\) = 1.25. Estimates of \(\sigma\) with a small-medium frailty variance were positively biased (30 to 180\%), negatively biased otherwise (-300%). Coverage was null to poor for parameters of the Weibull distribution and a medium-large frailty variance, less than 80\%; with a small frailty variance, coverage was good for \(\lambda\) and poor for \(p\). Coverage for the regression parameter \(\beta\) was good to suboptimal, decreasing to 75-80\% with a large frailty variance. Coverage for \(\sigma\) was superoptimal for a small frailty variance, null to poor for a medium-large frailty variance. Mean squared error was greatest for \(\sigma\), irrespectively of the magnitude of the frailty variance; conversely, mean squared error tended to increase as the frailty variance increased.

## Conclusions {#simst1-conclusions}

I showed in the previous Section how Gaussian quadrature performs (1) compared to analytical formulae and (2) when it is not possible to obtain analytical formulae. Overall, Gaussian quadrature performs well with a sufficient number of quadrature nodes but the variability is great. The regression coefficient \(\beta\) is the most robust estimand across different scenarios, it is mostly unbiased (or with little bias) and with good coverage and accuracy (in terms of mean squared error). The frailty variance is the least robust estimand, with precision and accuracy greatly depending on many factors: among others, important ones seems to be the number of quadrature nodes and the number of clusters. The latter makes sense on a theoretical level: with more clusters it should be easier to estimate a properly the variance of the frailty. Accuracy and precision of the parameters of the Weibull baseline hazard also varies greatly. In conclusion, using a shared frailty model to do inference on a regression coefficient seems to be robust to the accuracy of numerical integration methods; nevertheless, if the principal research interest lays in relative risk estimates, using a parametric model may not be the best choice after all. A semiparametric Cox model - even with frailty terms if necessary - could be utilised instead. If the research objectives include absolute risk estimations, though, a parametric model is immediately more appealing. However, checking the convergence, precision, and accuracy of numerical integration by evauating and comparing an increasing number of quadrature knots appears to be fundamental.
